{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcIOSn2FsLjS",
        "outputId": "1f5afc52-00b4-428c-9c29-7829b9618920"
      },
      "outputs": [],
      "source": [
        "# set module\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "\n",
        "#경로 설정\n",
        "import os\n",
        "\n",
        "new_dir = \"/content/drive/MyDrive/기상청/데이터/지면온도\"\n",
        "os.chdir(new_dir)  # 현재 작업 디렉토리 변경\n",
        "current_dir = os.getcwd()\n",
        "print(\"현재 작업 디렉토리:\", current_dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Tpk1H3WsSQA"
      },
      "outputs": [],
      "source": [
        "# 데이터 읽어오기\n",
        "data_train = pd.read_csv(\"./surface_tp_train.csv\")\n",
        "data_test= pd.read_csv(\"./surface_tp_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pT5eMKpZvFtS"
      },
      "outputs": [],
      "source": [
        "## column 명 추출\n",
        "\n",
        "data_train.drop('Unnamed: 0', axis = 1, inplace = True)\n",
        "data_train.drop('...1', axis = 1, inplace = True)\n",
        "data_test.drop('Unnamed: 0', axis = 1, inplace = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2mvs0_q6jXY",
        "outputId": "b476466f-1dc9-4af4-82a9-b481a49af8fc"
      },
      "outputs": [],
      "source": [
        "data_train.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yo4UDcWAxq2",
        "outputId": "9f228dfc-dc4f-4e67-de8b-4023b28e67e3"
      },
      "outputs": [],
      "source": [
        "data_test.columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHWfvF8StbSj"
      },
      "source": [
        "## 시각화 진행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "l6vsQ81Est5K",
        "outputId": "cc528e30-9428-4d62-cf27-5cd4500e2e26"
      },
      "outputs": [],
      "source": [
        "## 대푯값 확인 -> train\n",
        "data_train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTSBEUvLvs-f",
        "outputId": "ad32fa69-53b9-471c-8dfe-1bb639f3be0f"
      },
      "outputs": [],
      "source": [
        "data_train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVDiasnuwrnQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "## 제거 : -99, -99.9, -999\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class data_preprocessing():\n",
        "  def __init__(self, data, test):\n",
        "    self.data = data\n",
        "    self.na_counts = None\n",
        "    self.list_na = None\n",
        "    self.ratio_result = None\n",
        "    self.list_columns = None\n",
        "    self.test = test\n",
        "\n",
        "    # 99, 99.9 값을 null로 바꾸는 함수 정의\n",
        "  def replace_values_with_null(self, value):\n",
        "    if value == -99 or value == -99.9 or value == -999:\n",
        "        return np.nan\n",
        "    else:\n",
        "        return value\n",
        "\n",
        "  def replace_99(self): ## 이 함수를 실행하면 작동\n",
        "    self.data = self.data.applymap(self.replace_values_with_null)\n",
        "    return self.data\n",
        "\n",
        "\n",
        "  ## na 개수 발견\n",
        "  def count_na_per_column(self):\n",
        "    list_na = []\n",
        "    for i in self.data.columns:\n",
        "      self.na_counts = self.data[i].isna().sum()\n",
        "      list_na.append(self.na_counts)\n",
        "\n",
        "    self.list_na = list_na\n",
        "    return self.list_na\n",
        "\n",
        "  def divide_list_elements(self, lst, divisor):\n",
        "      divided_lst = [x / divisor for x in lst]\n",
        "      return divided_lst\n",
        "\n",
        "  ## na 개수 전체 데이터 중 비율 세기\n",
        "  def ratio(self):\n",
        "    self.ratio_result = self.divide_list_elements(self.list_na, len(self.data))\n",
        "    return self.ratio_result\n",
        "\n",
        "  def remove_column(self):\n",
        "    self.list_columns = []\n",
        "    # 역순으로 반복문 실행하여 열을 삭제\n",
        "    for i in range(len(self.ratio_result)-1, -1, -1):\n",
        "        if self.ratio_result[i] >= 0.4:\n",
        "          self.list_columns.append(self.data.columns[i])\n",
        "            # print(self.ratio_result[i])\n",
        "            # print(i)\n",
        "          self.data = self.data.drop(self.data.columns[i], axis=1)\n",
        "    return self.data\n",
        "\n",
        "  def remove_columns_test(self):\n",
        "    data = self.data\n",
        "    for i in self.list_columns:\n",
        "      ## . 기준으로 마지막 부분 없애고,\n",
        "      new_col = i.replace(\"train\", \"test\")\n",
        "      # print(new_col)\n",
        "      try: ## train만 있는 열이 있을 경우에는 pass 하자\n",
        "        data = data.drop(new_col, axis=1)\n",
        "      except:\n",
        "        pass\n",
        "    return data\n",
        "\n",
        "\n",
        "  def result(self, train_or_test):\n",
        "    if train_or_test == \"train\":\n",
        "      self.replace_99()\n",
        "      self.count_na_per_column()\n",
        "      self.ratio()\n",
        "      return self.remove_column()\n",
        "    else:\n",
        "      self.data = self.test\n",
        "      self.replace_99() ## null 로 변경\n",
        "      return self.remove_columns_test() ## train에서 제거되는 행으로 제거\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ib7h0ixtvsXr",
        "outputId": "25835688-bece-4766-cce6-4fd784135aa1"
      },
      "outputs": [],
      "source": [
        "dp= data_preprocessing(data_train, data_test)\n",
        "result_train = dp.result('train')\n",
        "result_train\n",
        "result_test = dp.result(\"test\")\n",
        "result_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "oBF-a62_2QLo",
        "outputId": "a28406a8-fe6e-4f66-d555-df7f2a64d764"
      },
      "outputs": [],
      "source": [
        "result_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_ik7rxZOW9Y",
        "outputId": "f3919a4c-c324-4ea2-d758-e106fe47f8b6"
      },
      "outputs": [],
      "source": [
        "result_train.dropna(inplace = True)\n",
        "result_train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MR_L-UaaJspy"
      },
      "outputs": [],
      "source": [
        "# result_train['hh'] = result_train['surface_tp_train.mmddhh'].astype(int).astype(str).str[-2:]\n",
        "# result_train['dd'] = result_train['surface_tp_train.mmddhh'].astype(int).astype(str).str[-4:-2]\n",
        "# result_train['mm'] = result_train['surface_tp_train.mmddhh'].astype(int).astype(str).str[:-4]\n",
        "# year_mapping = {'A': 2001, 'B': 2002, 'C': 2003, 'D': 2004, 'E': 2005, 'F': 2006}\n",
        "# result_train['new_year'] = result_train['surface_tp_train.year'].map(year_mapping)\n",
        "# result_train['mm'] = result_train['mm'].apply(lambda x: x.zfill(2))\n",
        "# result_train['date'] = pd.to_datetime(result_train['new_year'].astype(str) + result_train['mm'] + result_train['dd'] + result_train['hh'], format='%Y%m%d%H')\n",
        "\n",
        "def adding_date(data_original, season): # season: all, spring, summer, fall, winter 계절별로 넣기 가능\n",
        "  data = data_original\n",
        "  data['hh'] = data['surface_tp_train.mmddhh'].astype(int).astype(str).str[-2:]\n",
        "  data['dd'] = data['surface_tp_train.mmddhh'].astype(int).astype(str).str[-4:-2]\n",
        "  data['mm'] = data['surface_tp_train.mmddhh'].astype(int).astype(str).str[:-4]\n",
        "  year_mapping = {'A': 2001, 'B': 2002, 'C': 2003, 'D': 2004, 'E': 2005, 'F': 2006}\n",
        "  data['new_year'] = data['surface_tp_train.year'].map(year_mapping)\n",
        "  data['mm'] = data['mm'].apply(lambda x: x.zfill(2))\n",
        "  data['date'] = pd.to_datetime(data['new_year'].astype(str) + data['mm'] + data['dd'] + data['hh'], format='%Y%m%d%H')\n",
        "\n",
        "  spring_month_list = ['02', '03', '04']\n",
        "  summer_month_list = ['05', '06', '07']\n",
        "  fall_month_list = ['08', '09', '10']\n",
        "  winter_month_list = ['11', '12', '01']\n",
        "\n",
        "  if season == \"all\":\n",
        "    pass\n",
        "  elif season == \"spring\":\n",
        "    filtered_data = data[data['mm'].isin(spring_month_list)]\n",
        "  elif season == \"summer\":\n",
        "    filtered_data = data[data['mm'].isin(summer_month_list)]\n",
        "  elif season == \"fall\":\n",
        "    filtered_data = data[data['mm'].isin(fall_month_list)]\n",
        "  elif season == \"winter\":\n",
        "    filtered_data = data[data['mm'].isin(winter_month_list)]\n",
        "  else:\n",
        "    raise ValueError(\"not season value\")\n",
        "\n",
        "  # data = filtered_data.drop([\"surface_tp_train.year\", 'surface_tp_train.mmddhh', 'hh', 'dd', 'mm', 'new_year'], axis = 1)\n",
        "  data = filtered_data.drop([\"surface_tp_train.year\", 'surface_tp_train.mmddhh'], axis = 1)\n",
        "  # data[\"datetime\"] = pd.to_datetime(data[\"date\"])  # 문자열을 날짜/시간 유형으로 변환\n",
        "  ## datetime 순으로 index 생성\n",
        "  data.reset_index(inplace=True)\n",
        "  data.index = data.index.astype(int)\n",
        "  return data\n",
        "\n",
        "### 지역 별로 데이터를 쪼개주는 함수\n",
        "\n",
        "def region_category(data, region_number): # region_number는 자기가 알아서 넣기로\n",
        "\n",
        "  # 주어진 열을 기준으로 데이터 프레임을 필터링하고 값 종류별로 데이터를 쪼갬\n",
        "  unique_values = data['surface_tp_train.stn'].unique()  # 주어진 열의 고유한 값들을 추출\n",
        "\n",
        "  if region_number in unique_values:\n",
        "    result = data[data['surface_tp_train.stn'] == region_number]  # 주어진 값에 해당하는 데이터만 필터링하여 저장\n",
        "  else:\n",
        "    raise ValueError(\"not in region number\")\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "def change_columns(data):\n",
        "  data.columns = data.columns.str.replace(\".\", \"_\")\n",
        "\n",
        "\n",
        "def change_to_categrory(data, columns_name):\n",
        "  data[columns_name] = data[columns_name].astype(str).astype('category')\n",
        "  return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbJhBExZSotb",
        "outputId": "55c492dd-b84a-4b91-c887-e94dd87da70d"
      },
      "outputs": [],
      "source": [
        "result_train_spring = adding_date(result_train, \"spring\")\n",
        "result_train_spring.columns\n",
        "\n",
        "\n",
        "## region 별로 나누기\n",
        "train_region_1_spring = region_category(result_train_spring, 1)\n",
        "train_region_2_spring = region_category(result_train_spring, 2)\n",
        "train_region_3_spring = region_category(result_train_spring, 3)\n",
        "train_region_4_spring = region_category(result_train_spring, 4)\n",
        "train_region_5_spring = region_category(result_train_spring, 5)\n",
        "train_region_6_spring = region_category(result_train_spring, 6)\n",
        "train_region_7_spring = region_category(result_train_spring, 7)\n",
        "train_region_8_spring = region_category(result_train_spring, 8)\n",
        "train_region_9_spring = region_category(result_train_spring, 8)\n",
        "train_region_10_spring = region_category(result_train_spring, 10)\n",
        "\n",
        "change_columns(result_train_spring)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTk9fYEwcpx9",
        "outputId": "d7ebc64a-d96b-4395-f86c-c9cf60db6bc1"
      },
      "outputs": [],
      "source": [
        "list_colunms = [\"surface_tp_train_stn\", \"new_year\", \"mm\", \"dd\", \"hh\",\"surface_tp_train_ww\"]\n",
        "\n",
        "# train_region_1_spring = change_to_categrory(train_region_1_spring, \"surface_tp_train_stn\")\n",
        "# train_region_1_spring = change_to_categrory(train_region_1_spring, \"new_year\")\n",
        "# train_region_1_spring = change_to_categrory(train_region_1_spring, \"mm\")\n",
        "# train_region_1_spring = change_to_categrory(train_region_1_spring, \"dd\")\n",
        "# train_region_1_spring = change_to_categrory(train_region_1_spring, \"hh\")\n",
        "# train_region_1_spring = change_to_categrory(train_region_1_spring, \"hh\")\n",
        "# train_region_1_spring = change_to_categrory(train_region_1_spring, \"surface_tp_train_ww\")\n",
        "\n",
        "result_train_spring = change_to_categrory(result_train_spring, \"surface_tp_train_stn\")\n",
        "result_train_spring = change_to_categrory(result_train_spring, \"new_year\")\n",
        "result_train_spring = change_to_categrory(result_train_spring, \"mm\")\n",
        "result_train_spring = change_to_categrory(result_train_spring, \"dd\")\n",
        "result_train_spring = change_to_categrory(result_train_spring, \"hh\")\n",
        "result_train_spring = change_to_categrory(result_train_spring, \"hh\")\n",
        "result_train_spring = change_to_categrory(result_train_spring, \"surface_tp_train_ww\")\n",
        "\n",
        "\n",
        "\n",
        "## column 이름에 . 제거\n",
        "change_columns(result_train_spring)\n",
        "result_train_spring = result_train_spring.drop(\"index\", axis = 1)\n",
        "result_train_spring.info()\n",
        "\n",
        "## sort 하고 타임 index 생성\n",
        "result_train_spring.sort_values(\"date\", ascending = True, inplace = True)\n",
        "result_train_spring['time_idx'] = range(len(result_train_spring))\n",
        "\n",
        "result_train_spring.head(5)\n",
        "result_train_spring.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbnZkqDLQA7I"
      },
      "outputs": [],
      "source": [
        "result_train_spring = result_train_spring.drop(\"date\", axis = 1)\n",
        "result_train_spring = result_train_spring.drop(\"new_year\", axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1Kg7nfzG0CB"
      },
      "source": [
        "## Temporal Fusion Transformer 모델링 진행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4470WO3h8UwK",
        "outputId": "068e17c4-bb40-445f-f931-854fa7d7027f"
      },
      "outputs": [],
      "source": [
        "## Temporal Fusion Transformer(TFT) 모델 다운로드\n",
        "! pip install pytorch_forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kxd7VvR1z7aM",
        "outputId": "1c60b36b-203e-471d-f51a-c95720ac88a2"
      },
      "outputs": [],
      "source": [
        "\n",
        "new_directory = \"/content/drive/MyDrive/기상청 (1)\"\n",
        "os.chdir(new_directory)\n",
        "root_directory = os.getcwd()\n",
        "root_directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_TpkmCLKDMY"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ## Temporal Fusion Transformer(TFT) 모델\n",
        "\n",
        "\n",
        "# #경로만 자신의 환경에 맞게 잘 설정해주세요!\n",
        "# DATAROOT= root_directory + '/tft'\n",
        "# CKPTROOT = DATAROOT+\"/batch_128/ckpts\" # directory for model checkpoints\n",
        "# CSVROOT = DATAROOT+\"/batch_128/csvs\" # directory for prediction outputs\n",
        "# SUBFN = DATAROOT+\"/batch_128/sub.csv\" # final submission file path\n",
        "# LOGDIR = DATAROOT+\"/batch_128/logs\" # pytorch_forecasting requirs logger\n",
        "\n",
        "# import sys\n",
        "# import os\n",
        "# import argparse\n",
        "# import shutil\n",
        "# import random\n",
        "# from pathlib import Path\n",
        "\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# import lightning.pytorch as pl\n",
        "\n",
        "# # imports for training\n",
        "# # import lightning.pytorch as pl\n",
        "\n",
        "# from lightning.pytorch.loggers import TensorBoardLogger\n",
        "\n",
        "# from pytorch_forecasting.data import (\n",
        "#     TimeSeriesDataSet,\n",
        "#     # GroupNormalizer\n",
        "# )\n",
        "# from lightning.pytorch.callbacks import (\n",
        "#     ModelCheckpoint,\n",
        "#     EarlyStopping,\n",
        "#     LearningRateMonitor\n",
        "# )\n",
        "\n",
        "# # import dataset, network to train and metric to optimize\n",
        "# from pytorch_forecasting import (\n",
        "#     TimeSeriesDataSet,\n",
        "#     TemporalFusionTransformer,\n",
        "#     QuantileLoss\n",
        "# )\n",
        "\n",
        "# from pytorch_forecasting.metrics import SMAPE\n",
        "# from pytorch_forecasting.models import TemporalFusionTransformer\n",
        "\n",
        "# from lightning.pytorch.tuner import Tuner\n",
        "\n",
        "\n",
        "\n",
        "# # number of epochs found in cv run\n",
        "# NUM_EPOCHS = 200\n",
        "\n",
        "# # number of seeds to use\n",
        "# NUM_SEEDS = 10\n",
        "\n",
        "# # batch_size\n",
        "# BATCH_SIZE = 64\n",
        "\n",
        "\n",
        "# # learning rate determined by a cv run with train data less 1 trailing week as validation\n",
        "# LRS = [0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "#        0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "#        0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "#        0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "#        0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "#        0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "#        0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "#        0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "#        0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "#        0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "#        0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "#        0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "#        0.05099279397234306 , 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "#        0.005099279397234306, 0.005099279397234306, 0.005099279397234306, 0.005099279397234306,\n",
        "#        0.005099279397234306, 0.005099279397234306, 0.005099279397234306, 0.005099279397234306,\n",
        "#        0.005099279397234306, 0.0005099279397234307, 0.0005099279397234307, 0.0005099279397234307,\n",
        "#        0.0005099279397234307, 0.0005099279397234307, 0.0005099279397234307]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # hyper parameters determined by cv runs with train data less 1 trailing week as validation\n",
        "# PARAMS = {\n",
        "#     'gradient_clip_val': 0.9658579636307634,\n",
        "#     'hidden_size': 100,\n",
        "#     'dropout': 0.19610151695402608,\n",
        "#     'hidden_continuous_size': 90,\n",
        "#     'attention_head_size': 4,\n",
        "#     'learning_rate': 0.08\n",
        "# }\n",
        "\n",
        "\n",
        "# def seed_all(seed):\n",
        "#     random.seed(seed)\n",
        "#     torch.manual_seed(seed)\n",
        "#     torch.cuda.manual_seed_all(seed)\n",
        "#     np.random.seed(seed)\n",
        "#     os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "#     torch.backends.cudnn.deterministic = True\n",
        "#     torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "\n",
        "# def load_dataset(train_df, validate=False):\n",
        "#     ENCODER_LENGTH_IN_WEEKS = 5\n",
        "#     max_encoder_length = 24 * 7 *ENCODER_LENGTH_IN_WEEKS #5\n",
        "#     max_prediction_length = 24 * 7\n",
        "#     training_cutoff = train_df['time_idx'].max()-max_prediction_length #2040 - 24*7 = 1871\n",
        "\n",
        "#     tr_ds = TimeSeriesDataSet(\n",
        "#       train_df[lambda x: x.time_idx <=training_cutoff] if validate else train_df,\n",
        "#       time_idx = \"time_idx\",\n",
        "#       target = \"surface_tp_train_ts\", ## 예측값\n",
        "#       group_ids=[\"surface_tp_train_stn\"],\n",
        "#       min_encoder_length = 1,\n",
        "#       max_encoder_length = max_encoder_length,\n",
        "#       min_prediction_length=1,\n",
        "#       max_prediction_length=max_prediction_length,\n",
        "\n",
        "#       #Known Inputs 알고 있는 변수\n",
        "#       time_varying_known_categoricals = ['surface_tp_train_ww',\n",
        "#                                          'hh',\n",
        "#                                          'dd',\n",
        "#                                          'mm',\n",
        "#                                          'new_year'],\n",
        "#       time_varying_known_reals = [\n",
        "#         'surface_tp_train_ta',\n",
        "#         'surface_tp_train_td',\n",
        "#         'surface_tp_train_hm',\n",
        "#         'surface_tp_train_ws',\n",
        "#         'surface_tp_train_rn',\n",
        "#         'surface_tp_train_re',\n",
        "#         'time_idx'\n",
        "#         ],\n",
        "\n",
        "#       # target_normalizer=GroupNormalizer(groups=[\"surface_tp_train_stn\"], transformation=\"softplus\"),\n",
        "#       #모르고 있는 변수\n",
        "#       time_varying_unknown_categoricals=[],\n",
        "#       time_varying_unknown_reals=[\n",
        "#           \"surface_tp_train_ts\"\n",
        "#         ],\n",
        "#         add_relative_time_idx=True,  # add as feature\n",
        "#         add_target_scales=True,  # add as feature\n",
        "#         add_encoder_length=True,  # add as feature\n",
        "#     )\n",
        "\n",
        "\n",
        "#     va_ds = None\n",
        "#     if validate:\n",
        "#         va_ds = TimeSeriesDataSet.from_dataset(\n",
        "#         tr_ds, train_df, predict=True, stop_randomization=True\n",
        "#     )\n",
        "\n",
        "#     return tr_ds, va_ds\n",
        "\n",
        "# # training\n",
        "# def fit(seed, tr_ds, va_loader=None):\n",
        "#     seed_all(seed) # doesn't really work as training is non-deterministic\n",
        "\n",
        "#     # create dataloaders for model\n",
        "#     tr_loader = tr_ds.to_dataloader(\n",
        "#         train=True, batch_size=BATCH_SIZE, num_workers=12\n",
        "#     )\n",
        "#     if va_loader is not None:\n",
        "#         # stop training, when loss metric does not improve on validation set\n",
        "#         early_stopping_callback = EarlyStopping(\n",
        "#             monitor=\"val_loss\",\n",
        "#             min_delta=1e-4,\n",
        "#             patience=20,\n",
        "#             verbose=True,\n",
        "#             mode=\"min\"\n",
        "#         )\n",
        "#         lr_logger = LearningRateMonitor(logging_interval=\"epoch\")  # log the learning rate\n",
        "#         callbacks = [lr_logger, early_stopping_callback]\n",
        "#     else:\n",
        "#         # gather 10 checkpoints with best traing loss\n",
        "#         checkpoint_callback = ModelCheckpoint(\n",
        "#             monitor='train_loss',\n",
        "#             dirpath=CKPTROOT,\n",
        "#             filename=f'seed={seed}'+'-{epoch:03d}-{train_loss:.2f}',\n",
        "#             save_top_k=10\n",
        "#         )\n",
        "#         callbacks = [checkpoint_callback]\n",
        "\n",
        "#     # create trainer\n",
        "#     trainer = pl.Trainer(\n",
        "#         max_epochs=66,\n",
        "#         gradient_clip_val=PARAMS['gradient_clip_val'],\n",
        "#         limit_train_batches=100,\n",
        "#         callbacks=callbacks,\n",
        "#         logger=TensorBoardLogger(LOGDIR),\n",
        "#         # train_dataloaders = tr_loader\n",
        "#         accelerator=\"gpu\",\n",
        "#     )\n",
        "\n",
        "#     # use pre-deterined leraning rate schedule for final submission\n",
        "#     learning_rate = LRS if va_loader is None else PARAMS['learning_rate']\n",
        "\n",
        "#     # initialise model with pre-determined hyperparameters\n",
        "#     tft = TemporalFusionTransformer.from_dataset(\n",
        "#         tr_ds,\n",
        "#         learning_rate=learning_rate,\n",
        "#         hidden_size=PARAMS['hidden_size'],\n",
        "#         attention_head_size=PARAMS['attention_head_size'],\n",
        "#         dropout=PARAMS['dropout'],\n",
        "#         hidden_continuous_size=PARAMS['hidden_continuous_size'],\n",
        "#         output_size=1,\n",
        "#         loss=SMAPE(), # SMAPE loss\n",
        "#         log_interval=10,  # log example every 10 batches\n",
        "#         logging_metrics=[SMAPE()],\n",
        "#         reduce_on_plateau_patience=4,  # reduce learning automatically\n",
        "#     )\n",
        "#     print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
        "#     tft = tft.to('cuda:0')\n",
        "\n",
        "#     # kwargs = {'train_dataloaders': tr_loader}\n",
        "#     # if va_loader:\n",
        "#     #     kwargs['val_dataloaders'] = va_loader\n",
        "\n",
        "#     # fit network\n",
        "#     trainer.fit(\n",
        "#         tft,\n",
        "#         train_dataloaders = tr_loader,\n",
        "#         val_dataloaders = va_loader,\n",
        "#     )\n",
        "\n",
        "#     best_model_path = trainer.checkpoint_callback.best_model_path\n",
        "#     print(f'best_model_path={best_model_path}')\n",
        "#     best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
        "\n",
        "#     return best_tft\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ucKKwDSjIcrT",
        "outputId": "afbe87a3-3d0d-4fb3-8e05-b9fe97c0eee0"
      },
      "outputs": [],
      "source": [
        "train_region_1_spring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSwv94Vih_JL",
        "outputId": "8d8dc04b-d414-4158-892a-c9fbefa9bdcc"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaSAzVJImpgZ"
      },
      "outputs": [],
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJN6YmVdqNso"
      },
      "outputs": [],
      "source": [
        "# predict 1 week\n",
        "def forecast(ckpt, train_df, test_df):\n",
        "    # load model\n",
        "    best_tft = TemporalFusionTransformer.load_from_checkpoint(ckpt)\n",
        "    max_encoder_length = best_tft.dataset_parameters['max_encoder_length']\n",
        "    max_prediction_length = best_tft.dataset_parameters['max_prediction_length']\n",
        "\n",
        "    assert max_encoder_length == 5*24*7 and max_prediction_length == 1*24*7\n",
        "\n",
        "    # use 5 weeks of training data at the end\n",
        "    encoder_data = train_df[lambda x: x.time_idx > x.time_idx.max() - max_encoder_length]\n",
        "\n",
        "    # get last entry from training data\n",
        "    last_data = train_df.iloc[[-1]]\n",
        "\n",
        "    # fill NA target value in test data with last values from the train dataset\n",
        "    target_cols = [c for c in test_df.columns if 'target' in c]\n",
        "    for c in target_cols:\n",
        "        test_df.loc[:, c] = last_data[c].item()\n",
        "\n",
        "    decoder_data = test_df\n",
        "\n",
        "    # combine encoder and decoder data. decoder data is to be predicted\n",
        "    new_prediction_data = pd.concat([encoder_data, decoder_data], ignore_index=True)\n",
        "    new_raw_predictions, new_x = best_tft.predict(new_prediction_data, mode=\"raw\", return_x=True)\n",
        "\n",
        "    # num_labels: mapping from 'num' categorical feature to index in new_raw_predictions['prediction']\n",
        "    #             {'5': 4, '6': 6, ...}\n",
        "    # new_raw_predictions['prediction'].shape = (60, 168, 1)\n",
        "    num_labels = best_tft.dataset_parameters['categorical_encoders']['num'].classes_\n",
        "\n",
        "    preds = new_raw_predictions['prediction'].squeeze()\n",
        "\n",
        "    sub_df = pd.read_csv(DATAROOT+\"/sample_submission.csv\")\n",
        "\n",
        "    # get prediction for each building (num)\n",
        "    for n, ix in num_labels.items():\n",
        "        sub_df.loc[sub_df.num_date_time.str.startswith(f\"{n} \"), 'answer'] = preds[ix].numpy()\n",
        "\n",
        "    # save predction to a csv file\n",
        "    outfn = CSVROOT+'/'+(Path(ckpt).stem + '.csv')\n",
        "    print(outfn)\n",
        "    sub_df.to_csv(outfn, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "bsn06Zimqbdn",
        "outputId": "8dcbd1ff-4b95-4b04-9d95-50359c4d7d81"
      },
      "outputs": [],
      "source": [
        "# def ensemble(outfn):\n",
        "#     # get all prediction csv files\n",
        "#     fns = list(glob.glob(CSVROOT+\"/*.csv\"))\n",
        "#     df0 = pd.read_csv(fns[0])\n",
        "#     df = pd.concat([df0] + [pd.read_csv(fn).loc[:,'answer'] for fn in fns[1:]], axis=1)\n",
        "#     # get median of all predcitions\n",
        "#     df['median'] = df.iloc[:,1:].median(axis=1)\n",
        "#     df = df[['num_date_time', 'median']]\n",
        "#     df = df.rename({'median': 'answer'}, axis=1)\n",
        "#     # save to submission file\n",
        "#     df.to_csv(outfn, index=False)\n",
        "\n",
        "# # not used for final submission\n",
        "# def validate(seed, tr_ds, va_ds):\n",
        "#     va_loader = va_ds.to_dataloader(\n",
        "#         train=False, batch_size=BATCH_SIZE*10, num_workers=12\n",
        "#     )\n",
        "#     best_tft = fit(seed, tr_ds, va_loader)\n",
        "#     actuals = torch.cat([y[0] for x, y in iter(va_loader)])\n",
        "#     predictions = best_tft.predict(va_loader)\n",
        "#     smape_per_num = SMAPE(reduction=\"none\")(predictions, actuals).mean(1)\n",
        "#     print(smape_per_num)\n",
        "#     print(smape_per_num.mean())\n",
        "\n",
        "\n",
        "\n",
        "# import glob\n",
        "# print(\"### FORECAST ###\")\n",
        "# for p in glob.glob(CKPTROOT + \"/*.ckpt\"):\n",
        "#     forecast(p, train_df, test_df)\n",
        "\n",
        "\n",
        "# print(\"### ENSEMBLING ###\")\n",
        "# ensemble(SUBFN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "k8FnfqHKYaRJ",
        "outputId": "8458159a-4f08-4258-f328-3d66562292db"
      },
      "outputs": [],
      "source": [
        "## 정규화\n",
        "# 스케일링을 하자\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def scale_columns(data):\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = data.copy()\n",
        "    numeric_columns = scaled_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "    scaled_data[numeric_columns] = scaler.fit_transform(scaled_data[numeric_columns])\n",
        "    return scaled_data\n",
        "\n",
        "# 스케일링을 하자\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "def robust_scaling(data):\n",
        "    rbs = RobustScaler()\n",
        "    scaled_data = data.copy()\n",
        "    numeric_columns = ['surface_tp_train_ta',   'surface_tp_train_td',   'surface_tp_train_hm',   'surface_tp_train_ws',   'surface_tp_train_rn',   'surface_tp_train_re']\n",
        "    scaled_data[numeric_columns] = rbs.fit_transform(scaled_data[numeric_columns])\n",
        "    data[numeric_columns] = rbs.fit_transform(data[numeric_columns])\n",
        "    return data\n",
        "\n",
        "robust_scaling(result_train_spring)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QslZb9SXxkqZ"
      },
      "outputs": [],
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAcgP9SFymnj",
        "outputId": "fcfe3770-17bb-4840-8ee7-0cf02a9faaf1"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "77401be34c5f497f8d82489a1110aa77",
            "26a09ff796074c76a6d6c6751eaf8a73",
            "d17f77d0dcaa4b29808b369c2e911eac",
            "00eb0bc3d333436d846a3fd23b236916",
            "fd45814848ff4c488b85fd32ed65b484",
            "405525eec8c146b187fa8625429ee6a1",
            "01f33acdd9fc453289b19813e8ddaa60",
            "829459a7302a41908eebc6966e07b711",
            "29e4374108304d8e8b8c62ca88d28d7b",
            "b392f2bf7e084083b570a4b3b9fff211",
            "a1224c98af8d4abb90cf70ef74ba1c40"
          ]
        },
        "id": "PPggBr3zHSX0",
        "outputId": "69091ee6-acfa-47cc-8fb6-16d7e2d74544"
      },
      "outputs": [],
      "source": [
        "## Temporal Fusion Transformer(TFT) 모델\n",
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "#경로만 자신의 환경에 맞게 잘 설정해주세요!\n",
        "\n",
        "## epoch = 40\n",
        "DATAROOT= root_directory + '/tft/hidden_160/spring/final/epoch_40'\n",
        "CKPTROOT = DATAROOT+\"/batch_64/ckpts\" # directory for model checkpoints\n",
        "CSVROOT = DATAROOT+\"/batch_64/csvs\" # directory for prediction outputs\n",
        "SUBFN = DATAROOT+\"/batch_64/sub.csv\" # final submission file path\n",
        "LOGDIR = DATAROOT+\"/batch_64/logs\" # pytorch_forecasting requirs logger\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "import shutil\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import lightning.pytorch as pl\n",
        "import torchmetrics\n",
        "\n",
        "from lightning.pytorch.callbacks import Callback\n",
        "\n",
        "\n",
        "\n",
        "# imports for training\n",
        "# import lightning.pytorch as pl\n",
        "\n",
        "from lightning.pytorch.loggers import TensorBoardLogger\n",
        "\n",
        "from pytorch_forecasting.data import (\n",
        "    TimeSeriesDataSet,\n",
        "    # GroupNormalizer\n",
        ")\n",
        "from lightning.pytorch.callbacks import (\n",
        "    ModelCheckpoint,\n",
        "    EarlyStopping,\n",
        "    LearningRateMonitor\n",
        ")\n",
        "\n",
        "# import dataset, network to train and metric to optimize\n",
        "from pytorch_forecasting import (\n",
        "    TimeSeriesDataSet,\n",
        "    TemporalFusionTransformer,\n",
        "    QuantileLoss\n",
        ")\n",
        "\n",
        "from pytorch_forecasting.metrics import SMAPE\n",
        "from pytorch_forecasting.models import TemporalFusionTransformer\n",
        "\n",
        "from lightning.pytorch.tuner import Tuner\n",
        "\n",
        "from pytorch_forecasting.metrics import MAE\n",
        "\n",
        "\n",
        "# number of epochs found in cv run\n",
        "NUM_EPOCHS = 40\n",
        "\n",
        "# number of seeds to use\n",
        "NUM_SEEDS = 10\n",
        "\n",
        "# batch_size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# learning rate determined by a cv run with train data less 1 trailing week as validation\n",
        "LRS = [0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306 , 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.005099279397234306, 0.005099279397234306, 0.005099279397234306, 0.005099279397234306,\n",
        "       0.005099279397234306, 0.005099279397234306, 0.005099279397234306, 0.005099279397234306,\n",
        "       0.005099279397234306, 0.0005099279397234307, 0.0005099279397234307, 0.0005099279397234307,\n",
        "       0.0005099279397234307, 0.0005099279397234307, 0.0005099279397234307]\n",
        "\n",
        "# hyper parameters determined by cv runs with train data less 1 trailing week as validation\n",
        "\n",
        "## spring\n",
        "PARAMS = {\n",
        "    'gradient_clip_val': 0.9658579636307634,\n",
        "    'hidden_size': 160,\n",
        "    'dropout': 0.15610151695402608,\n",
        "    'hidden_continuous_size': 90,\n",
        "    'attention_head_size': 4,\n",
        "    'learning_rate': 0.08\n",
        "}\n",
        "\n",
        "# ### summer\n",
        "\n",
        "# PARAMS = {\n",
        "#     'gradient_clip_val': 0.9658579636307634,\n",
        "#     'hidden_size': 96,\n",
        "#     'dropout': 0.20610151695402608,\n",
        "#     'hidden_continuous_size': 90,\n",
        "#     'attention_head_size': 4,\n",
        "#     'learning_rate': 0.03\n",
        "# }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def seed_all(seed):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def load_dataset(train_df, validate=False):\n",
        "    # Code omitted for brevity\n",
        "\n",
        "    ENCODER_LENGTH_IN_WEEKS = 10\n",
        "    max_encoder_length = 24 * 7 *ENCODER_LENGTH_IN_WEEKS #10\n",
        "    max_prediction_length = 6153\n",
        "    training_cutoff = train_df['time_idx'].max()-max_prediction_length #2040 - 24*7 = 1871\n",
        "\n",
        "    tr_ds = TimeSeriesDataSet(\n",
        "      train_df[lambda x: x.time_idx <=training_cutoff] if validate else train_df,\n",
        "      time_idx = \"time_idx\",\n",
        "      target = \"surface_tp_train_ts\", ## 예측값\n",
        "      group_ids=[\"surface_tp_train_stn\"],\n",
        "      min_encoder_length = 1,\n",
        "      max_encoder_length = max_encoder_length,\n",
        "      min_prediction_length=1,\n",
        "      max_prediction_length=max_prediction_length,\n",
        "      allow_missing_timesteps=True,  # 시간 간격이 1보다 큰 경우 허용\n",
        "\n",
        "\n",
        "      #Known Inputs 알고 있는 변수\n",
        "      time_varying_known_categoricals = ['surface_tp_train_ww',\n",
        "                                         'hh',\n",
        "                                         'dd',\n",
        "                                         'mm'\n",
        "                                         ],\n",
        "      time_varying_known_reals = [\n",
        "        'surface_tp_train_ta',\n",
        "        'surface_tp_train_td',\n",
        "        'surface_tp_train_hm',\n",
        "        'surface_tp_train_ws',\n",
        "        'surface_tp_train_rn',\n",
        "        'surface_tp_train_re',\n",
        "        'time_idx'\n",
        "        ],\n",
        "\n",
        "      # target_normalizer=GroupNormalizer(groups=[\"surface_tp_train_stn\"], transformation=\"softplus\"),\n",
        "      #모르고 있는 변수\n",
        "      time_varying_unknown_categoricals=[],\n",
        "      time_varying_unknown_reals=[\n",
        "          \"surface_tp_train_ts\"\n",
        "        ],\n",
        "        add_relative_time_idx=True,  # add as feature\n",
        "        add_target_scales=True,  # add as feature\n",
        "        add_encoder_length=True,  # add as feature\n",
        "    )\n",
        "\n",
        "    va_ds = None\n",
        "    if validate:\n",
        "        va_ds = TimeSeriesDataSet.from_dataset(\n",
        "        tr_ds, train_df, predict=True, stop_randomization=True\n",
        "    )\n",
        "    return tr_ds, va_ds\n",
        "\n",
        "train_losses = []  # List to store training losses\n",
        "val_losses = []  # List to store validation losses\n",
        "def fit(seed, tr_ds, va_loader=None):\n",
        "    global train_losses, val_losses\n",
        "    seed_all(seed)\n",
        "    ##  cache 정리\n",
        "    # gc.collect()\n",
        "    # torch.cuda.empty_cache()\n",
        "\n",
        "    # Code omitted for brevity\n",
        "    # training\n",
        "    tr_loader = tr_ds.to_dataloader(\n",
        "        train=True,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=4,\n",
        "    )\n",
        "    ### loss 찍기 추가\n",
        "    if va_loader is not None:\n",
        "        # stop training, when loss metric does not improve on validation set\n",
        "\n",
        "        va_loader = va_loader.to_dataloader(\n",
        "            train=False,\n",
        "            batch_size=BATCH_SIZE*5,\n",
        "            num_workers=4,\n",
        "            shuffle = False,\n",
        "        )\n",
        "        checkpoint_callback = ModelCheckpoint(\n",
        "            monitor='val_loss',\n",
        "            dirpath=CKPTROOT,\n",
        "            filename=f'seed={seed}'+'-{epoch:03d}-{train_loss:.2f}',\n",
        "            save_top_k=10\n",
        "        )\n",
        "        early_stopping_callback = EarlyStopping(\n",
        "            monitor=\"val_loss\",\n",
        "            min_delta=3e-3,\n",
        "            patience=20,\n",
        "            verbose=True,\n",
        "            mode=\"min\"\n",
        "        )\n",
        "        lr_logger = LearningRateMonitor(logging_interval=\"epoch\")  # log the learning rate\n",
        "        callbacks = [lr_logger, early_stopping_callback, checkpoint_callback]\n",
        "    else:\n",
        "        # gather 10 checkpoints with best traing loss\n",
        "        checkpoint_callback = ModelCheckpoint(\n",
        "            monitor='train_loss',\n",
        "            dirpath=CKPTROOT,\n",
        "            filename=f'seed={seed}'+'-{epoch:03d}-{train_loss:.2f}',\n",
        "            save_top_k=10\n",
        "        )\n",
        "        callbacks = [checkpoint_callback]\n",
        "\n",
        "    # create trainer\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=100,\n",
        "        gradient_clip_val=PARAMS['gradient_clip_val'],\n",
        "        limit_train_batches=100,\n",
        "        callbacks=callbacks,\n",
        "        logger=TensorBoardLogger(LOGDIR),\n",
        "        # train_dataloaders = tr_loader\n",
        "        accelerator=\"gpu\",\n",
        "    )\n",
        "\n",
        "    # use pre-deterined leraning rate schedule for final submission\n",
        "    learning_rate = LRS if va_loader is None else PARAMS['learning_rate']\n",
        "\n",
        "    # initialise model with pre-determined hyperparameters\n",
        "    tft = TemporalFusionTransformer.from_dataset(\n",
        "        tr_ds,\n",
        "        learning_rate=learning_rate,\n",
        "        hidden_size=PARAMS['hidden_size'],\n",
        "        attention_head_size=PARAMS['attention_head_size'],\n",
        "        dropout=PARAMS['dropout'],\n",
        "        hidden_continuous_size=PARAMS['hidden_continuous_size'],\n",
        "        output_size=1,\n",
        "        loss=MAE(),  # MAE loss\n",
        "        log_interval=10,  # log example every 10 batches\n",
        "        logging_metrics=[MAE()],  # MAE metric\n",
        "        reduce_on_plateau_patience=4,  # reduce learning automatically\n",
        "    )\n",
        "    print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
        "    tft = tft.to('cuda:0')\n",
        "\n",
        "    # kwargs = {'train_dataloaders': tr_loader}\n",
        "    # if va_loader:\n",
        "    #     kwargs['val_dataloaders'] = va_loader\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    # fit network\n",
        "    trainer.fit(\n",
        "        tft,\n",
        "        train_dataloaders=tr_loader,\n",
        "        val_dataloaders=va_loader,\n",
        "    )\n",
        "\n",
        "    if va_loader is not None:\n",
        "      for output in trainer.callback_metrics:\n",
        "          if output.startswith('train_loss'):\n",
        "              train_loss = trainer.callback_metrics[output].item()\n",
        "              if train_loss not in train_losses:\n",
        "                train_losses.append(train_loss)\n",
        "          elif output.startswith('val_loss'):\n",
        "              val_loss = trainer.callback_metrics[output].item()\n",
        "              val_losses.append(val_loss)\n",
        "\n",
        "    best_model_path = trainer.checkpoint_callback.best_model_path\n",
        "    print(f'best_model_path={best_model_path}')\n",
        "    best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
        "    if va_loader is not None:\n",
        "        # train_loss = trainer.callback_metrics['train_loss'].item()\n",
        "        # val_loss = trainer.callback_metrics['val_loss'].item()\n",
        "        print(f'Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
        "        return best_tft, train_loss, val_loss\n",
        "    else:\n",
        "        return best_tft\n",
        "\n",
        "\n",
        "def main(train_df):\n",
        "\n",
        "    # load datasets\n",
        "    tr_ds, va_ds = load_dataset(train_df, validate=True)\n",
        "\n",
        "    # fit the model\n",
        "    s = 10\n",
        "    model, loss_train, loss_values = fit(s, tr_ds, va_ds)\n",
        "\n",
        "    return model, loss_train, loss_values\n",
        "\n",
        "# # call the main function\n",
        "# train_df = result_train_spring\n",
        "# model, loss_train, loss_values = main(train_df)\n",
        "\n",
        "\n",
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "    # Define the hyperparameter search space\n",
        "    params = {\n",
        "        'gradient_clip_val': trial.suggest_loguniform('gradient_clip_val', 1e-4, 1.0),\n",
        "        'hidden_size': trial.suggest_int('hidden_size', 50, 200),\n",
        "        'dropout': trial.suggest_uniform('dropout', 0.1, 0.5),\n",
        "        'hidden_continuous_size': trial.suggest_int('hidden_continuous_size', 50, 200),\n",
        "        'attention_head_size': trial.suggest_int('attention_head_size', 1, 8),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1.0)\n",
        "    }\n",
        "\n",
        "    # Set the parameters for training\n",
        "    PARAMS = params\n",
        "\n",
        "    train_df = result_train_spring\n",
        "    # Call the main function with the updated parameters\n",
        "    model, loss_train, loss_values = main(train_df)\n",
        "\n",
        "    # Return the validation loss as the objective value to be minimized\n",
        "    print(loss_train)\n",
        "\n",
        "\n",
        "# Create a study object and optimize the objective function\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Get the best parameters and objective value\n",
        "best_params = study.best_params\n",
        "best_value = study.best_value\n",
        "\n",
        "print('Best Parameters:', best_params)\n",
        "print('Best Objective Value:', best_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dK6NhH8fYkcZ"
      },
      "outputs": [],
      "source": [
        "def objective():\n",
        "    # Define the hyperparameter search space\n",
        "    PARAMS = {\n",
        "        'gradient_clip_val': 0.9658579636307634,\n",
        "        'hidden_size': 100,\n",
        "        'dropout': 0.20610151695402608,\n",
        "        'hidden_continuous_size': 90,\n",
        "        'attention_head_size': 4,\n",
        "        'learning_rate': 0.03\n",
        "    }\n",
        "\n",
        "    # Set the parameters for training\n",
        "    # PARAMS = params\n",
        "\n",
        "    train_df = result_train_spring\n",
        "    # Call the main function with the updated parameters\n",
        "    model, loss_train, loss_values = main(train_df)\n",
        "\n",
        "    # Return the validation loss as the objective value to be minimized\n",
        "    print(loss_train)\n",
        "\n",
        "\n",
        "# Create a study object and optimize the objective function\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Get the best parameters and objective value\n",
        "best_params = study.best_params\n",
        "best_value = study.best_value\n",
        "\n",
        "print('Best Parameters:', best_params)\n",
        "print('Best Objective Value:', best_value)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00eb0bc3d333436d846a3fd23b236916": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b392f2bf7e084083b570a4b3b9fff211",
            "placeholder": "​",
            "style": "IPY_MODEL_a1224c98af8d4abb90cf70ef74ba1c40",
            "value": " 0/1 [00:00&lt;?, ?it/s]"
          }
        },
        "01f33acdd9fc453289b19813e8ddaa60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26a09ff796074c76a6d6c6751eaf8a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_405525eec8c146b187fa8625429ee6a1",
            "placeholder": "​",
            "style": "IPY_MODEL_01f33acdd9fc453289b19813e8ddaa60",
            "value": "Sanity Checking DataLoader 0:   0%"
          }
        },
        "29e4374108304d8e8b8c62ca88d28d7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "405525eec8c146b187fa8625429ee6a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77401be34c5f497f8d82489a1110aa77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26a09ff796074c76a6d6c6751eaf8a73",
              "IPY_MODEL_d17f77d0dcaa4b29808b369c2e911eac",
              "IPY_MODEL_00eb0bc3d333436d846a3fd23b236916"
            ],
            "layout": "IPY_MODEL_fd45814848ff4c488b85fd32ed65b484"
          }
        },
        "829459a7302a41908eebc6966e07b711": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1224c98af8d4abb90cf70ef74ba1c40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b392f2bf7e084083b570a4b3b9fff211": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d17f77d0dcaa4b29808b369c2e911eac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_829459a7302a41908eebc6966e07b711",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_29e4374108304d8e8b8c62ca88d28d7b",
            "value": 0
          }
        },
        "fd45814848ff4c488b85fd32ed65b484": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
