{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34638d89",
   "metadata": {},
   "source": [
    "# 봄/여름 모델링 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93347c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pygam import LinearGAM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34473797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 읽어오기\n",
    "data_na = pd.read_csv(\"C:/Users/shn20/Desktop/FILES/기상청/지면온도 데이터/surface_tp_train.csv\")\n",
    "data_imp = pd.read_csv(\"C:/Users/shn20/Desktop/FILES/기상청/지면온도 데이터/지면온도train_imputed.csv\")\n",
    "\n",
    "# 분석에 이용할 데이터 만들기 \n",
    "data_na['surface_tp_train.mmddhh']=data_na['surface_tp_train.mmddhh'].astype('str')\n",
    "\n",
    "data_na['시간']=data_na['surface_tp_train.mmddhh'].str[-2:]\n",
    "data_na['일']=data_na['surface_tp_train.mmddhh'].str[-4:-2]\n",
    "data_na['월']=data_na['surface_tp_train.mmddhh'].str[:-4]\n",
    "\n",
    "data_na.rename(columns={'surface_tp_train.stn':'지점번호'},inplace=True)\n",
    "\n",
    "\n",
    "data=pd.concat([data_na[['지점번호','시간','월','일']],\n",
    "                data_imp],axis=1)\n",
    "\n",
    "data=data.drop(['time'],axis=1).dropna(axis=0)\n",
    "need_scale=['평균기온', '평균이슬점온도', '평균상대습도', \n",
    "         '평균풍속', '누적강수량', '누적강수유무']\n",
    "\n",
    "data[['월','일','시간']]=data[['월','일','시간']].astype('int')\n",
    "\n",
    "encoder=LabelEncoder()\n",
    "data['현천계현천']=encoder.fit_transform(data['현천계현천'])\n",
    "\n",
    "# 계절별로 나누어주기\n",
    "\n",
    "spring=data[data['월'].isin([2,3,4])] ## 봄: 2-4월 \n",
    "summer=data[data['월'].isin([5,6,7])] ## 여름: 5-7월\n",
    "fall=data[data['월'].isin([8,9,10])] ## 가을: 8-10월\n",
    "winter=data[data['월'].isin([11,12,1])] ## 겨울: 11-1월\n",
    "\n",
    "    \n",
    "# RobustScaler\n",
    "spscaler = RobustScaler() \n",
    "suscaler = RobustScaler() \n",
    "fascaler = RobustScaler()\n",
    "wiscaler = RobustScaler()\n",
    "\n",
    "\n",
    "# Robust Scaling 적용\n",
    "spring[need_scale] = spscaler.fit_transform(spring[need_scale])\n",
    "summer[need_scale] = suscaler.fit_transform(summer[need_scale])\n",
    "fall[need_scale] = fascaler.fit_transform(fall[need_scale])\n",
    "winter[need_scale] = wiscaler.fit_transform(winter[need_scale])\n",
    "\n",
    "train_spring=spring\n",
    "train_summer= summer\n",
    "train_fall = fall \n",
    "train_winter = winter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab98570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 읽어오기\n",
    "test_na = pd.read_csv(\"C:/Users/shn20/Desktop/FILES/기상청/지면온도 데이터/surface_tp_test.csv\")\n",
    "test_imp = pd.read_csv(\"C:/Users/shn20/Desktop/FILES/기상청/지면온도 데이터/지면온도test_imputed.csv\")\n",
    "\n",
    "# 분석에 이용할 데이터 만들기 \n",
    "test_na['surface_tp_test.mmddhh']=test_na['surface_tp_test.mmddhh'].astype('str')\n",
    "\n",
    "test_na['시간']=test_na['surface_tp_test.mmddhh'].str[-2:]\n",
    "test_na['일']=test_na['surface_tp_test.mmddhh'].str[-4:-2]\n",
    "test_na['월']=test_na['surface_tp_test.mmddhh'].str[:-4]\n",
    "\n",
    "test_na.rename(columns={'surface_tp_test.stn':'STN'},inplace=True)\n",
    "test_na.rename(columns={'surface_tp_test.year':'YEAR'},inplace=True)\n",
    "test_na.rename(columns={'surface_tp_test.mmddhh':'MMDDHH'},inplace=True)\n",
    "\n",
    "test_na[['STN','YEAR','MMDDHH']]=test_na[['STN','YEAR','MMDDHH']].astype('str')  # 답안지 데이터와의 type 통일 \n",
    "\n",
    "test=pd.concat([test_na[['STN','시간','월','일','YEAR','MMDDHH']],\n",
    "                test_imp],axis=1)\n",
    "\n",
    "test=test.drop(['time'],axis=1)\n",
    "\n",
    "need_scale=['평균기온', '평균이슬점온도', '평균상대습도', \n",
    "         '평균풍속', '누적강수량', '누적강수유무']\n",
    "\n",
    "test[['월','일','시간']]=test[['월','일','시간']].astype('int')\n",
    "\n",
    "test['현천계현천']=encoder.transform(test['현천계현천'])\n",
    "\n",
    "# 계절별로 나누어주기\n",
    "\n",
    "test_spring=test[test['월'].isin([2,3,4])] ## 봄: 2-4월 \n",
    "test_summer=test[test['월'].isin([5,6,7])] ## 여름: 5-7월\n",
    "test_fall=test[test['월'].isin([8,9,10])] ## 가을: 8-10월\n",
    "test_winter=test[test['월'].isin([11,12,1])] ## 겨울: 11-1월\n",
    "\n",
    "# Robust Scaling 적용\n",
    "test_spring[need_scale] = spscaler.transform(test_spring[need_scale])\n",
    "test_summer[need_scale] = suscaler.transform(test_summer[need_scale])\n",
    "test_fall[need_scale] = fascaler.transform(test_fall[need_scale])\n",
    "test_winter[need_scale] = wiscaler.transform(test_winter[need_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bb4ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col=data.drop(['지점번호','지면온도','적설깊이'],axis=1).columns.to_list() # 겨울의 경우 적설깊이 변수를 써줘야 한다. \n",
    "x_winter=data.drop(['지점번호','지면온도'],axis=1).columns.to_list()\n",
    "\n",
    "y_col='지면온도'\n",
    "print(x_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68784349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_MAE(data,model,x_col=x_col):\n",
    "    \n",
    "    MAE= np.array([])\n",
    "    \n",
    "    for i in tqdm(range(1,11)):\n",
    "        idx=(data['지점번호']==i)\n",
    "        \n",
    "        train,test = data[~idx],data[idx]\n",
    "        x_train,y_train = train[x_col],train[y_col]\n",
    "        x_test,y_test = test[x_col],test[y_col]\n",
    "    \n",
    "        model.fit(x_train,y_train)\n",
    "        y_pred=model.predict(x_test)\n",
    "    \n",
    "        score=mean_absolute_error(y_test,y_pred)\n",
    "        MAE=np.append(MAE,score)\n",
    "\n",
    "    print(\"폴드 별 MAE\",MAE)\n",
    "    print(\"CV 평균 MAE\",np.mean(MAE))\n",
    "    \n",
    "def score(data,model,x_col=x_col):\n",
    "    \n",
    "    MAE= np.array([])\n",
    "    \n",
    "    for i in tqdm(range(1,11)):\n",
    "        idx=(data['지점번호']==i)\n",
    "        \n",
    "        train,test = data[~idx],data[idx]\n",
    "        x_train,y_train = train[x_col],train[y_col]\n",
    "        x_test,y_test = test[x_col],test[y_col]\n",
    "    \n",
    "        model.fit(x_train,y_train)\n",
    "        y_pred=model.predict(x_test)\n",
    "    \n",
    "        score=mean_absolute_error(y_test,y_pred)\n",
    "        MAE=np.append(MAE,score)\n",
    "\n",
    "    return(np.mean(MAE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0e627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class regressor:\n",
    "    def __init__(self, model1=None, model2=None):\n",
    "        if model1 is None:\n",
    "            self.model1 = CatBoostRegressor(verbose=False,has_time=True,random_state=6)\n",
    "        else:\n",
    "            self.model1 = model1\n",
    "\n",
    "        if model2 is None:\n",
    "            self.model2 = lgb.LGBMRegressor(random_state=6)\n",
    "        else:\n",
    "            self.model2 = model2\n",
    "\n",
    "        self.model3 = None\n",
    "        self.model4 = None\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        # 1차 적합\n",
    "        self.model1.fit(x_train,y_train)\n",
    "        train_fitted_values = self.model1.predict(x_train)\n",
    "        train_residual = y_train - train_fitted_values\n",
    "\n",
    "        # 2차 적합\n",
    "        self.model2.fit(x_train, train_residual)\n",
    "        train_fitted_values2 = self.model2.predict(x_train)\n",
    "        train_residual2 = train_residual - train_fitted_values2\n",
    "\n",
    "        # 3차 적합\n",
    "        self.model3 = lgb.LGBMRegressor(random_state=6)\n",
    "        self.model3.fit(x_train, train_residual2)\n",
    "        train_fitted_values3 = self.model3.predict(x_train)\n",
    "        train_residual3 = train_residual2 - train_fitted_values3\n",
    "\n",
    "        # 4차 적합\n",
    "        self.model4 = lgb.LGBMRegressor(random_state=6)\n",
    "        self.model4.fit(x_train, train_residual3)\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        test_fitted_values = self.model1.predict(x_test)\n",
    "        test_fitted_values2 = self.model2.predict(x_test)\n",
    "        test_fitted_values3 = self.model3.predict(x_test)\n",
    "        test_residual = self.model4.predict(x_test)\n",
    "\n",
    "        return test_fitted_values + test_fitted_values2 + test_fitted_values3 + test_residual\n",
    "\n",
    "    def evaluate(self, x_test, y_test):\n",
    "        y_pred = self.predict(x_test)\n",
    "        score = mean_absolute_error(y_test, y_pred)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f85d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 봄 \n",
    "\n",
    "sp_param1= {'learning_rate': 0.04158500359538149, 'depth': 12, 'iterations': 419}\n",
    "spring1=CatBoostRegressor(verbose=False,**sp_param1,has_time=True,random_state=6)\n",
    "\n",
    "sp_param2 = {'max_depth': 15, 'learning_rate': 0.09950693202165563, 'n_estimators': 902, 'min_child_samples': 59, 'subsample': 0.8052368021730489}\n",
    "spring2= lgb.LGBMRegressor(**sp_param2,random_state=6) \n",
    "\n",
    "# 여름 \n",
    "\n",
    "su_param1= {'learning_rate': 0.12515542897654602, 'depth': 13, 'iterations': 401} \n",
    "summer1=CatBoostRegressor(verbose=False,**su_param1,has_time=True,random_state=6)\n",
    "\n",
    "su_param2= {'max_depth': 14, 'learning_rate': 0.09990601824979606, 'n_estimators': 924, 'min_child_samples': 74, 'subsample': 0.30422223766873524}\n",
    "summer2= lgb.LGBMRegressor(**su_param2,random_state=6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7122c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "spr = regressor(model1=spring1,model2=spring2) #봄 \n",
    "sumr= regressor(model1=summer1,model2=summer2) #여름 \n",
    "\n",
    "spr.fit(train_spring[x_col],train_spring[y_col])\n",
    "spring_predict=spr.predict(test_spring[x_col])\n",
    "\n",
    "sumr.fit(train_summer[x_col],train_summer[y_col])\n",
    "summer_predict=sumr.predict(test_summer[x_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9700e01",
   "metadata": {},
   "source": [
    "## 옵튜나를 이용한 하이퍼 파라미터 튜닝 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82fd0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna import Trial\n",
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e522f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 봄 모델 1차 적합 모델 튜닝 \n",
    "def objective_cbr(trial):\n",
    "    \n",
    "    \n",
    "    # 하이퍼파라미터 검색 범위\n",
    "    learning_rate = trial.suggest_uniform('learning_rate', 0.01, 0.5)\n",
    "    depth = trial.suggest_int('depth', 7, 16)\n",
    "    iterations = trial.suggest_int('iterations', 250, 500)\n",
    "\n",
    "    model = CatBoostRegressor(verbose=False,\n",
    "                              learning_rate=learning_rate,\n",
    "                              depth=depth,\n",
    "                              iterations=iterations)\n",
    "    \n",
    "    MAE= np.array([])\n",
    "    \n",
    "    data=spring\n",
    "    \n",
    "    for i in range(1,11):\n",
    "        idx=(data['지점번호']==i)\n",
    "        \n",
    "        train,test = data[~idx],data[idx]\n",
    "        x_train,y_train = train[x_col],train[y_col]\n",
    "        x_test,y_test = test[x_col],test[y_col]\n",
    "    \n",
    "        model.fit(x_train,y_train)\n",
    "        y_pred=model.predict(x_test)\n",
    "    \n",
    "        score=mean_absolute_error(y_test,y_pred)\n",
    "        MAE=np.append(MAE,score)\n",
    "        \n",
    "    mae= np.mean(MAE)\n",
    "    \n",
    "    \n",
    "    return mae\n",
    "\n",
    "study = optuna.create_study(\n",
    "        direction='minimize', \n",
    "        sampler=TPESampler())\n",
    "\n",
    "study.optimize(objective_cbr,n_trials=15)\n",
    "\n",
    "trial = study.best_trial\n",
    "trial_params = trial.params\n",
    "print('Best Trial: score {},\\nparams {}'.format(trial.value, trial_params))\n",
    "\n",
    "# Best Trial: score 1.6421336515585658,\n",
    "# params {'learning_rate': 0.04158500359538149, 'depth': 12, 'iterations': 419}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25c250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 봄 모델 2차 적합 모델 튜닝 \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    spring[x_col], spring[y_col], test_size=0.33,\n",
    "    shuffle=True)\n",
    "\n",
    "spring1.fit(x_train,y_train)\n",
    "train_fitted_values = spring1.predict(x_train)\n",
    "train_residual = y_train - train_fitted_values\n",
    "\n",
    "test_fitted_values= spring1.predict(x_test)\n",
    "test_residual = y_test - test_fitted_values\n",
    "\n",
    "def objective_spring(trial):\n",
    "    param = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 10, 17),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 900, 970),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 55, 75),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.4, 1.0)\n",
    "    }\n",
    "\n",
    "    model2 = lgb.LGBMRegressor(**param)\n",
    "    model2.fit(x_train, train_residual)\n",
    "\n",
    "    train_predictions2 = model2.predict(x_train)\n",
    "    train_residuals2 = train_residual - train_predictions2\n",
    "    y_pred = model2.predict(x_test)\n",
    "    score = mean_absolute_error(test_residual, y_pred)\n",
    "\n",
    "    return score\n",
    "\n",
    "#study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective_spring, n_trials=100)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(best_params)\n",
    "\n",
    "# {'max_depth': 12, 'learning_rate': 0.08334925654486108, 'n_estimators': 960, 'min_child_samples': 67, 'subsample': 0.5200456584507223}\n",
    "# score= 1.3836848170174547\n",
    "# {'max_depth': 15, 'learning_rate': 0.09950693202165563, 'n_estimators': 902, 'min_child_samples': 59, 'subsample': 0.8052368021730489}\n",
    "# 1.378931408387576.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05466d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여름 모델 1차 적합 모델 튜닝 \n",
    "\n",
    "def objective_cbr(trial):\n",
    "    \n",
    "    \n",
    "    # 하이퍼파라미터 검색 범위\n",
    "    learning_rate = trial.suggest_uniform('learning_rate', 0.01, 0.5)\n",
    "    depth = trial.suggest_int('depth', 7, 16)\n",
    "    iterations = trial.suggest_int('iterations', 250, 500)\n",
    "\n",
    "    model = CatBoostRegressor(verbose=False,\n",
    "                              learning_rate=learning_rate,\n",
    "                              depth=depth,\n",
    "                              iterations=iterations)\n",
    "    \n",
    "    MAE= np.array([])\n",
    "    \n",
    "    data=summer\n",
    "    \n",
    "    for i in range(1,11):\n",
    "        idx=(data['지점번호']==i)\n",
    "        \n",
    "        train,test = data[~idx],data[idx]\n",
    "        x_train,y_train = train[x_col],train[y_col]\n",
    "        x_test,y_test = test[x_col],test[y_col]\n",
    "    \n",
    "        model.fit(x_train,y_train)\n",
    "        y_pred=model.predict(x_test)\n",
    "    \n",
    "        score=mean_absolute_error(y_test,y_pred)\n",
    "        MAE=np.append(MAE,score)\n",
    "        \n",
    "    mae= np.mean(MAE)\n",
    "    \n",
    "    \n",
    "    return mae\n",
    "\n",
    "study = optuna.create_study(\n",
    "        direction='minimize', \n",
    "        sampler=TPESampler())\n",
    "\n",
    "study.optimize(objective_cbr,n_trials=5)\n",
    "\n",
    "trial = study.best_trial\n",
    "trial_params = trial.params\n",
    "print('Best Trial: score {},\\nparams {}'.format(trial.value, trial_params))\n",
    "\n",
    "# {'learning_rate': 0.12515542897654602, 'depth': 13, 'iterations': 401} \n",
    "# Best is trial 1 with value: 1.8528896916820217."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a2d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여름 모델 2차 적합 모델 튜닝 \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    summer[x_col], summer[y_col], test_size=0.33,\n",
    "    shuffle=True)\n",
    "\n",
    "summer1.fit(x_train,y_train)\n",
    "train_fitted_values = summer1.predict(x_train)\n",
    "train_residual = y_train - train_fitted_values\n",
    "\n",
    "test_fitted_values= summer1.predict(x_test)\n",
    "test_residual = y_test - test_fitted_values\n",
    "\n",
    "def objective_summer(trial):\n",
    "    \n",
    "    param = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 900, 1000),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 70, 80),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.1, 1.0)\n",
    "    }\n",
    "\n",
    "    model2 = lgb.LGBMRegressor(**param)\n",
    "    model2.fit(x_train, train_residual)\n",
    "\n",
    "    train_predictions2 = model2.predict(x_train)\n",
    "    train_residuals2 = train_residual - train_predictions2\n",
    "    y_pred = model2.predict(x_test)\n",
    "    score = mean_absolute_error(test_residual, y_pred)\n",
    "\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective_summer, n_trials=100)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(best_params)\n",
    "\n",
    "# {'max_depth': 14, 'learning_rate': 0.09990601824979606, 'n_estimators': 924, 'min_child_samples': 74, 'subsample': 0.30422223766873524}\n",
    "# 1.5166825339675678."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c698b6",
   "metadata": {},
   "source": [
    "# 가을 모델링 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f6d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.graphics.tsaplots as sgt\n",
    "import missingno as msno\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder , RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "## CV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "## Model\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, Lasso, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor , HistGradientBoostingRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "## Tunning\n",
    "import re\n",
    "import optuna\n",
    "from optuna.integration import XGBoostPruningCallback\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fca22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fin = pd.read_csv('/content/drive/MyDrive/기상청/코드/이지윤/데이터/보간_수정_train.csv',encoding = 'UTF8')\n",
    "test_fin = pd.read_csv('/content/drive/MyDrive/기상청/코드/이지윤/데이터/보간_수정_test.csv',encoding = 'UTF8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf8ee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "\n",
    "# Label Encoding\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_fin['year'])\n",
    "train_fin[\"year\"] = encoder.transform(train_fin['year']) + 1\n",
    "\n",
    "# month / day / hour\n",
    "train_fin['mmddhh'] = train_fin['mmddhh'].astype('str')\n",
    "train_fin['hour'] = train_fin['mmddhh'].str[-2:]\n",
    "train_fin['day'] = train_fin['mmddhh'].str[-4:-2]\n",
    "train_fin['month'] = train_fin['mmddhh'].str[:-4]\n",
    "\n",
    "train_fin['datetime'] = '000' + train_fin['year'].astype(str) + '-' + train_fin['month'] + '-' + train_fin['day'] + ' ' + train_fin['hour'] + ':00:00'\n",
    "train_fin.drop(['mmddhh','year'], axis = 1 , inplace = True)\n",
    "\n",
    "train_fin['month'] = train_fin['month'].astype('int')\n",
    "\n",
    "# Y 에 Na 존재하는거 제거\n",
    "train_fin['month'] = train_fin['month'].astype(int)\n",
    "train_fin.dropna(subset=['ts'], inplace = True)\n",
    "\n",
    "# 불필요 열 제거 및 인덱스 설정\n",
    "train_fin.drop(['hour','day','Unnamed: 0'], axis = 1 , inplace = True)\n",
    "train_fin = train_fin.reset_index(drop = True).set_index('datetime')\n",
    "\n",
    "# test data\n",
    "\n",
    "# Label Encoding\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(test_fin['year'])\n",
    "test_fin[\"year\"] = encoder.transform(test_fin['year']) + 1\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(test_fin['stn'])\n",
    "test_fin['stn'] = encoder.transform(test_fin['stn']) + 1\n",
    "\n",
    "# month / day / hour\n",
    "test_fin['mmddhh'] = test_fin['mmddhh'].astype('str')\n",
    "test_fin['hour'] = test_fin['mmddhh'].str[-2:]\n",
    "test_fin['day'] = test_fin['mmddhh'].str[-4:-2]\n",
    "test_fin['month'] = test_fin['mmddhh'].str[:-4]\n",
    "\n",
    "test_fin['datetime'] = '000' + test_fin['year'].astype(str) + '-' + test_fin['month'] + '-' + test_fin['day'] + ' ' + test_fin['hour'] + ':00:00'\n",
    "\n",
    "test_fin.drop(['Unnamed: 0','mmddhh','year','hour','day','month'], axis = 1 , inplace = True)\n",
    "test_fin = test_fin.reset_index(drop = True).set_index('datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d49b2d",
   "metadata": {},
   "source": [
    "## 지점별 적합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2df7dd",
   "metadata": {},
   "source": [
    "### 지점 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b166665",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_tp_train_hybrid = surface_tp_train.copy()\n",
    "\n",
    "surface_tp_train_hybrid_stn1 = surface_tp_train_hybrid[surface_tp_train_hybrid.stn == 1].reset_index(drop=True)\n",
    "\n",
    "start_date = \"1900-02-01 00:00:00\"  # 시작 날짜\n",
    "end_date = \"1905-01-31 23:00:00\"  # 종료 날짜\n",
    "freq = \"H\"  # 월 단위로 설정\n",
    "\n",
    "surface_tp_train_hybrid_stn1.index = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "\n",
    "surface_tp_train_hybrid_stn1.drop(['stn', 'mmddhh', 'year'], axis=1, inplace=True)\n",
    "\n",
    "surface_tp_train_hybrid_stn1 = pd.get_dummies(data=surface_tp_train_hybrid_stn1, drop_first=True)\n",
    "\n",
    "# surface_tp_train_hybrid_stn1 = surface_tp_train_hybrid[surface_tp_train_hybrid.stn == 1].reset_index(drop=True)\n",
    "\n",
    "# # 로버스트 스케일링\n",
    "# train_X_num = surface_tp_train_hybrid_stn1[['ta', 'td', 'hm', 'ws', 'rn', 're', 'si',\n",
    "#        'ss', 'sn']]\n",
    "\n",
    "# # scikit-learn 패키지의 RobustScaler 클래스를 불러옵니다.\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "# # RobustScaler 객체를 생성합니다.\n",
    "# robustScaler = RobustScaler()\n",
    "\n",
    "# # fit_transform()을 사용해서 학습과 스케일링을 한 번에 적용합니다.\n",
    "# X_train_robust = robustScaler.fit_transform(train_X_num)\n",
    "\n",
    "# surface_tp_train_hybrid_stn1[['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn']] = X_train_robust\n",
    "\n",
    "# start_date = \"1900-02-01 00:00:00\"  # 시작 날짜\n",
    "# end_date = \"1905-01-31 23:00:00\"  # 종료 날짜\n",
    "# freq = \"H\"  # 월 단위로 설정\n",
    "\n",
    "# surface_tp_train_hybrid_stn1.index = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "\n",
    "# surface_tp_train_hybrid_stn1.drop(['stn', 'mmddhh', 'year'], axis=1, inplace=True)\n",
    "\n",
    "# surface_tp_train_hybrid_stn1 = pd.get_dummies(data=surface_tp_train_hybrid_stn1, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8695c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = ['ts']\n",
    "f_col = ['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn', 'ww_F',\n",
    "       'ww_G', 'ww_H', 'ww_R', 'ww_S', 'ww_X']\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "                                                    surface_tp_train_hybrid_stn1[f_col],\n",
    "                                                    surface_tp_train_hybrid_stn1[target],\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=False,\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751511a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.tsa.deterministic import DeterministicProcess\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# dp = DeterministicProcess(\n",
    "#                           index = train_x.index,\n",
    "#                           constant = True,\n",
    "#                           order = 2,\n",
    "#                           drop = True,\n",
    "#                           )\n",
    "\n",
    "# X = dp.in_sample()\n",
    "# lm = LinearRegression(fit_intercept=False)\n",
    "# lm.fit(X, train_y)\n",
    "\n",
    "# test_const = [1 for i in range(test_x.shape[0])]\n",
    "# test_trend = np.arange(X['trend'].max(), X['trend'].max()+test_x.shape[0])\n",
    "# test_trend_squared = [i**2 for i in test_trend]\n",
    "# X_ = pd.DataFrame([test_const, test_trend, test_trend_squared]).T\n",
    "# X_.columns = ['const', 'trend', 'trend_squared']\n",
    "# X_.index = test_x.index\n",
    "\n",
    "# trend_train = lm.predict(X)\n",
    "# trend_test = lm.predict(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cfd4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.deterministic import DeterministicProcess\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "dp = DeterministicProcess(\n",
    "                          index = train_x.index,\n",
    "                          constant = True,\n",
    "                          order = 1,\n",
    "                          drop = True,\n",
    "                          )\n",
    "\n",
    "X = dp.in_sample()\n",
    "lm1 = LinearRegression(fit_intercept=False)\n",
    "lm1.fit(X, train_y)\n",
    "\n",
    "test_const = [1 for i in range(test_x.shape[0])]\n",
    "test_trend = np.arange(X['trend'].max(), X['trend'].max()+test_x.shape[0])\n",
    "# test_trend_squared = [i**2 for i in test_trend]\n",
    "X_ = pd.DataFrame([test_const, test_trend]).T\n",
    "X_.columns = ['const', 'trend']\n",
    "X_.index = test_x.index\n",
    "\n",
    "trend_train = lm1.predict(X)\n",
    "trend_test = lm1.predict(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c2d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "train_y_delta = train_y - trend_train\n",
    "test_y_delta = test_y - trend_test\n",
    "\n",
    "# xgb= xgboost.XGBRegressor()\n",
    "# xgb.fit(train_x, train_y_delta)\n",
    "\n",
    "# xgb_train = xgb.predict(train_x)\n",
    "# xgb_test = xgb.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4090a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# misc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2366f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_params_fold_start(XGB_params, X, y):\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    pred_list = []\n",
    "    mae_list = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "\n",
    "        x_train, x_val, y_train, y_val = X.iloc[train_index], X.iloc[val_index], y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        model = xgboost.XGBRegressor(**XGB_params)\n",
    "\n",
    "        model.fit(x_train, y_train,\n",
    "                eval_set=[(x_val,y_val)],\n",
    "                eval_metric='mae', verbose=False, early_stopping_rounds=100)\n",
    "\n",
    "        pred = model.predict(x_val)\n",
    "        result = mean_absolute_error(pred,y_val)\n",
    "\n",
    "        mae_list.append(result)\n",
    "\n",
    "    return np.mean(mae_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78d15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectiveXGB(trial: Trial, X, y):\n",
    "    param = {\n",
    "        'n_estimators' : trial.suggest_int('n_estimators', 500, 4000),\n",
    "        'max_depth' : trial.suggest_int('max_depth', 8, 16),\n",
    "        'min_child_weight' : trial.suggest_int('min_child_weight', 1, 300),\n",
    "        'gamma' : trial.suggest_int('gamma', 1, 3),\n",
    "        'learning_rate' : 0.01,\n",
    "        'colsample_bytree' : trial.suggest_discrete_uniform('colsample_bytree', 0.5, 1, 0.1),\n",
    "        'nthread' : -1,\n",
    "        'tree_method' : 'gpu_hist',\n",
    "        'predictor' : 'gpu_predictor',\n",
    "        'lambda' : trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "        'alpha' : trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "        'subsample' : trial.suggest_categorical('subsample', [0.6,0.7,0.8,1.0]),\n",
    "        'random_state' : 1127,\n",
    "        'objective' : 'reg:absoluteerror'\n",
    "    }\n",
    "\n",
    "    score = XGB_params_fold_start(param, X, y)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984d01ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize',sampler=TPESampler(seed=42))\n",
    "\n",
    "study.optimize(lambda trial : objectiveXGB(trial, train_x, train_y_delta), n_trials=30)\n",
    "\n",
    "print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae3b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best1 = xgboost.XGBRegressor(**study.best_trial.params, **{'tree_method' : 'gpu_hist', 'predictor' : 'gpu_predictor'})\n",
    "xgb_best1.fit(train_x, train_y_delta)\n",
    "\n",
    "xgb_train = xgb_best1.predict(train_x)\n",
    "xgb_test = xgb_best1.predict(test_x)\n",
    "\n",
    "plt.plot(surface_tp_train_hybrid_stn1.index, surface_tp_train_hybrid_stn1.loc[:, 'ts'], label=\"원본\")\n",
    "\n",
    "plt.plot(train_x.index, xgb_train, label=\"train\")\n",
    "\n",
    "plt.plot(test_x.index, xgb_test, label=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6732cdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train = [i+j for i,j in zip(sum(trend_train.tolist(), []), xgb_train.tolist())]\n",
    "\n",
    "original_test = [i+j for i,j in zip(sum(trend_test.tolist(), []), xgb_test.tolist())]\n",
    "\n",
    "plt.plot(surface_tp_train_hybrid_stn1.index, surface_tp_train_hybrid_stn1.loc[:, 'ts'], label=\"원본\")\n",
    "\n",
    "plt.plot(train_x.index, original_train, label=\"train\")\n",
    "\n",
    "plt.plot(test_x.index, original_test, label=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc78eb83",
   "metadata": {},
   "source": [
    "### 지점2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5205db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_tp_train_hybrid = surface_tp_train.copy()\n",
    "\n",
    "surface_tp_train_hybrid_stn2 = surface_tp_train_hybrid[surface_tp_train_hybrid.stn == 2].reset_index(drop=True)\n",
    "\n",
    "start_date = \"1900-02-01 00:00:00\"  # 시작 날짜\n",
    "end_date = \"1905-01-31 23:00:00\"  # 종료 날짜\n",
    "freq = \"H\"  # 월 단위로 설정\n",
    "\n",
    "surface_tp_train_hybrid_stn2.index = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "\n",
    "surface_tp_train_hybrid_stn2.drop(['stn', 'mmddhh', 'year'], axis=1, inplace=True)\n",
    "\n",
    "surface_tp_train_hybrid_stn2 = pd.get_dummies(data=surface_tp_train_hybrid_stn2, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef97be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# surface_tp_train_hybrid_stn2 = surface_tp_train_hybrid[surface_tp_train_hybrid.stn == 2].reset_index(drop=True)\n",
    "\n",
    "# # 로버스트 스케일링\n",
    "# train_X_num = surface_tp_train_hybrid_stn2[['ta', 'td', 'hm', 'ws', 'rn', 're', 'si',\n",
    "#        'ss', 'sn']]\n",
    "\n",
    "# # scikit-learn 패키지의 RobustScaler 클래스를 불러옵니다.\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "# # RobustScaler 객체를 생성합니다.\n",
    "# robustScaler = RobustScaler()\n",
    "\n",
    "# # fit_transform()을 사용해서 학습과 스케일링을 한 번에 적용합니다.\n",
    "# X_train_robust = robustScaler.fit_transform(train_X_num)\n",
    "\n",
    "# surface_tp_train_hybrid_stn2[['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn']] = X_train_robust\n",
    "\n",
    "# start_date = \"1900-02-01 00:00:00\"  # 시작 날짜\n",
    "# end_date = \"1905-01-31 23:00:00\"  # 종료 날짜\n",
    "# freq = \"H\"  # 월 단위로 설정\n",
    "\n",
    "# surface_tp_train_hybrid_stn2.index = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "\n",
    "# surface_tp_train_hybrid_stn2.drop(['stn', 'mmddhh', 'year'], axis=1, inplace=True)\n",
    "\n",
    "# surface_tp_train_hybrid_stn2 = pd.get_dummies(data=surface_tp_train_hybrid_stn2, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb057c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = ['ts']\n",
    "f_col = ['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn', 'ww_F',\n",
    "       'ww_G', 'ww_H', 'ww_R', 'ww_S', 'ww_X']\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "                                                    surface_tp_train_hybrid_stn2[f_col],\n",
    "                                                    surface_tp_train_hybrid_stn2[target],\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=False,\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e6115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.tsa.deterministic import DeterministicProcess\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# dp = DeterministicProcess(\n",
    "#                           index = train_x.index,\n",
    "#                           constant = True,\n",
    "#                           order = 2,\n",
    "#                           drop = True,\n",
    "#                           )\n",
    "\n",
    "# X = dp.in_sample()\n",
    "# lm = LinearRegression(fit_intercept=False)\n",
    "# lm.fit(X, train_y)\n",
    "\n",
    "# test_const = [1 for i in range(test_x.shape[0])]\n",
    "# test_trend = np.arange(X['trend'].max(), X['trend'].max()+test_x.shape[0])\n",
    "# test_trend_squared = [i**2 for i in test_trend]\n",
    "# X_ = pd.DataFrame([test_const, test_trend, test_trend_squared]).T\n",
    "# X_.columns = ['const', 'trend', 'trend_squared']\n",
    "# X_.index = test_x.index\n",
    "\n",
    "# trend_train = lm.predict(X)\n",
    "# trend_test = lm.predict(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d83812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.deterministic import DeterministicProcess\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "dp = DeterministicProcess(\n",
    "                          index = train_x.index,\n",
    "                          constant = True,\n",
    "                          order = 1,\n",
    "                          drop = True,\n",
    "                          )\n",
    "\n",
    "X = dp.in_sample()\n",
    "lm2 = LinearRegression(fit_intercept=False)\n",
    "lm2.fit(X, train_y)\n",
    "\n",
    "test_const = [1 for i in range(test_x.shape[0])]\n",
    "test_trend = np.arange(X['trend'].max(), X['trend'].max()+test_x.shape[0])\n",
    "# test_trend_squared = [i**2 for i in test_trend]\n",
    "X_ = pd.DataFrame([test_const, test_trend]).T\n",
    "X_.columns = ['const', 'trend']\n",
    "X_.index = test_x.index\n",
    "\n",
    "trend_train = lm2.predict(X)\n",
    "trend_test = lm2.predict(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8172f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(surface_tp_train_hybrid_stn2.index, surface_tp_train_hybrid_stn2.loc[:, 'ts'], label=\"원본\")\n",
    "\n",
    "plt.plot(train_x.index, trend_train, label=\"train\")\n",
    "\n",
    "plt.plot(test_x.index, trend_test, label=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0538c6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "train_y_delta = train_y - trend_train\n",
    "test_y_delta = test_y - trend_test\n",
    "\n",
    "# xgb= xgboost.XGBRegressor()\n",
    "# xgb.fit(train_x, train_y_delta)\n",
    "\n",
    "# xgb_train = xgb.predict(train_x)\n",
    "# xgb_test = xgb.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e13121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# misc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86dfb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_params_fold_start(XGB_params, X, y):\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    pred_list = []\n",
    "    mae_list = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "\n",
    "        x_train, x_val, y_train, y_val = X.iloc[train_index], X.iloc[val_index], y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        model = xgboost.XGBRegressor(**XGB_params)\n",
    "\n",
    "        model.fit(x_train, y_train,\n",
    "                eval_set=[(x_val,y_val)],\n",
    "                eval_metric='mae', verbose=False, early_stopping_rounds=100)\n",
    "\n",
    "        pred = model.predict(x_val)\n",
    "        result = mean_absolute_error(pred,y_val)\n",
    "\n",
    "        mae_list.append(result)\n",
    "\n",
    "    return np.mean(mae_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc4d9201",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Trial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjectiveXGB\u001b[39m(trial: \u001b[43mTrial\u001b[49m, X, y):\n\u001b[0;32m      2\u001b[0m     param \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m : trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m4000\u001b[39m),\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m : trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m16\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg:absoluteerror\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     17\u001b[0m     }\n\u001b[0;32m     19\u001b[0m     score \u001b[38;5;241m=\u001b[39m XGB_params_fold_start(param, X, y)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Trial' is not defined"
     ]
    }
   ],
   "source": [
    "def objectiveXGB(trial: Trial, X, y):\n",
    "    param = {\n",
    "        'n_estimators' : trial.suggest_int('n_estimators', 500, 4000),\n",
    "        'max_depth' : trial.suggest_int('max_depth', 8, 16),\n",
    "        'min_child_weight' : trial.suggest_int('min_child_weight', 1, 300),\n",
    "        'gamma' : trial.suggest_int('gamma', 1, 3),\n",
    "        'learning_rate' : 0.01,\n",
    "        'colsample_bytree' : trial.suggest_discrete_uniform('colsample_bytree', 0.5, 1, 0.1),\n",
    "        'nthread' : -1,\n",
    "        'tree_method' : 'gpu_hist',\n",
    "        'predictor' : 'gpu_predictor',\n",
    "        'lambda' : trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "        'alpha' : trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "        'subsample' : trial.suggest_categorical('subsample', [0.6,0.7,0.8,1.0]),\n",
    "        'random_state' : 1127,\n",
    "        'objective' : 'reg:absoluteerror'\n",
    "    }\n",
    "\n",
    "    score = XGB_params_fold_start(param, X, y)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184a3f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize',sampler=TPESampler(seed=42))\n",
    "\n",
    "study.optimize(lambda trial : objectiveXGB(trial, train_x, train_y_delta), n_trials=30)\n",
    "\n",
    "print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228000ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best2 = xgboost.XGBRegressor(**study.best_trial.params, **{'tree_method' : 'gpu_hist', 'predictor' : 'gpu_predictor'})\n",
    "xgb_best2.fit(train_x, train_y_delta)\n",
    "\n",
    "xgb_train = xgb_best2.predict(train_x)\n",
    "xgb_test = xgb_best2.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bc412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(surface_tp_train_hybrid_stn2.index, surface_tp_train_hybrid_stn2.loc[:, 'ts'], label=\"원본\")\n",
    "\n",
    "plt.plot(train_x.index, xgb_train, label=\"train\")\n",
    "\n",
    "plt.plot(test_x.index, xgb_test, label=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090bfec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train = [i+j for i,j in zip(sum(trend_train.tolist(), []), xgb_train.tolist())]\n",
    "\n",
    "original_test = [i+j for i,j in zip(sum(trend_test.tolist(), []), xgb_test.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a1ddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(surface_tp_train_hybrid_stn2.index, surface_tp_train_hybrid_stn2.loc[:, 'ts'], label=\"원본\")\n",
    "\n",
    "plt.plot(train_x.index, original_train, label=\"train\")\n",
    "\n",
    "plt.plot(test_x.index, original_test, label=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b85c2",
   "metadata": {},
   "source": [
    "### 지점3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4deb2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_tp_train_hybrid = surface_tp_train.copy()\n",
    "\n",
    "surface_tp_train_hybrid_stn3 = surface_tp_train_hybrid[surface_tp_train_hybrid.stn == 3].reset_index(drop=True)\n",
    "\n",
    "start_date = \"1900-02-01 00:00:00\"  # 시작 날짜\n",
    "end_date = \"1905-01-31 23:00:00\"  # 종료 날짜\n",
    "freq = \"H\"  # 월 단위로 설정\n",
    "\n",
    "surface_tp_train_hybrid_stn3.index = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "\n",
    "surface_tp_train_hybrid_stn3.drop(['stn', 'mmddhh', 'year'], axis=1, inplace=True)\n",
    "\n",
    "surface_tp_train_hybrid_stn3 = pd.get_dummies(data=surface_tp_train_hybrid_stn3, drop_first=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = ['ts']\n",
    "f_col = ['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn', 'ww_F',\n",
    "       'ww_G', 'ww_H', 'ww_R', 'ww_S', 'ww_X']\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "                                                    surface_tp_train_hybrid_stn3[f_col],\n",
    "                                                    surface_tp_train_hybrid_stn3[target],\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=False,\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc5704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.deterministic import DeterministicProcess\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "dp = DeterministicProcess(\n",
    "                          index = train_x.index,\n",
    "                          constant = True,\n",
    "                          order = 1,\n",
    "                          drop = True,\n",
    "                          )\n",
    "\n",
    "X = dp.in_sample()\n",
    "lm3 = LinearRegression(fit_intercept=False)\n",
    "lm3.fit(X, train_y)\n",
    "\n",
    "test_const = [1 for i in range(test_x.shape[0])]\n",
    "test_trend = np.arange(X['trend'].max(), X['trend'].max()+test_x.shape[0])\n",
    "# test_trend_squared = [i**2 for i in test_trend]\n",
    "X_ = pd.DataFrame([test_const, test_trend]).T\n",
    "X_.columns = ['const', 'trend']\n",
    "X_.index = test_x.index\n",
    "\n",
    "trend_train = lm3.predict(X)\n",
    "trend_test = lm3.predict(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d1a4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(surface_tp_train_hybrid_stn3.index, surface_tp_train_hybrid_stn3.loc[:, 'ts'], label=\"원본\")\n",
    "\n",
    "plt.plot(train_x.index, trend_train, label=\"train\")\n",
    "\n",
    "plt.plot(test_x.index, trend_test, label=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d95f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "train_y_delta = train_y - trend_train\n",
    "test_y_delta = test_y - trend_test\n",
    "\n",
    "# xgb= xgboost.XGBRegressor()\n",
    "# xgb.fit(train_x, train_y_delta)\n",
    "\n",
    "# xgb_train = xgb.predict(train_x)\n",
    "# xgb_test = xgb.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3633c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# misc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcad7553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_params_fold_start(XGB_params, X, y):\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    pred_list = []\n",
    "    mae_list = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "\n",
    "        x_train, x_val, y_train, y_val = X.iloc[train_index], X.iloc[val_index], y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        model = xgboost.XGBRegressor(**XGB_params)\n",
    "\n",
    "        model.fit(x_train, y_train,\n",
    "                eval_set=[(x_val,y_val)],\n",
    "                eval_metric='mae', verbose=False, early_stopping_rounds=100)\n",
    "\n",
    "        pred = model.predict(x_val)\n",
    "        result = mean_absolute_error(pred,y_val)\n",
    "\n",
    "        mae_list.append(result)\n",
    "\n",
    "    return np.mean(mae_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6823aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectiveXGB(trial: Trial, X, y):\n",
    "    param = {\n",
    "        'n_estimators' : trial.suggest_int('n_estimators', 500, 4000),\n",
    "        'max_depth' : trial.suggest_int('max_depth', 8, 16),\n",
    "        'min_child_weight' : trial.suggest_int('min_child_weight', 1, 300),\n",
    "        'gamma' : trial.suggest_int('gamma', 1, 3),\n",
    "        'learning_rate' : 0.01,\n",
    "        'colsample_bytree' : trial.suggest_discrete_uniform('colsample_bytree', 0.5, 1, 0.1),\n",
    "        'nthread' : -1,\n",
    "        'tree_method' : 'gpu_hist',\n",
    "        'predictor' : 'gpu_predictor',\n",
    "        'lambda' : trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "        'alpha' : trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "        'subsample' : trial.suggest_categorical('subsample', [0.6,0.7,0.8,1.0]),\n",
    "        'random_state' : 1127,\n",
    "        'objective' : 'reg:absoluteerror'\n",
    "    }\n",
    "\n",
    "    score = XGB_params_fold_start(param, X, y)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b35986",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize',sampler=TPESampler(seed=42))\n",
    "\n",
    "study.optimize(lambda trial : objectiveXGB(trial, train_x, train_y_delta), n_trials=30)\n",
    "\n",
    "print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aedf078",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best3 = xgboost.XGBRegressor(**study.best_trial.params, **{'tree_method' : 'gpu_hist', 'predictor' : 'gpu_predictor'})\n",
    "xgb_best3.fit(train_x, train_y_delta)\n",
    "\n",
    "xgb_train = xgb_best3.predict(train_x)\n",
    "xgb_test = xgb_best3.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124f186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(surface_tp_train_hybrid_stn3.index, surface_tp_train_hybrid_stn3.loc[:, 'ts'], label=\"원본\")\n",
    "\n",
    "plt.plot(train_x.index, xgb_train, label=\"train\")\n",
    "\n",
    "plt.plot(test_x.index, xgb_test, label=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f5c983",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train = [i+j for i,j in zip(sum(trend_train.tolist(), []), xgb_train.tolist())]\n",
    "\n",
    "original_test = [i+j for i,j in zip(sum(trend_test.tolist(), []), xgb_test.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eddce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(surface_tp_train_hybrid_stn3.index, surface_tp_train_hybrid_stn3.loc[:, 'ts'], label=\"원본\")\n",
    "\n",
    "plt.plot(train_x.index, original_train, label=\"train\")\n",
    "\n",
    "plt.plot(test_x.index, original_test, label=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab2fb04",
   "metadata": {},
   "source": [
    "### 지점3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29905115",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_tp_train_hybrid = surface_tp_train.copy()\n",
    "\n",
    "surface_tp_train_hybrid_stn4 = surface_tp_train_hybrid[surface_tp_train_hybrid.stn == 4].reset_index(drop=True)\n",
    "\n",
    "start_date = \"1900-02-01 00:00:00\"  # 시작 날짜\n",
    "end_date = \"1905-01-31 23:00:00\"  # 종료 날짜\n",
    "freq = \"H\"  # 월 단위로 설정\n",
    "\n",
    "surface_tp_train_hybrid_stn4.index = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "\n",
    "surface_tp_train_hybrid_stn4.drop(['stn', 'mmddhh', 'year'], axis=1, inplace=True)\n",
    "\n",
    "surface_tp_train_hybrid_stn4 = pd.get_dummies(data=surface_tp_train_hybrid_stn4, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa6c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = ['ts']\n",
    "f_col = ['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn', 'ww_F',\n",
    "       'ww_G', 'ww_H', 'ww_R', 'ww_S', 'ww_X']\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "                                                    surface_tp_train_hybrid_stn4[f_col],\n",
    "                                                    surface_tp_train_hybrid_stn4[target],\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=False,\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef35c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.deterministic import DeterministicProcess\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "dp = DeterministicProcess(\n",
    "                          index = train_x.index,\n",
    "                          constant = True,\n",
    "                          order = 1,\n",
    "                          drop = True,\n",
    "                          )\n",
    "\n",
    "X = dp.in_sample()\n",
    "lm4 = LinearRegression(fit_intercept=False)\n",
    "lm4.fit(X, train_y)\n",
    "\n",
    "test_const = [1 for i in range(test_x.shape[0])]\n",
    "test_trend = np.arange(X['trend'].max(), X['trend'].max()+test_x.shape[0])\n",
    "# test_trend_squared = [i**2 for i in test_trend]\n",
    "X_ = pd.DataFrame([test_const, test_trend]).T\n",
    "X_.columns = ['const', 'trend']\n",
    "X_.index = test_x.index\n",
    "\n",
    "trend_train = lm4.predict(X)\n",
    "trend_test = lm4.predict(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d675d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(surface_tp_train_hybrid_stn4.index, surface_tp_train_hybrid_stn4.loc[:, 'ts'], label=\"원본\")\n",
    "\n",
    "plt.plot(train_x.index, trend_train, label=\"train\")\n",
    "\n",
    "plt.plot(test_x.index, trend_test, label=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8196256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "train_y_delta = train_y - trend_train\n",
    "test_y_delta = test_y - trend_test\n",
    "\n",
    "# xgb= xgboost.XGBRegressor()\n",
    "# xgb.fit(train_x, train_y_delta)\n",
    "\n",
    "# xgb_train = xgb.predict(train_x)\n",
    "# xgb_test = xgb.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bf42af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# misc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de005b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_params_fold_start(XGB_params, X, y):\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    pred_list = []\n",
    "    mae_list = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "\n",
    "        x_train, x_val, y_train, y_val = X.iloc[train_index], X.iloc[val_index], y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        model = xgboost.XGBRegressor(**XGB_params)\n",
    "\n",
    "        model.fit(x_train, y_train,\n",
    "                eval_set=[(x_val,y_val)],\n",
    "                eval_metric='mae', verbose=False, early_stopping_rounds=100)\n",
    "\n",
    "        pred = model.predict(x_val)\n",
    "        result = mean_absolute_error(pred,y_val)\n",
    "\n",
    "        mae_list.append(result)\n",
    "\n",
    "    return np.mean(mae_list)\n",
    "\n",
    "def objectiveXGB(trial: Trial, X, y):\n",
    "    param = {\n",
    "        'n_estimators' : trial.suggest_int('n_estimators', 500, 4000),\n",
    "        'max_depth' : trial.suggest_int('max_depth', 8, 16),\n",
    "        'min_child_weight' : trial.suggest_int('min_child_weight', 1, 300),\n",
    "        'gamma' : trial.suggest_int('gamma', 1, 3),\n",
    "        'learning_rate' : 0.01,\n",
    "        'colsample_bytree' : trial.suggest_discrete_uniform('colsample_bytree', 0.5, 1, 0.1),\n",
    "        'nthread' : -1,\n",
    "        'tree_method' : 'gpu_hist',\n",
    "        'predictor' : 'gpu_predictor',\n",
    "        'lambda' : trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "        'alpha' : trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "        'subsample' : trial.suggest_categorical('subsample', [0.6,0.7,0.8,1.0]),\n",
    "        'random_state' : 1127,\n",
    "        'objective' : 'reg:absoluteerror'\n",
    "    }\n",
    "\n",
    "    score = XGB_params_fold_start(param, X, y)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96417393",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize',sampler=TPESampler(seed=42))\n",
    "\n",
    "study.optimize(lambda trial : objectiveXGB(trial, train_x, train_y_delta), n_trials=30)\n",
    "\n",
    "print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400596cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best4 = xgboost.XGBRegressor(**study.best_trial.params, **{'tree_method' : 'gpu_hist', 'predictor' : 'gpu_predictor'})\n",
    "xgb_best4.fit(train_x, train_y_delta)\n",
    "\n",
    "xgb_train = xgb_best4.predict(train_x)\n",
    "xgb_test = xgb_best4.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10ec3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(surface_tp_train_hybrid_stn4.index, surface_tp_train_hybrid_stn4.loc[:, 'ts'], label=\"원본\")\n",
    "\n",
    "plt.plot(train_x.index, xgb_train, label=\"train\")\n",
    "\n",
    "plt.plot(test_x.index, xgb_test, label=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2a27d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train = [i+j for i,j in zip(sum(trend_train.tolist(), []), xgb_train.tolist())]\n",
    "\n",
    "original_test = [i+j for i,j in zip(sum(trend_test.tolist(), []), xgb_test.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7de395",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(surface_tp_train_hybrid_stn4.index, surface_tp_train_hybrid_stn4.loc[:, 'ts'], label=\"원본\")\n",
    "\n",
    "plt.plot(train_x.index, original_train, label=\"train\")\n",
    "\n",
    "plt.plot(test_x.index, original_test, label=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62afa719",
   "metadata": {},
   "source": [
    "### 지점5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c41939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_tp_train_hybrid = surface_tp_train.copy()\n",
    "\n",
    "surface_tp_train_hybrid_stn5 = surface_tp_train_hybrid[surface_tp_train_hybrid.stn == 5].reset_index(drop=True)\n",
    "\n",
    "start_date = \"1900-02-01 00:00:00\"  # 시작 날짜\n",
    "end_date = \"1905-01-31 23:00:00\"  # 종료 날짜\n",
    "freq = \"H\"  # 월 단위로 설정\n",
    "\n",
    "surface_tp_train_hybrid_stn5.index = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "\n",
    "surface_tp_train_hybrid_stn5.drop(['stn', 'mmddhh', 'year'], axis=1, inplace=True)\n",
    "\n",
    "surface_tp_train_hybrid_stn5 = pd.get_dummies(data=surface_tp_train_hybrid_stn5, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8112db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = ['ts']\n",
    "f_col = ['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn', 'ww_F',\n",
    "       'ww_G', 'ww_H', 'ww_R', 'ww_S', 'ww_X']\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "                                                    surface_tp_train_hybrid_stn5[f_col],\n",
    "                                                    surface_tp_train_hybrid_stn5[target],\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=False,\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c26f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.deterministic import DeterministicProcess\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "dp = DeterministicProcess(\n",
    "                          index = train_x.index,\n",
    "                          constant = True,\n",
    "                          order = 1,\n",
    "                          drop = True,\n",
    "                          )\n",
    "\n",
    "X = dp.in_sample()\n",
    "lm5 = LinearRegression(fit_intercept=False)\n",
    "lm5.fit(X, train_y)\n",
    "\n",
    "test_const = [1 for i in range(test_x.shape[0])]\n",
    "test_trend = np.arange(X['trend'].max(), X['trend'].max()+test_x.shape[0])\n",
    "# test_trend_squared = [i**2 for i in test_trend]\n",
    "X_ = pd.DataFrame([test_const, test_trend]).T\n",
    "X_.columns = ['const', 'trend']\n",
    "X_.index = test_x.index\n",
    "\n",
    "trend_train = lm5.predict(X)\n",
    "trend_test = lm5.predict(X_)\n",
    "\n",
    "plt.plot(surface_tp_train_hybrid_stn5.index, surface_tp_train_hybrid_stn5.loc[:, 'ts'], label=\"원본\")\n",
    "\n",
    "plt.plot(train_x.index, trend_train, label=\"train\")\n",
    "\n",
    "plt.plot(test_x.index, trend_test, label=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c91442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "train_y_delta = train_y - trend_train\n",
    "test_y_delta = test_y - trend_test\n",
    "\n",
    "# xgb= xgboost.XGBRegressor()\n",
    "# xgb.fit(train_x, train_y_delta)\n",
    "\n",
    "# xgb_train = xgb.predict(train_x)\n",
    "# xgb_test = xgb.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d56898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# misc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b062c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_params_fold_start(XGB_params, X, y):\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    pred_list = []\n",
    "    mae_list = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "\n",
    "        x_train, x_val, y_train, y_val = X.iloc[train_index], X.iloc[val_index], y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        model = xgboost.XGBRegressor(**XGB_params)\n",
    "\n",
    "        model.fit(x_train, y_train,\n",
    "                eval_set=[(x_val,y_val)],\n",
    "                eval_metric='mae', verbose=False, early_stopping_rounds=100)\n",
    "\n",
    "        pred = model.predict(x_val)\n",
    "        result = mean_absolute_error(pred,y_val)\n",
    "\n",
    "        mae_list.append(result)\n",
    "\n",
    "    return np.mean(mae_list)\n",
    "\n",
    "def objectiveXGB(trial: Trial, X, y):\n",
    "    param = {\n",
    "        'n_estimators' : trial.suggest_int('n_estimators', 500, 4000),\n",
    "        'max_depth' : trial.suggest_int('max_depth', 8, 16),\n",
    "        'min_child_weight' : trial.suggest_int('min_child_weight', 1, 300),\n",
    "        'gamma' : trial.suggest_int('gamma', 1, 3),\n",
    "        'learning_rate' : 0.01,\n",
    "        'colsample_bytree' : trial.suggest_discrete_uniform('colsample_bytree', 0.5, 1, 0.1),\n",
    "        'nthread' : -1,\n",
    "        'tree_method' : 'gpu_hist',\n",
    "        'predictor' : 'gpu_predictor',\n",
    "        'lambda' : trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "        'alpha' : trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "        'subsample' : trial.suggest_categorical('subsample', [0.6,0.7,0.8,1.0]),\n",
    "        'random_state' : 1127,\n",
    "        'objective' : 'reg:absoluteerror'\n",
    "    }\n",
    "\n",
    "    score = XGB_params_fold_start(param, X, y)\n",
    "\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction='minimize',sampler=TPESampler(seed=42))\n",
    "\n",
    "study.optimize(lambda trial : objectiveXGB(trial, train_x, train_y_delta), n_trials=30)\n",
    "\n",
    "print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff01c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best5 = xgboost.XGBRegressor(**study.best_trial.params, **{'tree_method' : 'gpu_hist', 'predictor' : 'gpu_predictor'})\n",
    "xgb_best5.fit(train_x, train_y_delta)\n",
    "\n",
    "xgb_train = xgb_best5.predict(train_x)\n",
    "xgb_test = xgb_best5.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80603547",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(surface_tp_train_hybrid_stn5.index, surface_tp_train_hybrid_stn5.loc[:, 'ts'], label=\"원본\")\n",
    "\n",
    "plt.plot(train_x.index, xgb_train, label=\"train\")\n",
    "\n",
    "plt.plot(test_x.index, xgb_test, label=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba1440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train = [i+j for i,j in zip(sum(trend_train.tolist(), []), xgb_train.tolist())]\n",
    "\n",
    "original_test = [i+j for i,j in zip(sum(trend_test.tolist(), []), xgb_test.tolist())]\n",
    "\n",
    "plt.plot(surface_tp_train_hybrid_stn5.index, surface_tp_train_hybrid_stn5.loc[:, 'ts'], label=\"원본\")\n",
    "\n",
    "plt.plot(train_x.index, original_train, label=\"train\")\n",
    "\n",
    "plt.plot(test_x.index, original_test, label=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5db0e56",
   "metadata": {},
   "source": [
    "### 지점6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ef832",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_tp_train_hybrid = surface_tp_train.copy()\n",
    "\n",
    "surface_tp_train_hybrid_stn6 = surface_tp_train_hybrid[surface_tp_train_hybrid.stn == 6].reset_index(drop=True)\n",
    "\n",
    "start_date = \"1900-02-01 00:00:00\"  # 시작 날짜\n",
    "end_date = \"1905-01-31 23:00:00\"  # 종료 날짜\n",
    "freq = \"H\"  # 월 단위로 설정\n",
    "\n",
    "surface_tp_train_hybrid_stn6.index = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "\n",
    "surface_tp_train_hybrid_stn6.drop(['stn', 'mmddhh', 'year'], axis=1, inplace=True)\n",
    "\n",
    "surface_tp_train_hybrid_stn6 = pd.get_dummies(data=surface_tp_train_hybrid_stn6, drop_first=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = ['ts']\n",
    "f_col = ['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn', 'ww_F',\n",
    "       'ww_G', 'ww_H', 'ww_R', 'ww_S', 'ww_X']\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "                                                    surface_tp_train_hybrid_stn6[f_col],\n",
    "                                                    surface_tp_train_hybrid_stn6[target],\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=False,\n",
    "                                                    )\n",
    "\n",
    "from statsmodels.tsa.deterministic import DeterministicProcess\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "dp = DeterministicProcess(\n",
    "                          index = train_x.index,\n",
    "                          constant = True,\n",
    "                          order = 1,\n",
    "                          drop = True,\n",
    "                          )\n",
    "\n",
    "X = dp.in_sample()\n",
    "lm6 = LinearRegression(fit_intercept=False)\n",
    "lm6.fit(X, train_y)\n",
    "\n",
    "test_const = [1 for i in range(test_x.shape[0])]\n",
    "test_trend = np.arange(X['trend'].max(), X['trend'].max()+test_x.shape[0])\n",
    "# test_trend_squared = [i**2 for i in test_trend]\n",
    "X_ = pd.DataFrame([test_const, test_trend]).T\n",
    "X_.columns = ['const', 'trend']\n",
    "X_.index = test_x.index\n",
    "\n",
    "trend_train = lm6.predict(X)\n",
    "trend_test = lm6.predict(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f73cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "train_y_delta = train_y - trend_train\n",
    "test_y_delta = test_y - trend_test\n",
    "\n",
    "# xgb= xgboost.XGBRegressor()\n",
    "# xgb.fit(train_x, train_y_delta)\n",
    "\n",
    "# xgb_train = xgb.predict(train_x)\n",
    "# xgb_test = xgb.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c376e494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# misc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71b6950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_params_fold_start(XGB_params, X, y):\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    pred_list = []\n",
    "    mae_list = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "\n",
    "        x_train, x_val, y_train, y_val = X.iloc[train_index], X.iloc[val_index], y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        model = xgboost.XGBRegressor(**XGB_params)\n",
    "\n",
    "        model.fit(x_train, y_train,\n",
    "                eval_set=[(x_val,y_val)],\n",
    "                eval_metric='mae', verbose=False, early_stopping_rounds=100)\n",
    "\n",
    "        pred = model.predict(x_val)\n",
    "        result = mean_absolute_error(pred,y_val)\n",
    "\n",
    "        mae_list.append(result)\n",
    "\n",
    "    return np.mean(mae_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b5c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectiveXGB(trial: Trial, X, y):\n",
    "    param = {\n",
    "        'n_estimators' : trial.suggest_int('n_estimators', 500, 4000),\n",
    "        'max_depth' : trial.suggest_int('max_depth', 8, 16),\n",
    "        'min_child_weight' : trial.suggest_int('min_child_weight', 1, 300),\n",
    "        'gamma' : trial.suggest_int('gamma', 1, 3),\n",
    "        'learning_rate' : 0.01,\n",
    "        'colsample_bytree' : trial.suggest_discrete_uniform('colsample_bytree', 0.5, 1, 0.1),\n",
    "        'nthread' : -1,\n",
    "        'tree_method' : 'gpu_hist',\n",
    "        'predictor' : 'gpu_predictor',\n",
    "        'lambda' : trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "        'alpha' : trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "        'subsample' : trial.suggest_categorical('subsample', [0.6,0.7,0.8,1.0]),\n",
    "        'random_state' : 1127,\n",
    "        'objective' : 'reg:absoluteerror'\n",
    "    }\n",
    "\n",
    "    score = XGB_params_fold_start(param, X, y)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71deb7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize',sampler=TPESampler(seed=42))\n",
    "\n",
    "study.optimize(lambda trial : objectiveXGB(trial, train_x, train_y_delta), n_trials=30)\n",
    "\n",
    "print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936ad3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best6 = xgboost.XGBRegressor(**study.best_trial.params, **{'tree_method' : 'gpu_hist', 'predictor' : 'gpu_predictor'})\n",
    "xgb_best6.fit(train_x, train_y_delta)\n",
    "\n",
    "xgb_train = xgb_best6.predict(train_x)\n",
    "xgb_test = xgb_best6.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee0855",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train = [i+j for i,j in zip(sum(trend_train.tolist(), []), xgb_train.tolist())]\n",
    "\n",
    "original_test = [i+j for i,j in zip(sum(trend_test.tolist(), []), xgb_test.tolist())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df95b1a",
   "metadata": {},
   "source": [
    "### 지점 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3706af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_tp_train_hybrid = surface_tp_train.copy()\n",
    "\n",
    "surface_tp_train_hybrid_stn7 = surface_tp_train_hybrid[surface_tp_train_hybrid.stn == 7].reset_index(drop=True)\n",
    "\n",
    "start_date = \"1900-02-01 00:00:00\"  # 시작 날짜\n",
    "end_date = \"1905-01-31 23:00:00\"  # 종료 날짜\n",
    "freq = \"H\"  # 월 단위로 설정\n",
    "\n",
    "surface_tp_train_hybrid_stn7.index = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "\n",
    "surface_tp_train_hybrid_stn7.drop(['stn', 'mmddhh', 'year'], axis=1, inplace=True)\n",
    "\n",
    "surface_tp_train_hybrid_stn7 = pd.get_dummies(data=surface_tp_train_hybrid_stn7, drop_first=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = ['ts']\n",
    "f_col = ['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn', 'ww_F',\n",
    "       'ww_G', 'ww_H', 'ww_R', 'ww_S', 'ww_X']\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "                                                    surface_tp_train_hybrid_stn7[f_col],\n",
    "                                                    surface_tp_train_hybrid_stn7[target],\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=False,\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3b4045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.deterministic import DeterministicProcess\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "dp = DeterministicProcess(\n",
    "                          index = train_x.index,\n",
    "                          constant = True,\n",
    "                          order = 1,\n",
    "                          drop = True,\n",
    "                          )\n",
    "\n",
    "X = dp.in_sample()\n",
    "lm7 = LinearRegression(fit_intercept=False)\n",
    "lm7.fit(X, train_y)\n",
    "\n",
    "test_const = [1 for i in range(test_x.shape[0])]\n",
    "test_trend = np.arange(X['trend'].max(), X['trend'].max()+test_x.shape[0])\n",
    "# test_trend_squared = [i**2 for i in test_trend]\n",
    "X_ = pd.DataFrame([test_const, test_trend]).T\n",
    "X_.columns = ['const', 'trend']\n",
    "X_.index = test_x.index\n",
    "\n",
    "trend_train = lm7.predict(X)\n",
    "trend_test = lm7.predict(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef74154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "train_y_delta = train_y - trend_train\n",
    "test_y_delta = test_y - trend_test\n",
    "\n",
    "# xgb= xgboost.XGBRegressor()\n",
    "# xgb.fit(train_x, train_y_delta)\n",
    "\n",
    "# xgb_train = xgb.predict(train_x)\n",
    "# xgb_test = xgb.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869aca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_params_fold_start(XGB_params, X, y):\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    pred_list = []\n",
    "    mae_list = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "\n",
    "        x_train, x_val, y_train, y_val = X.iloc[train_index], X.iloc[val_index], y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        model = xgboost.XGBRegressor(**XGB_params)\n",
    "\n",
    "        model.fit(x_train, y_train,\n",
    "                eval_set=[(x_val,y_val)],\n",
    "                eval_metric='mae', verbose=False, early_stopping_rounds=100)\n",
    "\n",
    "        pred = model.predict(x_val)\n",
    "        result = mean_absolute_error(pred,y_val)\n",
    "\n",
    "        mae_list.append(result)\n",
    "\n",
    "    return np.mean(mae_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6804fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectiveXGB(trial: Trial, X, y):\n",
    "    param = {\n",
    "        'n_estimators' : trial.suggest_int('n_estimators', 500, 4000),\n",
    "        'max_depth' : trial.suggest_int('max_depth', 8, 16),\n",
    "        'min_child_weight' : trial.suggest_int('min_child_weight', 1, 300),\n",
    "        'gamma' : trial.suggest_int('gamma', 1, 3),\n",
    "        'learning_rate' : 0.01,\n",
    "        'colsample_bytree' : trial.suggest_discrete_uniform('colsample_bytree', 0.5, 1, 0.1),\n",
    "        'nthread' : -1,\n",
    "        'tree_method' : 'gpu_hist',\n",
    "        'predictor' : 'gpu_predictor',\n",
    "        'lambda' : trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "        'alpha' : trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "        'subsample' : trial.suggest_categorical('subsample', [0.6,0.7,0.8,1.0]),\n",
    "        'random_state' : 1127,\n",
    "        'objective' : 'reg:absoluteerror'\n",
    "    }\n",
    "\n",
    "    score = XGB_params_fold_start(param, X, y)\n",
    "\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction='minimize',sampler=TPESampler(seed=42))\n",
    "\n",
    "study.optimize(lambda trial : objectiveXGB(trial, train_x, train_y_delta), n_trials=30)\n",
    "\n",
    "print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f58b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best7 = xgboost.XGBRegressor(**study.best_trial.params, **{'tree_method' : 'gpu_hist', 'predictor' : 'gpu_predictor'})\n",
    "xgb_best7.fit(train_x, train_y_delta)\n",
    "\n",
    "xgb_train = xgb_best7.predict(train_x)\n",
    "xgb_test = xgb_best7.predict(test_x)\n",
    "\n",
    "plt.plot(surface_tp_train_hybrid_stn7.index, surface_tp_train_hybrid_stn7.loc[:, 'ts'], label=\"원본\")\n",
    "\n",
    "plt.plot(train_x.index, xgb_train, label=\"train\")\n",
    "\n",
    "plt.plot(test_x.index, xgb_test, label=\"train\")\n",
    "\n",
    "original_train = [i+j for i,j in zip(sum(trend_train.tolist(), []), xgb_train.tolist())]\n",
    "\n",
    "original_test = [i+j for i,j in zip(sum(trend_test.tolist(), []), xgb_test.tolist())]\n",
    "\n",
    "plt.plot(surface_tp_train_hybrid_stn7.index, surface_tp_train_hybrid_stn7.loc[:, 'ts'], label=\"원본\")\n",
    "\n",
    "plt.plot(train_x.index, original_train, label=\"train\")\n",
    "\n",
    "plt.plot(test_x.index, original_test, label=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217d17c5",
   "metadata": {},
   "source": [
    "### 지점 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df98a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_tp_train_hybrid = surface_tp_train.copy()\n",
    "\n",
    "surface_tp_train_hybrid_stn8 = surface_tp_train_hybrid[surface_tp_train_hybrid.stn == 8].reset_index(drop=True)\n",
    "\n",
    "start_date = \"1900-02-01 00:00:00\"  # 시작 날짜\n",
    "end_date = \"1905-01-31 23:00:00\"  # 종료 날짜\n",
    "freq = \"H\"  # 월 단위로 설정\n",
    "\n",
    "surface_tp_train_hybrid_stn8.index = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "\n",
    "surface_tp_train_hybrid_stn8.drop(['stn', 'mmddhh', 'year'], axis=1, inplace=True)\n",
    "\n",
    "surface_tp_train_hybrid_stn8 = pd.get_dummies(data=surface_tp_train_hybrid_stn8, drop_first=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = ['ts']\n",
    "f_col = ['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn', 'ww_F',\n",
    "       'ww_G', 'ww_H', 'ww_R', 'ww_S', 'ww_X']\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "                                                    surface_tp_train_hybrid_stn8[f_col],\n",
    "                                                    surface_tp_train_hybrid_stn8[target],\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=False,\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfe4b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.deterministic import DeterministicProcess\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "dp = DeterministicProcess(\n",
    "                          index = train_x.index,\n",
    "                          constant = True,\n",
    "                          order = 1,\n",
    "                          drop = True,\n",
    "                          )\n",
    "\n",
    "X = dp.in_sample()\n",
    "lm8 = LinearRegression(fit_intercept=False)\n",
    "lm8.fit(X, train_y)\n",
    "\n",
    "test_const = [1 for i in range(test_x.shape[0])]\n",
    "test_trend = np.arange(X['trend'].max(), X['trend'].max()+test_x.shape[0])\n",
    "# test_trend_squared = [i**2 for i in test_trend]\n",
    "X_ = pd.DataFrame([test_const, test_trend]).T\n",
    "X_.columns = ['const', 'trend']\n",
    "X_.index = test_x.index\n",
    "\n",
    "trend_train = lm8.predict(X)\n",
    "trend_test = lm8.predict(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aac08ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(surface_tp_train_hybrid_stn8.index, surface_tp_train_hybrid_stn8.loc[:, 'ts'], label=\"원본\")\n",
    "\n",
    "plt.plot(train_x.index, trend_train, label=\"train\")\n",
    "\n",
    "plt.plot(test_x.index, trend_test, label=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a58f99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "train_y_delta = train_y - trend_train\n",
    "test_y_delta = test_y - trend_test\n",
    "\n",
    "# xgb= xgboost.XGBRegressor()\n",
    "# xgb.fit(train_x, train_y_delta)\n",
    "\n",
    "# xgb_train = xgb.predict(train_x)\n",
    "# xgb_test = xgb.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0065992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# misc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d59b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_params_fold_start(XGB_params, X, y):\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    pred_list = []\n",
    "    mae_list = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "\n",
    "        x_train, x_val, y_train, y_val = X.iloc[train_index], X.iloc[val_index], y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        model = xgboost.XGBRegressor(**XGB_params)\n",
    "\n",
    "        model.fit(x_train, y_train,\n",
    "                eval_set=[(x_val,y_val)],\n",
    "                eval_metric='mae', verbose=False, early_stopping_rounds=100)\n",
    "\n",
    "        pred = model.predict(x_val)\n",
    "        result = mean_absolute_error(pred,y_val)\n",
    "\n",
    "        mae_list.append(result)\n",
    "\n",
    "    return np.mean(mae_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb127cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectiveXGB(trial: Trial, X, y):\n",
    "    param = {\n",
    "        'n_estimators' : trial.suggest_int('n_estimators', 500, 4000),\n",
    "        'max_depth' : trial.suggest_int('max_depth', 8, 16),\n",
    "        'min_child_weight' : trial.suggest_int('min_child_weight', 1, 300),\n",
    "        'gamma' : trial.suggest_int('gamma', 1, 3),\n",
    "        'learning_rate' : 0.01,\n",
    "        'colsample_bytree' : trial.suggest_discrete_uniform('colsample_bytree', 0.5, 1, 0.1),\n",
    "        'nthread' : -1,\n",
    "        'tree_method' : 'gpu_hist',\n",
    "        'predictor' : 'gpu_predictor',\n",
    "        'lambda' : trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "        'alpha' : trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "        'subsample' : trial.suggest_categorical('subsample', [0.6,0.7,0.8,1.0]),\n",
    "        'random_state' : 1127,\n",
    "        'objective' : 'reg:absoluteerror'\n",
    "    }\n",
    "\n",
    "    score = XGB_params_fold_start(param, X, y)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ff12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize',sampler=TPESampler(seed=42))\n",
    "\n",
    "study.optimize(lambda trial : objectiveXGB(trial, train_x, train_y_delta), n_trials=30)\n",
    "\n",
    "print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b01c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best8 = xgboost.XGBRegressor(**study.best_trial.params, **{'tree_method' : 'gpu_hist', 'predictor' : 'gpu_predictor'})\n",
    "xgb_best8.fit(train_x, train_y_delta)\n",
    "\n",
    "xgb_train = xgb_best8.predict(train_x)\n",
    "xgb_test = xgb_best8.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf16138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train = [i+j for i,j in zip(sum(trend_train.tolist(), []), xgb_train.tolist())]\n",
    "\n",
    "original_test = [i+j for i,j in zip(sum(trend_test.tolist(), []), xgb_test.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964e2164",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_tp_train_hybrid = surface_tp_train.copy()\n",
    "\n",
    "surface_tp_train_hybrid_stn9 = surface_tp_train_hybrid[surface_tp_train_hybrid.stn == 9].reset_index(drop=True)\n",
    "\n",
    "start_date = \"1900-02-01 00:00:00\"  # 시작 날짜\n",
    "end_date = \"1905-01-31 23:00:00\"  # 종료 날짜\n",
    "freq = \"H\"  # 월 단위로 설정\n",
    "\n",
    "surface_tp_train_hybrid_stn9.index = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "\n",
    "surface_tp_train_hybrid_stn9.drop(['stn', 'mmddhh', 'year'], axis=1, inplace=True)\n",
    "\n",
    "surface_tp_train_hybrid_stn9 = pd.get_dummies(data=surface_tp_train_hybrid_stn9, drop_first=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = ['ts']\n",
    "f_col = ['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn', 'ww_F',\n",
    "       'ww_G', 'ww_H', 'ww_R', 'ww_S', 'ww_X']\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "                                                    surface_tp_train_hybrid_stn9[f_col],\n",
    "                                                    surface_tp_train_hybrid_stn9[target],\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=False,\n",
    "                                                    )\n",
    "\n",
    "from statsmodels.tsa.deterministic import DeterministicProcess\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "dp = DeterministicProcess(\n",
    "                          index = train_x.index,\n",
    "                          constant = True,\n",
    "                          order = 1,\n",
    "                          drop = True,\n",
    "                          )\n",
    "\n",
    "X = dp.in_sample()\n",
    "lm9 = LinearRegression(fit_intercept=False)\n",
    "lm9.fit(X, train_y)\n",
    "\n",
    "test_const = [1 for i in range(test_x.shape[0])]\n",
    "test_trend = np.arange(X['trend'].max(), X['trend'].max()+test_x.shape[0])\n",
    "# test_trend_squared = [i**2 for i in test_trend]\n",
    "X_ = pd.DataFrame([test_const, test_trend]).T\n",
    "X_.columns = ['const', 'trend']\n",
    "X_.index = test_x.index\n",
    "\n",
    "trend_train = lm9.predict(X)\n",
    "trend_test = lm9.predict(X_)\n",
    "\n",
    "import xgboost\n",
    "\n",
    "train_y_delta = train_y - trend_train\n",
    "test_y_delta = test_y - trend_test\n",
    "\n",
    "# xgb= xgboost.XGBRegressor()\n",
    "# xgb.fit(train_x, train_y_delta)\n",
    "\n",
    "# xgb_train = xgb.predict(train_x)\n",
    "# xgb_test = xgb.predict(test_x)\n",
    "\n",
    "# model\n",
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# misc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def XGB_params_fold_start(XGB_params, X, y):\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    pred_list = []\n",
    "    mae_list = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "\n",
    "        x_train, x_val, y_train, y_val = X.iloc[train_index], X.iloc[val_index], y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        model = xgboost.XGBRegressor(**XGB_params)\n",
    "\n",
    "        model.fit(x_train, y_train,\n",
    "                eval_set=[(x_val,y_val)],\n",
    "                eval_metric='mae', verbose=False, early_stopping_rounds=100)\n",
    "\n",
    "        pred = model.predict(x_val)\n",
    "        result = mean_absolute_error(pred,y_val)\n",
    "\n",
    "        mae_list.append(result)\n",
    "\n",
    "    return np.mean(mae_list)\n",
    "\n",
    "def objectiveXGB(trial: Trial, X, y):\n",
    "    param = {\n",
    "        'n_estimators' : trial.suggest_int('n_estimators', 500, 4000),\n",
    "        'max_depth' : trial.suggest_int('max_depth', 8, 16),\n",
    "        'min_child_weight' : trial.suggest_int('min_child_weight', 1, 300),\n",
    "        'gamma' : trial.suggest_int('gamma', 1, 3),\n",
    "        'learning_rate' : 0.01,\n",
    "        'colsample_bytree' : trial.suggest_discrete_uniform('colsample_bytree', 0.5, 1, 0.1),\n",
    "        'nthread' : -1,\n",
    "        'tree_method' : 'gpu_hist',\n",
    "        'predictor' : 'gpu_predictor',\n",
    "        'lambda' : trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "        'alpha' : trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "        'subsample' : trial.suggest_categorical('subsample', [0.6,0.7,0.8,1.0]),\n",
    "        'random_state' : 1127,\n",
    "        'objective' : 'reg:absoluteerror'\n",
    "    }\n",
    "\n",
    "    score = XGB_params_fold_start(param, X, y)\n",
    "\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction='minimize',sampler=TPESampler(seed=42))\n",
    "\n",
    "study.optimize(lambda trial : objectiveXGB(trial, train_x, train_y_delta), n_trials=30)\n",
    "\n",
    "print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))\n",
    "\n",
    "xgb_best9 = xgboost.XGBRegressor(**study.best_trial.params, **{'tree_method' : 'gpu_hist', 'predictor' : 'gpu_predictor'})\n",
    "xgb_best9.fit(train_x, train_y_delta)\n",
    "\n",
    "xgb_train = xgb_best9.predict(train_x)\n",
    "xgb_test = xgb_best9.predict(test_x)\n",
    "\n",
    "original_train = [i+j for i,j in zip(sum(trend_train.tolist(), []), xgb_train.tolist())]\n",
    "\n",
    "original_test = [i+j for i,j in zip(sum(trend_test.tolist(), []), xgb_test.tolist())]\n",
    "\n",
    "plt.plot(surface_tp_train_hybrid_stn9.index, surface_tp_train_hybrid_stn9.loc[:, 'ts'], label=\"원본\")\n",
    "\n",
    "plt.plot(train_x.index, original_train, label=\"train\")\n",
    "\n",
    "plt.plot(test_x.index, original_test, label=\"train\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09abe48a",
   "metadata": {},
   "source": [
    "### 지점 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150758d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_tp_train_hybrid = surface_tp_train.copy()\n",
    "\n",
    "surface_tp_train_hybrid_stn10 = surface_tp_train_hybrid[surface_tp_train_hybrid.stn == 10].reset_index(drop=True)\n",
    "\n",
    "start_date = \"1900-02-01 00:00:00\"  # 시작 날짜\n",
    "end_date = \"1905-01-31 23:00:00\"  # 종료 날짜\n",
    "freq = \"H\"  # 월 단위로 설정\n",
    "\n",
    "surface_tp_train_hybrid_stn10.index = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "\n",
    "surface_tp_train_hybrid_stn10.drop(['stn', 'mmddhh', 'year'], axis=1, inplace=True)\n",
    "\n",
    "surface_tp_train_hybrid_stn10 = pd.get_dummies(data=surface_tp_train_hybrid_stn10, drop_first=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = ['ts']\n",
    "f_col = ['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn', 'ww_F',\n",
    "       'ww_G', 'ww_H', 'ww_R', 'ww_S', 'ww_X']\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "                                                    surface_tp_train_hybrid_stn10[f_col],\n",
    "                                                    surface_tp_train_hybrid_stn10[target],\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=False,\n",
    "                                                    )\n",
    "\n",
    "from statsmodels.tsa.deterministic import DeterministicProcess\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "dp = DeterministicProcess(\n",
    "                          index = train_x.index,\n",
    "                          constant = True,\n",
    "                          order = 1,\n",
    "                          drop = True,\n",
    "                          )\n",
    "\n",
    "X = dp.in_sample()\n",
    "lm10 = LinearRegression(fit_intercept=False)\n",
    "lm10.fit(X, train_y)\n",
    "\n",
    "test_const = [1 for i in range(test_x.shape[0])]\n",
    "test_trend = np.arange(X['trend'].max(), X['trend'].max()+test_x.shape[0])\n",
    "# test_trend_squared = [i**2 for i in test_trend]\n",
    "X_ = pd.DataFrame([test_const, test_trend]).T\n",
    "X_.columns = ['const', 'trend']\n",
    "X_.index = test_x.index\n",
    "\n",
    "trend_train = lm10.predict(X)\n",
    "trend_test = lm10.predict(X_)\n",
    "\n",
    "import xgboost\n",
    "\n",
    "train_y_delta = train_y - trend_train\n",
    "test_y_delta = test_y - trend_test\n",
    "\n",
    "# model\n",
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# misc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def XGB_params_fold_start(XGB_params, X, y):\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    pred_list = []\n",
    "    mae_list = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "\n",
    "        x_train, x_val, y_train, y_val = X.iloc[train_index], X.iloc[val_index], y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        model = xgboost.XGBRegressor(**XGB_params)\n",
    "\n",
    "        model.fit(x_train, y_train,\n",
    "                eval_set=[(x_val,y_val)],\n",
    "                eval_metric='mae', verbose=False, early_stopping_rounds=100)\n",
    "\n",
    "        pred = model.predict(x_val)\n",
    "        result = mean_absolute_error(pred,y_val)\n",
    "\n",
    "        mae_list.append(result)\n",
    "\n",
    "    return np.mean(mae_list)\n",
    "\n",
    "def objectiveXGB(trial: Trial, X, y):\n",
    "    param = {\n",
    "        'n_estimators' : trial.suggest_int('n_estimators', 500, 4000),\n",
    "        'max_depth' : trial.suggest_int('max_depth', 8, 16),\n",
    "        'min_child_weight' : trial.suggest_int('min_child_weight', 1, 300),\n",
    "        'gamma' : trial.suggest_int('gamma', 1, 3),\n",
    "        'learning_rate' : 0.01,\n",
    "        'colsample_bytree' : trial.suggest_discrete_uniform('colsample_bytree', 0.5, 1, 0.1),\n",
    "        'nthread' : -1,\n",
    "        'tree_method' : 'gpu_hist',\n",
    "        'predictor' : 'gpu_predictor',\n",
    "        'lambda' : trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "        'alpha' : trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "        'subsample' : trial.suggest_categorical('subsample', [0.6,0.7,0.8,1.0]),\n",
    "        'random_state' : 1127,\n",
    "        'objective' : 'reg:absoluteerror'\n",
    "    }\n",
    "\n",
    "    score = XGB_params_fold_start(param, X, y)\n",
    "\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction='minimize',sampler=TPESampler(seed=42))\n",
    "\n",
    "study.optimize(lambda trial : objectiveXGB(trial, train_x, train_y_delta), n_trials=30)\n",
    "\n",
    "print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))\n",
    "\n",
    "xgb_best10 = xgboost.XGBRegressor(**study.best_trial.params, **{'tree_method' : 'gpu_hist', 'predictor' : 'gpu_predictor'})\n",
    "xgb_best10.fit(train_x, train_y_delta)\n",
    "\n",
    "xgb_train = xgb_best10.predict(train_x)\n",
    "xgb_test = xgb_best10.predict(test_x)\n",
    "\n",
    "original_train = [i+j for i,j in zip(sum(trend_train.tolist(), []), xgb_train.tolist())]\n",
    "\n",
    "original_test = [i+j for i,j in zip(sum(trend_test.tolist(), []), xgb_test.tolist())]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec638ed1",
   "metadata": {},
   "source": [
    "## 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d932bcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_tp_test_hybrid = surface_tp_test.copy()\n",
    "\n",
    "# 기상청 설명자료 속 선행지식 활용 (지점 abc)\n",
    "surface_tp_test_hybrid_stn1 = surface_tp_test_hybrid[surface_tp_test_hybrid.stn == 'a'].reset_index(drop=True)\n",
    "surface_tp_test_hybrid_stn2 = surface_tp_test_hybrid[surface_tp_test_hybrid.stn == 'b'].reset_index(drop=True)\n",
    "surface_tp_test_hybrid_stn3 = surface_tp_test_hybrid[surface_tp_test_hybrid.stn == 'c'].reset_index(drop=True)\n",
    "\n",
    "# 기상청 설명자료 속 선행지식 활용 (F~G년)\n",
    "start_date = \"1906-02-01 00:00:00\"  # 시작 날짜\n",
    "end_date = \"1907-01-31 23:00:00\"  # 종료 날짜\n",
    "freq = \"H\"  # 월 단위로 설정\n",
    "\n",
    "surface_tp_test_hybrid_stn1.index = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "surface_tp_test_hybrid_stn2.index = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "surface_tp_test_hybrid_stn3.index = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "\n",
    "surface_tp_test_hybrid_stn1.drop(['stn', 'mmddhh', 'year'], axis=1, inplace=True)\n",
    "surface_tp_test_hybrid_stn2.drop(['stn', 'mmddhh', 'year'], axis=1, inplace=True)\n",
    "surface_tp_test_hybrid_stn3.drop(['stn', 'mmddhh', 'year'], axis=1, inplace=True)\n",
    "\n",
    "surface_tp_test_hybrid_stn1 = pd.get_dummies(data=surface_tp_test_hybrid_stn1, drop_first=True)\n",
    "surface_tp_test_hybrid_stn2 = pd.get_dummies(data=surface_tp_test_hybrid_stn2, drop_first=True)\n",
    "surface_tp_test_hybrid_stn3 = pd.get_dummies(data=surface_tp_test_hybrid_stn3, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d9ac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = DeterministicProcess(\n",
    "                          index = surface_tp_train_hybrid_stn1.index,\n",
    "                          constant = True,\n",
    "                          order = 1,\n",
    "                          drop = True,\n",
    "                          )\n",
    "\n",
    "X = dp.in_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8690e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['ts']\n",
    "f_col = ['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn', 'ww_F',\n",
    "       'ww_G', 'ww_H', 'ww_R', 'ww_S', 'ww_X']\n",
    "\n",
    "lm1.fit(X, surface_tp_train_hybrid_stn1[target])\n",
    "lm2.fit(X, surface_tp_train_hybrid_stn2[target])\n",
    "lm3.fit(X, surface_tp_train_hybrid_stn3[target])\n",
    "lm4.fit(X, surface_tp_train_hybrid_stn4[target])\n",
    "lm5.fit(X, surface_tp_train_hybrid_stn5[target])\n",
    "lm6.fit(X, surface_tp_train_hybrid_stn6[target])\n",
    "lm7.fit(X, surface_tp_train_hybrid_stn7[target])\n",
    "lm8.fit(X, surface_tp_train_hybrid_stn8[target])\n",
    "lm9.fit(X, surface_tp_train_hybrid_stn9[target])\n",
    "lm10.fit(X, surface_tp_train_hybrid_stn10[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8fab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지점별 train data 추세 추출\n",
    "trend_stn1 = lm1.predict(X)\n",
    "trend_stn2 = lm2.predict(X)\n",
    "trend_stn3 = lm3.predict(X)\n",
    "trend_stn4 = lm4.predict(X)\n",
    "trend_stn5 = lm5.predict(X)\n",
    "trend_stn6 = lm6.predict(X)\n",
    "trend_stn7 = lm7.predict(X)\n",
    "trend_stn8 = lm8.predict(X)\n",
    "trend_stn9 = lm9.predict(X)\n",
    "trend_stn10 = lm10.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed587d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추세를 제거한 계절성만 남은 전체 train data로 학습\n",
    "xgb_best1.fit(surface_tp_train_hybrid_stn1[f_col], [i-j for i,j in zip(surface_tp_train_hybrid_stn1[target].ts.tolist(),  sum(trend_stn1.tolist(), []))])\n",
    "xgb_best2.fit(surface_tp_train_hybrid_stn2[f_col], [i-j for i,j in zip(surface_tp_train_hybrid_stn2[target].ts.tolist(),  sum(trend_stn2.tolist(), []))])\n",
    "xgb_best3.fit(surface_tp_train_hybrid_stn3[f_col], [i-j for i,j in zip(surface_tp_train_hybrid_stn3[target].ts.tolist(),  sum(trend_stn3.tolist(), []))])\n",
    "xgb_best4.fit(surface_tp_train_hybrid_stn4[f_col], [i-j for i,j in zip(surface_tp_train_hybrid_stn4[target].ts.tolist(),  sum(trend_stn4.tolist(), []))])\n",
    "xgb_best5.fit(surface_tp_train_hybrid_stn5[f_col], [i-j for i,j in zip(surface_tp_train_hybrid_stn5[target].ts.tolist(),  sum(trend_stn5.tolist(), []))])\n",
    "xgb_best6.fit(surface_tp_train_hybrid_stn6[f_col], [i-j for i,j in zip(surface_tp_train_hybrid_stn6[target].ts.tolist(),  sum(trend_stn6.tolist(), []))])\n",
    "xgb_best7.fit(surface_tp_train_hybrid_stn7[f_col], [i-j for i,j in zip(surface_tp_train_hybrid_stn7[target].ts.tolist(),  sum(trend_stn7.tolist(), []))])\n",
    "xgb_best8.fit(surface_tp_train_hybrid_stn8[f_col], [i-j for i,j in zip(surface_tp_train_hybrid_stn8[target].ts.tolist(),  sum(trend_stn8.tolist(), []))])\n",
    "xgb_best9.fit(surface_tp_train_hybrid_stn9[f_col], [i-j for i,j in zip(surface_tp_train_hybrid_stn9[target].ts.tolist(),  sum(trend_stn9.tolist(), []))])\n",
    "xgb_best10.fit(surface_tp_train_hybrid_stn10[f_col], [i-j for i,j in zip(surface_tp_train_hybrid_stn10[target].ts.tolist(),  sum(trend_stn10.tolist(), []))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef22584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 없는 열은 0으로 설정\n",
    "surface_tp_test_hybrid_stn1['ww_X'] = 0\n",
    "surface_tp_test_hybrid_stn2['ww_X'] = 0\n",
    "surface_tp_test_hybrid_stn3['ww_X'] = 0\n",
    "\n",
    "surface_tp_test_hybrid_stn3['ww_H'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7974da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지점 a\n",
    "# a_trend_lm1 = lm1.predict(surface_tp_test_hybrid_stn1[f_col])\n",
    "# a_trend_lm2 = lm2.predict(surface_tp_test_hybrid_stn1[f_col])\n",
    "# a_trend_lm3 = lm3.predict(surface_tp_test_hybrid_stn1[f_col])\n",
    "# a_trend_lm4 = lm4.predict(surface_tp_test_hybrid_stn1[f_col])\n",
    "# a_trend_lm5 = lm5.predict(surface_tp_test_hybrid_stn1[f_col])\n",
    "# a_trend_lm6 = lm6.predict(surface_tp_test_hybrid_stn1[f_col])\n",
    "# a_trend_lm7 = lm7.predict(surface_tp_test_hybrid_stn1[f_col])\n",
    "# a_trend_lm8 = lm8.predict(surface_tp_test_hybrid_stn1[f_col])\n",
    "# a_trend_lm9 = lm9.predict(surface_tp_test_hybrid_stn1[f_col])\n",
    "# a_trend_lm10 = lm10.predict(surface_tp_test_hybrid_stn1[f_col])\n",
    "\n",
    "a_seasonality_xgb_best1 = xgb_best1.predict(surface_tp_test_hybrid_stn1[f_col])\n",
    "a_seasonality_xgb_best2 = xgb_best2.predict(surface_tp_test_hybrid_stn1[f_col])\n",
    "a_seasonality_xgb_best3 = xgb_best3.predict(surface_tp_test_hybrid_stn1[f_col])\n",
    "a_seasonality_xgb_best4 = xgb_best4.predict(surface_tp_test_hybrid_stn1[f_col])\n",
    "a_seasonality_xgb_best5 = xgb_best5.predict(surface_tp_test_hybrid_stn1[f_col])\n",
    "a_seasonality_xgb_best6 = xgb_best6.predict(surface_tp_test_hybrid_stn1[f_col])\n",
    "a_seasonality_xgb_best7 = xgb_best7.predict(surface_tp_test_hybrid_stn1[f_col])\n",
    "a_seasonality_xgb_best8 = xgb_best8.predict(surface_tp_test_hybrid_stn1[f_col])\n",
    "a_seasonality_xgb_best9 = xgb_best9.predict(surface_tp_test_hybrid_stn1[f_col])\n",
    "a_seasonality_xgb_best10 = xgb_best10.predict(surface_tp_test_hybrid_stn1[f_col])\n",
    "\n",
    "# 지점 b\n",
    "# b_trend_lm1 = lm1.predict(surface_tp_test_hybrid_stn2[f_col])\n",
    "# b_trend_lm2 = lm2.predict(surface_tp_test_hybrid_stn2[f_col])\n",
    "# b_trend_lm3 = lm3.predict(surface_tp_test_hybrid_stn2[f_col])\n",
    "# b_trend_lm4 = lm4.predict(surface_tp_test_hybrid_stn2[f_col])\n",
    "# b_trend_lm5 = lm5.predict(surface_tp_test_hybrid_stn2[f_col])\n",
    "# b_trend_lm6 = lm6.predict(surface_tp_test_hybrid_stn2[f_col])\n",
    "# b_trend_lm7 = lm7.predict(surface_tp_test_hybrid_stn2[f_col])\n",
    "# b_trend_lm8 = lm8.predict(surface_tp_test_hybrid_stn2[f_col])\n",
    "# b_trend_lm9 = lm9.predict(surface_tp_test_hybrid_stn2[f_col])\n",
    "# b_trend_lm10 = lm10.predict(surface_tp_test_hybrid_stn2[f_col])\n",
    "\n",
    "b_seasonality_xgb_best1 = xgb_best1.predict(surface_tp_test_hybrid_stn2[f_col])\n",
    "b_seasonality_xgb_best2 = xgb_best2.predict(surface_tp_test_hybrid_stn2[f_col])\n",
    "b_seasonality_xgb_best3 = xgb_best3.predict(surface_tp_test_hybrid_stn2[f_col])\n",
    "b_seasonality_xgb_best4 = xgb_best4.predict(surface_tp_test_hybrid_stn2[f_col])\n",
    "b_seasonality_xgb_best5 = xgb_best5.predict(surface_tp_test_hybrid_stn2[f_col])\n",
    "b_seasonality_xgb_best6 = xgb_best6.predict(surface_tp_test_hybrid_stn2[f_col])\n",
    "b_seasonality_xgb_best7 = xgb_best7.predict(surface_tp_test_hybrid_stn2[f_col])\n",
    "b_seasonality_xgb_best8 = xgb_best8.predict(surface_tp_test_hybrid_stn2[f_col])\n",
    "b_seasonality_xgb_best9 = xgb_best9.predict(surface_tp_test_hybrid_stn2[f_col])\n",
    "b_seasonality_xgb_best10 = xgb_best10.predict(surface_tp_test_hybrid_stn2[f_col])\n",
    "\n",
    "# 지점 c\n",
    "# c_trend_lm1 = lm1.predict(surface_tp_test_hybrid_stn3[f_col])\n",
    "# c_trend_lm2 = lm2.predict(surface_tp_test_hybrid_stn3[f_col])\n",
    "# c_trend_lm3 = lm3.predict(surface_tp_test_hybrid_stn3[f_col])\n",
    "# c_trend_lm4 = lm4.predict(surface_tp_test_hybrid_stn3[f_col])\n",
    "# c_trend_lm5 = lm5.predict(surface_tp_test_hybrid_stn3[f_col])\n",
    "# c_trend_lm6 = lm6.predict(surface_tp_test_hybrid_stn3[f_col])\n",
    "# c_trend_lm7 = lm7.predict(surface_tp_test_hybrid_stn3[f_col])\n",
    "# c_trend_lm8 = lm8.predict(surface_tp_test_hybrid_stn3[f_col])\n",
    "# c_trend_lm9 = lm9.predict(surface_tp_test_hybrid_stn3[f_col])\n",
    "# c_trend_lm10 = lm10.predict(surface_tp_test_hybrid_stn3[f_col])\n",
    "\n",
    "c_seasonality_xgb_best1 = xgb_best1.predict(surface_tp_test_hybrid_stn3[f_col])\n",
    "c_seasonality_xgb_best2 = xgb_best2.predict(surface_tp_test_hybrid_stn3[f_col])\n",
    "c_seasonality_xgb_best3 = xgb_best3.predict(surface_tp_test_hybrid_stn3[f_col])\n",
    "c_seasonality_xgb_best4 = xgb_best4.predict(surface_tp_test_hybrid_stn3[f_col])\n",
    "c_seasonality_xgb_best5 = xgb_best5.predict(surface_tp_test_hybrid_stn3[f_col])\n",
    "c_seasonality_xgb_best6 = xgb_best6.predict(surface_tp_test_hybrid_stn3[f_col])\n",
    "c_seasonality_xgb_best7 = xgb_best7.predict(surface_tp_test_hybrid_stn3[f_col])\n",
    "c_seasonality_xgb_best8 = xgb_best8.predict(surface_tp_test_hybrid_stn3[f_col])\n",
    "c_seasonality_xgb_best9 = xgb_best9.predict(surface_tp_test_hybrid_stn3[f_col])\n",
    "c_seasonality_xgb_best10 = xgb_best10.predict(surface_tp_test_hybrid_stn3[f_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8211d932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10개 지점 추세 평균값을 사용\n",
    "avg_trend = [(a+b+c+d+e+f+g+h+i+j)/10 for a,b,c,d,e,f,g,h,i,j in zip(sum(trend_stn1.tolist(), []), sum(trend_stn2.tolist(), []), sum(trend_stn3.tolist(), []),\n",
    "                                    sum(trend_stn4.tolist(), []), sum(trend_stn5.tolist(), []), sum(trend_stn6.tolist(), []),\n",
    "                                    sum(trend_stn7.tolist(), []), sum(trend_stn8.tolist(), []), sum(trend_stn9.tolist(), []),\n",
    "                                    sum(trend_stn10.tolist(), []))]\n",
    "\n",
    "# a_trend = [(a+b+c+d+e+f+g+h+i+j)/10 for a,b,c,d,e,f,g,h,i,j in zip(sum(a_trend_lm1.tolist(), []), sum(a_trend_lm2.tolist(), []), sum(a_trend_lm3.tolist(), []),\n",
    "#                                     sum(a_trend_lm4.tolist(), []), sum(a_trend_lm5.tolist(), []), sum(a_trend_lm6.tolist(), []),\n",
    "#                                     sum(a_trend_lm7.tolist(), []), sum(a_trend_lm8.tolist(), []), sum(a_trend_lm9.tolist(), []),\n",
    "#                                     sum(a_trend_lm10.tolist(), []))]\n",
    "\n",
    "a_seasonality = [(a+b+c+d+e+f+g+h+i+j)/10 for a,b,c,d,e,f,g,h,i,j in zip(a_seasonality_xgb_best1.tolist(),a_seasonality_xgb_best2.tolist(), a_seasonality_xgb_best3.tolist(),\n",
    "                                                                         a_seasonality_xgb_best4.tolist(),a_seasonality_xgb_best5.tolist(),a_seasonality_xgb_best6.tolist(),\n",
    "                                                                         a_seasonality_xgb_best7.tolist(),a_seasonality_xgb_best8.tolist(),a_seasonality_xgb_best9.tolist(),\n",
    "                                                                         a_seasonality_xgb_best10.tolist())]\n",
    "\n",
    "a_pred = [i+j for i,j in zip(avg_trend, a_seasonality)]\n",
    "\n",
    "# b_trend = [(a+b+c+d+e+f+g+h+i+j)/10 for a,b,c,d,e,f,g,h,i,j in zip(sum(b_trend_lm1.tolist(), []), sum(b_trend_lm2.tolist(), []), sum(b_trend_lm3.tolist(), []),\n",
    "#                                     sum(b_trend_lm4.tolist(), []), sum(b_trend_lm5.tolist(), []), sum(b_trend_lm6.tolist(), []),\n",
    "#                                     sum(b_trend_lm7.tolist(), []), sum(b_trend_lm8.tolist(), []), sum(b_trend_lm9.tolist(), []),\n",
    "#                                     sum(b_trend_lm10.tolist(), []))]\n",
    "\n",
    "b_seasonality = [(a+b+c+d+e+f+g+h+i+j)/10 for a,b,c,d,e,f,g,h,i,j in zip(b_seasonality_xgb_best1.tolist(),b_seasonality_xgb_best2.tolist(), b_seasonality_xgb_best3.tolist(),\n",
    "                                                                         b_seasonality_xgb_best4.tolist(),b_seasonality_xgb_best5.tolist(), b_seasonality_xgb_best6.tolist(),\n",
    "                                                                         b_seasonality_xgb_best7.tolist(),b_seasonality_xgb_best8.tolist(), b_seasonality_xgb_best9.tolist(),\n",
    "                                                                         b_seasonality_xgb_best10.tolist())]\n",
    "\n",
    "b_pred = [i+j for i,j in zip(avg_trend, b_seasonality)]\n",
    "\n",
    "# c_trend = [(a+b+c+d+e+f+g+h+i+j)/10 for a,b,c,d,e,f,g,h,i,j in zip(sum(c_trend_lm1.tolist(), []), sum(c_trend_lm2.tolist(), []), sum(c_trend_lm3.tolist(), []),\n",
    "#                                     sum(c_trend_lm4.tolist(), []), sum(c_trend_lm5.tolist(), []), sum(c_trend_lm6.tolist(), []),\n",
    "#                                     sum(c_trend_lm7.tolist(), []), sum(c_trend_lm8.tolist(), []), sum(c_trend_lm9.tolist(), []),\n",
    "#                                     sum(c_trend_lm10.tolist(), []))]\n",
    "\n",
    "c_seasonality = [(a+b+c+d+e+f+g+h+i+j)/10 for a,b,c,d,e,f,g,h,i,j in zip(c_seasonality_xgb_best1.tolist(),c_seasonality_xgb_best2.tolist(), c_seasonality_xgb_best3.tolist(),\n",
    "                                                                         c_seasonality_xgb_best4.tolist(),c_seasonality_xgb_best5.tolist(), c_seasonality_xgb_best6.tolist(),\n",
    "                                                                         c_seasonality_xgb_best7.tolist(),c_seasonality_xgb_best8.tolist(), c_seasonality_xgb_best9.tolist(),\n",
    "                                                                         c_seasonality_xgb_best10.tolist())]\n",
    "\n",
    "c_pred = [i+j for i,j in zip(avg_trend, c_seasonality)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72de5e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_tp_test = pd.read_csv('/content/drive/MyDrive/기상청/데이터/지면온도/surface_tp_test.csv')\n",
    "\n",
    "# 불필요 첫 2열 제거\n",
    "surface_tp_test = surface_tp_test.iloc[:, 1:]\n",
    "\n",
    "# 예측값 병합\n",
    "a_pred.extend(b_pred)\n",
    "\n",
    "a_pred.extend(c_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3932a4",
   "metadata": {},
   "source": [
    "# 겨울 모델링 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67efd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.graphics.tsaplots as sgt\n",
    "import missingno as msno\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder , RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "## CV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "## Model\n",
    "from statsmodels.tsa.deterministic import DeterministicProcess\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost\n",
    "import lightgbm as lgb\n",
    "\n",
    "## Tunning\n",
    "import time\n",
    "import re\n",
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.integration import XGBoostPruningCallback\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcab3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fin = pd.read_csv('/content/drive/MyDrive/기상청/코드/이지윤/데이터/보간_수정_train.csv',encoding = 'UTF8')\n",
    "test_fin = pd.read_csv('/content/drive/MyDrive/기상청/코드/이지윤/데이터/보간_수정_test.csv',encoding = 'UTF8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0caf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Data\n",
    "# Label Encoding\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_fin['year'])\n",
    "train_fin[\"year\"] = encoder.transform(train_fin['year']) + 1\n",
    "\n",
    "# month / day / hour\n",
    "train_fin['mmddhh'] = train_fin['mmddhh'].astype('str')\n",
    "train_fin['hour'] = train_fin['mmddhh'].str[-2:]\n",
    "train_fin['day'] = train_fin['mmddhh'].str[-4:-2]\n",
    "train_fin['month'] = train_fin['mmddhh'].str[:-4]\n",
    "\n",
    "train_fin['datetime'] = '000' + train_fin['year'].astype(str) + '-' + train_fin['month'] + '-' + train_fin['day'] + ' ' + train_fin['hour'] + ':00:00'\n",
    "train_fin.drop(['mmddhh','year'], axis = 1 , inplace = True)\n",
    "\n",
    "train_fin['month'] = train_fin['month'].astype('int')\n",
    "\n",
    "# Y 에 Na 존재하는거 제거\n",
    "train_fin['month'] = train_fin['month'].astype(int)\n",
    "train_fin.dropna(subset=['ts'], inplace = True)\n",
    "\n",
    "# Date를 Axis로 지정\n",
    "train_fin.drop(['hour','day','Unnamed: 0'], axis = 1 , inplace = True)\n",
    "train_fin = train_fin.reset_index(drop = True).set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a46bb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Data\n",
    "# Label Encoding\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(test_fin['year'])\n",
    "test_fin[\"year\"] = encoder.transform(test_fin['year']) + 1\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(test_fin['stn'])\n",
    "test_fin['stn'] = encoder.transform(test_fin['stn']) + 1\n",
    "\n",
    "# month / day / hour\n",
    "test_fin['mmddhh'] = test_fin['mmddhh'].astype('str')\n",
    "test_fin['hour'] = test_fin['mmddhh'].str[-2:]\n",
    "test_fin['day'] = test_fin['mmddhh'].str[-4:-2]\n",
    "test_fin['month'] = test_fin['mmddhh'].str[:-4]\n",
    "\n",
    "test_fin['datetime'] = '000' + test_fin['year'].astype(str) + '-' + test_fin['month'] + '-' + test_fin['day'] + ' ' + test_fin['hour'] + ':00:00'\n",
    "\n",
    "# Date를 Axis로 지정\n",
    "test_fin.drop(['Unnamed: 0','mmddhh','year','hour','day','month'], axis = 1 , inplace = True)\n",
    "test_fin = test_fin.reset_index(drop = True).set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38a1150",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hybrid = train_fin.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e1fcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LGBM_params_fold_start(LGBM_params, X, y):\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    pred_list = []\n",
    "    mae_list = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "\n",
    "        x_train, x_val, y_train, y_val = X.iloc[train_index], X.iloc[val_index], y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        model = lgb.LGBMRegressor(**LGBM_params)\n",
    "        model.fit(x_train , y_train ,\n",
    "                eval_set=[(x_val,y_val)],\n",
    "                eval_metric = 'mae' , verbose = False , early_stopping_rounds = 100)\n",
    "\n",
    "        pred = model.predict(x_val)\n",
    "        result = mean_absolute_error(pred,y_val)\n",
    "        mae_list.append(result)\n",
    "\n",
    "    return np.mean(mae_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8017d456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectiveLGB(trial: Trial, X, y):\n",
    "    param_lgb = {\n",
    "    'learning_rate': trial.suggest_float('learning_rate', 0.15, 0.2),\n",
    "    'n_estimators': trial.suggest_categorical('n_estimators', [700 , 800 , 900]),\n",
    "    'max_depth': trial.suggest_categorical('max_depth', [14, 15, 16, 17, 18]),\n",
    "    'min_child_weight': trial.suggest_categorical('min_child_weight', [1 , 2, 3, 4]),\n",
    "    'reg_lambda': trial.suggest_float('l2_regularization', 0, 1),\n",
    "    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1),\n",
    "    'num_leaves': trial.suggest_int('num_leaves', 20, 30,),\n",
    "    'min_split_gain': trial.suggest_float('min_split_gain', 0, 1),\n",
    "    'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "    'random_state': 112,\n",
    "    'objective': 'huber',\n",
    "    'metric': 'mae'\n",
    "  }\n",
    "    score = LGBM_params_fold_start(param_lgb, X, y)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81db89f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STN 1\n",
    "# Data\n",
    "train_hybrid_stn1 = train_hybrid[train_hybrid.stn == 1].reset_index(drop = True)\n",
    "train_hybrid_stn1 = pd.get_dummies(data = train_hybrid_stn1, drop_first = True)\n",
    "\n",
    "# train-test split\n",
    "target = ['ts'] ; f_col = ['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn', 'ww_F', 'ww_G', 'ww_H', 'ww_R', 'ww_S', 'ww_X']\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_hybrid_stn1[f_col] , train_hybrid_stn1[target] , test_size = 0.2 , shuffle = False)\n",
    "\n",
    "# Time series\n",
    "dp = DeterministicProcess(index = train_x.index , constant = True , order = 1 , drop = True)\n",
    "\n",
    "X = dp.in_sample()\n",
    "lm1 = LinearRegression(fit_intercept=False)\n",
    "lm1.fit(X, train_y)\n",
    "\n",
    "test_const = [1 for i in range(test_x.shape[0])]\n",
    "test_trend = np.arange(X['trend'].max(), X['trend'].max()+test_x.shape[0])\n",
    "X_ = pd.DataFrame([test_const, test_trend]).T\n",
    "X_.columns = ['const', 'trend']\n",
    "X_.index = test_x.index\n",
    "\n",
    "trend_train = lm1.predict(X)\n",
    "trend_test = lm1.predict(X_)\n",
    "\n",
    "train_y_delta = train_y - trend_train\n",
    "test_y_delta = test_y - trend_test\n",
    "\n",
    "# Optuna tunning for tree model (LGBM)\n",
    "study = optuna.create_study(direction='minimize',sampler=TPESampler(seed=42))\n",
    "\n",
    "timeout = 1800 ; start_time = time.time()\n",
    "\n",
    "while (time.time() - start_time) < timeout:\n",
    "    study.optimize(lambda trial : objectiveLGB(trial, train_x, train_y_delta), n_trials= 1)\n",
    "    print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))\n",
    "\n",
    "    if (time.time() - start_time) >= timeout:\n",
    "        break\n",
    "\n",
    "lgb_param1 = study.best_trial.params\n",
    "lgb_best1 = lgb.LGBMRegressor(**lgb_param1)\n",
    "lgb_best1.fit(train_x, train_y_delta)\n",
    "\n",
    "lgb_train = lgb_best1.predict(train_x)\n",
    "lgb_test = lgb_best1.predict(test_x)\n",
    "\n",
    "# Result\n",
    "original_train = [i+j for i,j in zip(sum(trend_train.tolist(), []), lgb_train.tolist())]\n",
    "original_test = [i+j for i,j in zip(sum(trend_test.tolist(), []), lgb_test.tolist())]\n",
    "\n",
    "mean_absolute_error(original_test,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b5d9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STN 2\n",
    "# Data\n",
    "train_hybrid_stn2 = train_hybrid[train_hybrid.stn == 2].reset_index(drop = True)\n",
    "train_hybrid_stn2 = pd.get_dummies(data = train_hybrid_stn2, drop_first = True)\n",
    "\n",
    "# train-test split\n",
    "target = ['ts'] ; f_col = ['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn', 'ww_F', 'ww_G', 'ww_H', 'ww_R', 'ww_S', 'ww_X']\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_hybrid_stn2[f_col] , train_hybrid_stn2[target] , test_size = 0.2 , shuffle = False)\n",
    "\n",
    "# Time series\n",
    "dp = DeterministicProcess(index = train_x.index , constant = True , order = 1 , drop = True)\n",
    "\n",
    "X = dp.in_sample()\n",
    "lm2 = LinearRegression(fit_intercept=False)\n",
    "lm2.fit(X, train_y)\n",
    "\n",
    "test_const = [1 for i in range(test_x.shape[0])]\n",
    "test_trend = np.arange(X['trend'].max(), X['trend'].max()+test_x.shape[0])\n",
    "X_ = pd.DataFrame([test_const, test_trend]).T\n",
    "X_.columns = ['const', 'trend']\n",
    "X_.index = test_x.index\n",
    "\n",
    "trend_train2 = lm2.predict(X)\n",
    "trend_test2 = lm2.predict(X_)\n",
    "\n",
    "train_y_delta = train_y - trend_train2\n",
    "test_y_delta = test_y - trend_test2\n",
    "\n",
    "# Optuna tunning for tree model (LGBM)\n",
    "study = optuna.create_study(direction='minimize',sampler = TPESampler(seed=42))\n",
    "timeout = 1800 ; start_time = time.time()\n",
    "\n",
    "while (time.time() - start_time) < timeout:\n",
    "    study.optimize(lambda trial : objectiveLGB(trial, train_x, train_y_delta), n_trials=1)\n",
    "    print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))\n",
    "\n",
    "    if (time.time() - start_time) >= timeout:\n",
    "        break\n",
    "\n",
    "lgb_param2 = study.best_trial.params\n",
    "lgb_best2 = lgb.LGBMRegressor(**lgb_param2)\n",
    "lgb_best2.fit(train_x, train_y_delta)\n",
    "\n",
    "lgb_train2 = lgb_best2.predict(train_x)\n",
    "lgb_test2 = lgb_best2.predict(test_x)\n",
    "\n",
    "# Result\n",
    "original_train2 = [i+j for i,j in zip(sum(trend_train2.tolist(), []), lgb_train2.tolist())]\n",
    "original_test2 = [i+j for i,j in zip(sum(trend_test2.tolist(), []), lgb_test2.tolist())]\n",
    "\n",
    "mean_absolute_error(original_test2 , test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7b5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STN 3\n",
    "# Data\n",
    "train_hybrid_stn3 = train_hybrid[train_hybrid.stn == 3].reset_index(drop = True)\n",
    "train_hybrid_stn3 = pd.get_dummies(data = train_hybrid_stn3, drop_first = True)\n",
    "\n",
    "# train-test split\n",
    "target = ['ts'] ; f_col = ['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn', 'ww_F', 'ww_G', 'ww_H', 'ww_R', 'ww_S', 'ww_X']\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_hybrid_stn3[f_col] , train_hybrid_stn3[target] , test_size = 0.2 , shuffle = False)\n",
    "\n",
    "# Time series\n",
    "dp = DeterministicProcess(index = train_x.index , constant = True , order = 1 , drop = True)\n",
    "\n",
    "X = dp.in_sample()\n",
    "lm3 = LinearRegression(fit_intercept=False)\n",
    "lm3.fit(X, train_y)\n",
    "\n",
    "test_const = [1 for i in range(test_x.shape[0])]\n",
    "test_trend = np.arange(X['trend'].max(), X['trend'].max()+test_x.shape[0])\n",
    "X_ = pd.DataFrame([test_const, test_trend]).T\n",
    "X_.columns = ['const', 'trend']\n",
    "X_.index = test_x.index\n",
    "\n",
    "trend_train3 = lm3.predict(X)\n",
    "trend_test3 = lm3.predict(X_)\n",
    "\n",
    "train_y_delta = train_y - trend_train3\n",
    "test_y_delta = test_y - trend_test3\n",
    "\n",
    "# Optuna tunning for tree model (LGBM)\n",
    "study = optuna.create_study(direction='minimize',sampler = TPESampler(seed=42))\n",
    "timeout = 1800 ; start_time = time.time()\n",
    "\n",
    "while (time.time() - start_time) < timeout:\n",
    "    study.optimize(lambda trial : objectiveLGB(trial, train_x, train_y_delta), n_trials=1)\n",
    "    print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))\n",
    "\n",
    "    if (time.time() - start_time) >= timeout:\n",
    "        break\n",
    "\n",
    "lgb_param3 = study.best_trial.params\n",
    "lgb_best3 = lgb.LGBMRegressor(**lgb_param3)\n",
    "lgb_best3.fit(train_x, train_y_delta)\n",
    "\n",
    "lgb_train3 = lgb_best3.predict(train_x)\n",
    "lgb_test3 = lgb_best3.predict(test_x)\n",
    "\n",
    "# Result\n",
    "original_train3 = [i+j for i,j in zip(sum(trend_train3.tolist(), []), lgb_train3.tolist())]\n",
    "original_test3 = [i+j for i,j in zip(sum(trend_test3.tolist(), []), lgb_test3.tolist())]\n",
    "\n",
    "mean_absolute_error(original_test3 , test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e6a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STN 4\n",
    "# Data\n",
    "train_hybrid_stn4 = train_hybrid[train_hybrid.stn == 4].reset_index(drop = True)\n",
    "train_hybrid_stn4 = pd.get_dummies(data = train_hybrid_stn4, drop_first = True)\n",
    "\n",
    "# train-test split\n",
    "target = ['ts'] ; f_col = ['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn', 'ww_F', 'ww_G', 'ww_H', 'ww_R', 'ww_S', 'ww_X']\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_hybrid_stn4[f_col] , train_hybrid_stn4[target] , test_size = 0.2 , shuffle = False)\n",
    "\n",
    "# Time series\n",
    "dp = DeterministicProcess(index = train_x.index , constant = True , order = 1 , drop = True)\n",
    "\n",
    "X = dp.in_sample()\n",
    "lm4 = LinearRegression(fit_intercept=False)\n",
    "lm4.fit(X, train_y)\n",
    "\n",
    "test_const = [1 for i in range(test_x.shape[0])]\n",
    "test_trend = np.arange(X['trend'].max(), X['trend'].max()+test_x.shape[0])\n",
    "X_ = pd.DataFrame([test_const, test_trend]).T\n",
    "X_.columns = ['const', 'trend']\n",
    "X_.index = test_x.index\n",
    "\n",
    "trend_train4 = lm4.predict(X)\n",
    "trend_test4 = lm4.predict(X_)\n",
    "\n",
    "train_y_delta = train_y - trend_train4\n",
    "test_y_delta = test_y - trend_test4\n",
    "\n",
    "# Optuna tunning for tree model (LGBM)\n",
    "study = optuna.create_study(direction='minimize',sampler = TPESampler(seed=42))\n",
    "timeout = 1800 ; start_time = time.time()\n",
    "\n",
    "while (time.time() - start_time) < timeout:\n",
    "    study.optimize(lambda trial : objectiveLGB(trial, train_x, train_y_delta), n_trials=1)\n",
    "    print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))\n",
    "\n",
    "    if (time.time() - start_time) >= timeout:\n",
    "        break\n",
    "\n",
    "lgb_param4 = study.best_trial.params\n",
    "lgb_best4 = lgb.LGBMRegressor(**lgb_param4)\n",
    "lgb_best4.fit(train_x, train_y_delta)\n",
    "\n",
    "lgb_train4 = lgb_best4.predict(train_x)\n",
    "lgb_test4 = lgb_best4.predict(test_x)\n",
    "\n",
    "# Result\n",
    "original_train4 = [i+j for i,j in zip(sum(trend_train4.tolist(), []), lgb_train4.tolist())]\n",
    "original_test4 = [i+j for i,j in zip(sum(trend_test4.tolist(), []), lgb_test4.tolist())]\n",
    "\n",
    "mean_absolute_error(original_test4 , test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a86dfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STN 5\n",
    "# Data\n",
    "train_hybrid_stn5 = train_hybrid[train_hybrid.stn == 5].reset_index(drop = True)\n",
    "train_hybrid_stn5 = pd.get_dummies(data = train_hybrid_stn5, drop_first = True)\n",
    "\n",
    "# train-test split\n",
    "target = ['ts'] ; f_col = ['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn', 'ww_F', 'ww_G', 'ww_H', 'ww_R', 'ww_S', 'ww_X']\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_hybrid_stn5[f_col] , train_hybrid_stn5[target] , test_size = 0.2 , shuffle = False)\n",
    "\n",
    "# Time series\n",
    "dp = DeterministicProcess(index = train_x.index , constant = True , order = 1 , drop = True)\n",
    "\n",
    "X = dp.in_sample()\n",
    "lm5 = LinearRegression(fit_intercept=False)\n",
    "lm5.fit(X, train_y)\n",
    "\n",
    "test_const = [1 for i in range(test_x.shape[0])]\n",
    "test_trend = np.arange(X['trend'].max(), X['trend'].max()+test_x.shape[0])\n",
    "X_ = pd.DataFrame([test_const, test_trend]).T\n",
    "X_.columns = ['const', 'trend']\n",
    "X_.index = test_x.index\n",
    "\n",
    "trend_train5 = lm5.predict(X)\n",
    "trend_test5 = lm5.predict(X_)\n",
    "\n",
    "train_y_delta = train_y - trend_train5\n",
    "test_y_delta = test_y - trend_test5\n",
    "\n",
    "# Optuna tunning for tree model (LGBM)\n",
    "study = optuna.create_study(direction='minimize',sampler = TPESampler(seed=42))\n",
    "timeout = 1800 ; start_time = time.time()\n",
    "\n",
    "while (time.time() - start_time) < timeout:\n",
    "    study.optimize(lambda trial : objectiveLGB(trial, train_x, train_y_delta), n_trials=1)\n",
    "    print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))\n",
    "\n",
    "    if (time.time() - start_time) >= timeout:\n",
    "        break\n",
    "\n",
    "lgb_param5 = study.best_trial.params\n",
    "lgb_best5 = lgb.LGBMRegressor(**lgb_param5)\n",
    "lgb_best5.fit(train_x, train_y_delta)\n",
    "\n",
    "lgb_train5 = lgb_best5.predict(train_x)\n",
    "lgb_test5 = lgb_best5.predict(test_x)\n",
    "\n",
    "# Result\n",
    "original_train5 = [i+j for i,j in zip(sum(trend_train5.tolist(), []), lgb_train5.tolist())]\n",
    "original_test5 = [i+j for i,j in zip(sum(trend_test5.tolist(), []), lgb_test5.tolist())]\n",
    "\n",
    "mean_absolute_error(original_test5 , test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fa0e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STN 6\n",
    "# Data\n",
    "train_hybrid_stn6 = train_hybrid[train_hybrid.stn == 6].reset_index(drop = True)\n",
    "train_hybrid_stn6 = pd.get_dummies(data = train_hybrid_stn6, drop_first = True)\n",
    "\n",
    "# train-test split\n",
    "target = ['ts'] ; f_col = ['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn', 'ww_F', 'ww_G', 'ww_H', 'ww_R', 'ww_S', 'ww_X']\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_hybrid_stn6[f_col] , train_hybrid_stn6[target] , test_size = 0.2 , shuffle = False)\n",
    "\n",
    "# Time series\n",
    "dp = DeterministicProcess(index = train_x.index , constant = True , order = 1 , drop = True)\n",
    "\n",
    "X = dp.in_sample()\n",
    "lm6 = LinearRegression(fit_intercept=False)\n",
    "lm6.fit(X, train_y)\n",
    "\n",
    "test_const = [1 for i in range(test_x.shape[0])]\n",
    "test_trend = np.arange(X['trend'].max(), X['trend'].max()+test_x.shape[0])\n",
    "X_ = pd.DataFrame([test_const, test_trend]).T\n",
    "X_.columns = ['const', 'trend']\n",
    "X_.index = test_x.index\n",
    "\n",
    "trend_train6 = lm6.predict(X)\n",
    "trend_test6 = lm6.predict(X_)\n",
    "\n",
    "train_y_delta = train_y - trend_train6\n",
    "test_y_delta = test_y - trend_test6\n",
    "\n",
    "# Optuna tunning for tree model (LGBM)\n",
    "study = optuna.create_study(direction='minimize',sampler = TPESampler(seed=42))\n",
    "timeout = 1800 ; start_time = time.time()\n",
    "\n",
    "while (time.time() - start_time) < timeout:\n",
    "    study.optimize(lambda trial : objectiveLGB(trial, train_x, train_y_delta), n_trials=1)\n",
    "    print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))\n",
    "\n",
    "    if (time.time() - start_time) >= timeout:\n",
    "        break\n",
    "\n",
    "lgb_param6 = study.best_trial.params\n",
    "lgb_best6 = lgb.LGBMRegressor(**lgb_param6)\n",
    "lgb_best6.fit(train_x, train_y_delta)\n",
    "\n",
    "lgb_train6 = lgb_best6.predict(train_x)\n",
    "lgb_test6 = lgb_best6.predict(test_x)\n",
    "\n",
    "# Result\n",
    "original_train6 = [i+j for i,j in zip(sum(trend_train6.tolist(), []), lgb_train6.tolist())]\n",
    "original_test6 = [i+j for i,j in zip(sum(trend_test6.tolist(), []), lgb_test6.tolist())]\n",
    "\n",
    "mean_absolute_error(original_test6 , test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd8c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STN 7\n",
    "# Data\n",
    "train_hybrid_stn7 = train_hybrid[train_hybrid.stn == 7].reset_index(drop = True)\n",
    "train_hybrid_stn7 = pd.get_dummies(data = train_hybrid_stn7, drop_first = True)\n",
    "\n",
    "# train-test split\n",
    "target = ['ts'] ; f_col = ['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn', 'ww_F', 'ww_G', 'ww_H', 'ww_R', 'ww_S', 'ww_X']\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_hybrid_stn7[f_col] , train_hybrid_stn7[target] , test_size = 0.2 , shuffle = False)\n",
    "\n",
    "# Time series\n",
    "dp = DeterministicProcess(index = train_x.index , constant = True , order = 1 , drop = True)\n",
    "\n",
    "X = dp.in_sample()\n",
    "lm7 = LinearRegression(fit_intercept=False)\n",
    "lm7.fit(X, train_y)\n",
    "\n",
    "test_const = [1 for i in range(test_x.shape[0])]\n",
    "test_trend = np.arange(X['trend'].max(), X['trend'].max()+test_x.shape[0])\n",
    "X_ = pd.DataFrame([test_const, test_trend]).T\n",
    "X_.columns = ['const', 'trend']\n",
    "X_.index = test_x.index\n",
    "\n",
    "trend_train7 = lm7.predict(X)\n",
    "trend_test7 = lm7.predict(X_)\n",
    "\n",
    "train_y_delta = train_y - trend_train7\n",
    "test_y_delta = test_y - trend_test7\n",
    "\n",
    "# Optuna tunning for tree model (LGBM)\n",
    "study = optuna.create_study(direction='minimize',sampler = TPESampler(seed=42))\n",
    "timeout = 1800 ; start_time = time.time()\n",
    "\n",
    "while (time.time() - start_time) < timeout:\n",
    "    study.optimize(lambda trial : objectiveLGB(trial, train_x, train_y_delta), n_trials=1)\n",
    "    print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))\n",
    "\n",
    "    if (time.time() - start_time) >= timeout:\n",
    "        break\n",
    "\n",
    "lgb_param7 = study.best_trial.params\n",
    "lgb_best7 = lgb.LGBMRegressor(**lgb_param7)\n",
    "lgb_best7.fit(train_x, train_y_delta)\n",
    "\n",
    "lgb_train7 = lgb_best7.predict(train_x)\n",
    "lgb_test7 = lgb_best7.predict(test_x)\n",
    "\n",
    "# Result\n",
    "original_train7 = [i+j for i,j in zip(sum(trend_train7.tolist(), []), lgb_train7.tolist())]\n",
    "original_test7 = [i+j for i,j in zip(sum(trend_test7.tolist(), []), lgb_test7.tolist())]\n",
    "\n",
    "mean_absolute_error(original_test7 , test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7bf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STN 8\n",
    "# Data\n",
    "train_hybrid_stn8 = train_hybrid[train_hybrid.stn == 8].reset_index(drop = True)\n",
    "train_hybrid_stn8 = pd.get_dummies(data = train_hybrid_stn8, drop_first = True)\n",
    "\n",
    "# train-test split\n",
    "target = ['ts'] ; f_col = ['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn', 'ww_F', 'ww_G', 'ww_H', 'ww_R', 'ww_S', 'ww_X']\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_hybrid_stn8[f_col] , train_hybrid_stn8[target] , test_size = 0.2 , shuffle = False)\n",
    "\n",
    "# Time series\n",
    "dp = DeterministicProcess(index = train_x.index , constant = True , order = 1 , drop = True)\n",
    "\n",
    "X = dp.in_sample()\n",
    "lm8 = LinearRegression(fit_intercept=False)\n",
    "lm8.fit(X, train_y)\n",
    "\n",
    "test_const = [1 for i in range(test_x.shape[0])]\n",
    "test_trend = np.arange(X['trend'].max(), X['trend'].max()+test_x.shape[0])\n",
    "X_ = pd.DataFrame([test_const, test_trend]).T\n",
    "X_.columns = ['const', 'trend']\n",
    "X_.index = test_x.index\n",
    "\n",
    "trend_train8 = lm8.predict(X)\n",
    "trend_test8 = lm8.predict(X_)\n",
    "\n",
    "train_y_delta = train_y - trend_train8\n",
    "test_y_delta = test_y - trend_test8\n",
    "\n",
    "# Optuna tunning for tree model (LGBM)\n",
    "study = optuna.create_study(direction='minimize',sampler = TPESampler(seed=42))\n",
    "timeout = 1800 ; start_time = time.time()\n",
    "\n",
    "while (time.time() - start_time) < timeout:\n",
    "    study.optimize(lambda trial : objectiveLGB(trial, train_x, train_y_delta), n_trials=1)\n",
    "    print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))\n",
    "\n",
    "    if (time.time() - start_time) >= timeout:\n",
    "        break\n",
    "\n",
    "lgb_param8 = study.best_trial.params\n",
    "lgb_best8 = lgb.LGBMRegressor(**lgb_param8)\n",
    "lgb_best8.fit(train_x, train_y_delta)\n",
    "\n",
    "lgb_train8 = lgb_best8.predict(train_x)\n",
    "lgb_test8 = lgb_best8.predict(test_x)\n",
    "\n",
    "# Result\n",
    "original_train8 = [i+j for i,j in zip(sum(trend_train8.tolist(), []), lgb_train8.tolist())]\n",
    "original_test8 = [i+j for i,j in zip(sum(trend_test8.tolist(), []), lgb_test8.tolist())]\n",
    "\n",
    "mean_absolute_error(original_test8 , test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fe5a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STN 9\n",
    "# Data\n",
    "train_hybrid_stn9 = train_hybrid[train_hybrid.stn == 9].reset_index(drop = True)\n",
    "train_hybrid_stn9 = pd.get_dummies(data = train_hybrid_stn9, drop_first = True)\n",
    "\n",
    "# train-test split\n",
    "target = ['ts'] ; f_col = ['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn', 'ww_F', 'ww_G', 'ww_H', 'ww_R', 'ww_S', 'ww_X']\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_hybrid_stn9[f_col] , train_hybrid_stn9[target] , test_size = 0.2 , shuffle = False)\n",
    "\n",
    "# Time series\n",
    "dp = DeterministicProcess(index = train_x.index , constant = True , order = 1 , drop = True)\n",
    "\n",
    "X = dp.in_sample()\n",
    "lm9 = LinearRegression(fit_intercept=False)\n",
    "lm9.fit(X, train_y)\n",
    "\n",
    "test_const = [1 for i in range(test_x.shape[0])]\n",
    "test_trend = np.arange(X['trend'].max(), X['trend'].max() + test_x.shape[0])\n",
    "X_ = pd.DataFrame([test_const, test_trend]).T\n",
    "X_.columns = ['const', 'trend']\n",
    "X_.index = test_x.index\n",
    "\n",
    "trend_train9 = lm9.predict(X)\n",
    "trend_test9 = lm9.predict(X_)\n",
    "\n",
    "train_y_delta = train_y - trend_train9\n",
    "test_y_delta = test_y - trend_test9\n",
    "\n",
    "# Optuna tunning for tree model (LGBM)\n",
    "study = optuna.create_study(direction='minimize',sampler = TPESampler(seed=42))\n",
    "timeout = 1800 ; start_time = time.time()\n",
    "\n",
    "while (time.time() - start_time) < timeout:\n",
    "    study.optimize(lambda trial : objectiveLGB(trial, train_x, train_y_delta), n_trials=1)\n",
    "    print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))\n",
    "\n",
    "    if (time.time() - start_time) >= timeout:\n",
    "        break\n",
    "\n",
    "lgb_param9 = study.best_trial.params\n",
    "lgb_best9 = lgb.LGBMRegressor(**lgb_param9)\n",
    "lgb_best9.fit(train_x, train_y_delta)\n",
    "\n",
    "lgb_train9 = lgb_best9.predict(train_x)\n",
    "lgb_test9 = lgb_best9.predict(test_x)\n",
    "\n",
    "# Result\n",
    "original_train9 = [i+j for i,j in zip(sum(trend_train9.tolist(), []), lgb_train9.tolist())]\n",
    "original_test9 = [i+j for i,j in zip(sum(trend_test9.tolist(), []), lgb_test9.tolist())]\n",
    "\n",
    "mean_absolute_error(original_test9 , test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b03aae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STN 10\n",
    "train_hybrid_stn10 = train_hybrid[train_hybrid.stn == 10].reset_index(drop = True)\n",
    "train_hybrid_stn10 = pd.get_dummies(data = train_hybrid_stn10, drop_first = True)\n",
    "\n",
    "target = ['ts'] ; f_col = ['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn', 'ww_F', 'ww_G', 'ww_H', 'ww_R', 'ww_S', 'ww_X']\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_hybrid_stn10[f_col] , train_hybrid_stn10[target] , test_size = 0.2 , shuffle = False)\n",
    "\n",
    "dp = DeterministicProcess(index = train_x.index , constant = True , order = 1 , drop = True)\n",
    "\n",
    "X = dp.in_sample()\n",
    "lm10 = LinearRegression(fit_intercept=False)\n",
    "lm10.fit(X, train_y)\n",
    "\n",
    "test_const = [1 for i in range(test_x.shape[0])]\n",
    "test_trend = np.arange(X['trend'].max(), X['trend'].max()+test_x.shape[0])\n",
    "X_ = pd.DataFrame([test_const, test_trend]).T\n",
    "X_.columns = ['const', 'trend']\n",
    "X_.index = test_x.index\n",
    "\n",
    "trend_train10 = lm10.predict(X)\n",
    "trend_test10 = lm10.predict(X_)\n",
    "\n",
    "train_y_delta = train_y - trend_train10\n",
    "test_y_delta = test_y - trend_test10\n",
    "\n",
    "study = optuna.create_study(direction='minimize',sampler = TPESampler(seed=42))\n",
    "timeout = 1800 ; start_time = time.time()\n",
    "\n",
    "while (time.time() - start_time) < timeout:\n",
    "    study.optimize(lambda trial : objectiveLGB(trial, train_x, train_y_delta), n_trials=1)\n",
    "    print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))\n",
    "\n",
    "    if (time.time() - start_time) >= timeout:\n",
    "        break\n",
    "\n",
    "lgb_param10 = study.best_trial.params\n",
    "lgb_best10 = lgb.LGBMRegressor(**lgb_param10)\n",
    "lgb_best10.fit(train_x, train_y_delta)\n",
    "\n",
    "lgb_train10 = lgb_best10.predict(train_x)\n",
    "lgb_test10 = lgb_best10.predict(test_x)\n",
    "\n",
    "original_train10 = [i+j for i,j in zip(sum(trend_train.tolist(), []), lgb_train10.tolist())]\n",
    "original_test10 = [i+j for i,j in zip(sum(trend_test.tolist(), []), lgb_test10.tolist())]\n",
    "\n",
    "mean_absolute_error(original_test10 , test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1eb756",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_tp_test_hybrid = test_fin.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f680c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기상청 설명자료 속 선행지식 활용 (지점 abc)\n",
    "test_hybrid_stn1 = surface_tp_test_hybrid[surface_tp_test_hybrid.stn == 1].reset_index(drop=True)\n",
    "test_hybrid_stn2 = surface_tp_test_hybrid[surface_tp_test_hybrid.stn == 2].reset_index(drop=True)\n",
    "test_hybrid_stn3 = surface_tp_test_hybrid[surface_tp_test_hybrid.stn == 3].reset_index(drop=True)\n",
    "\n",
    "test_hybrid_stn1 = pd.get_dummies(data=test_hybrid_stn1, drop_first=True)\n",
    "test_hybrid_stn2 = pd.get_dummies(data=test_hybrid_stn2, drop_first=True)\n",
    "test_hybrid_stn3 = pd.get_dummies(data=test_hybrid_stn3, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f7552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy encoding을 했을 때 train에는 존재하지만, test에는 없는 열은 0으로 설정\n",
    "test_hybrid_stn1['ww_X'] = 0\n",
    "test_hybrid_stn2['ww_X'] = 0\n",
    "test_hybrid_stn3['ww_X'] = 0\n",
    "test_hybrid_stn3['ww_H'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be79441",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Time Series\n",
    "target = ['ts'] ; f_col = ['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'sn', 'ww_F', 'ww_G', 'ww_H', 'ww_R', 'ww_S', 'ww_X']\n",
    "\n",
    "## STN 1\n",
    "train_hybrid_stn1 = train_hybrid[train_hybrid.stn == 1].reset_index(drop = True)\n",
    "train_hybrid_stn1 = pd.get_dummies(data = train_hybrid_stn1, drop_first = True)\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_hybrid_stn1[f_col] , train_hybrid_stn1[target] , test_size = 0.2 , shuffle = False)\n",
    "dp = DeterministicProcess(index = train_x.index , constant = True , order = 1 , drop = True)\n",
    "X1 = dp.in_sample()\n",
    "lm1 = LinearRegression(fit_intercept=False)\n",
    "\n",
    "## STN 2\n",
    "train_hybrid_stn2 = train_hybrid[train_hybrid.stn == 2].reset_index(drop = True)\n",
    "train_hybrid_stn2 = pd.get_dummies(data = train_hybrid_stn2, drop_first = True)\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_hybrid_stn2[f_col] , train_hybrid_stn2[target] , test_size = 0.2 , shuffle = False)\n",
    "dp = DeterministicProcess(index = train_x.index , constant = True , order = 1 , drop = True)\n",
    "X2 = dp.in_sample()\n",
    "lm2 = LinearRegression(fit_intercept=False)\n",
    "\n",
    "## STN 3\n",
    "train_hybrid_stn3 = train_hybrid[train_hybrid.stn == 3].reset_index(drop = True)\n",
    "train_hybrid_stn3 = pd.get_dummies(data = train_hybrid_stn3, drop_first = True)\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_hybrid_stn3[f_col] , train_hybrid_stn3[target] , test_size = 0.2 , shuffle = False)\n",
    "dp = DeterministicProcess(index = train_x.index , constant = True , order = 1 , drop = True)\n",
    "X3 = dp.in_sample()\n",
    "lm3 = LinearRegression(fit_intercept=False)\n",
    "\n",
    "## STN 4\n",
    "train_hybrid_stn4 = train_hybrid[train_hybrid.stn == 4].reset_index(drop = True)\n",
    "train_hybrid_stn4 = pd.get_dummies(data = train_hybrid_stn4, drop_first = True)\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_hybrid_stn4[f_col] , train_hybrid_stn4[target] , test_size = 0.2 , shuffle = False)\n",
    "dp = DeterministicProcess(index = train_x.index , constant = True , order = 1 , drop = True)\n",
    "X4 = dp.in_sample()\n",
    "lm4 = LinearRegression(fit_intercept=False)\n",
    "\n",
    "## STN 5\n",
    "train_hybrid_stn5 = train_hybrid[train_hybrid.stn == 5].reset_index(drop = True)\n",
    "train_hybrid_stn5 = pd.get_dummies(data = train_hybrid_stn2, drop_first = True)\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_hybrid_stn5[f_col] , train_hybrid_stn5[target] , test_size = 0.2 , shuffle = False)\n",
    "dp = DeterministicProcess(index = train_x.index , constant = True , order = 1 , drop = True)\n",
    "X5 = dp.in_sample()\n",
    "lm5 = LinearRegression(fit_intercept=False)\n",
    "\n",
    "## STN 6\n",
    "train_hybrid_stn6 = train_hybrid[train_hybrid.stn == 6].reset_index(drop = True)\n",
    "train_hybrid_stn6 = pd.get_dummies(data = train_hybrid_stn6, drop_first = True)\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_hybrid_stn6[f_col] , train_hybrid_stn6[target] , test_size = 0.2 , shuffle = False)\n",
    "dp = DeterministicProcess(index = train_x.index , constant = True , order = 1 , drop = True)\n",
    "X6 = dp.in_sample()\n",
    "lm6 = LinearRegression(fit_intercept=False)\n",
    "\n",
    "## STN 7\n",
    "train_hybrid_stn7 = train_hybrid[train_hybrid.stn == 7].reset_index(drop = True)\n",
    "train_hybrid_stn7 = pd.get_dummies(data = train_hybrid_stn7, drop_first = True)\n",
    "train_hybrid_stn7['ww_X'] = 0\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_hybrid_stn7[f_col] , train_hybrid_stn7[target] , test_size = 0.2 , shuffle = False)\n",
    "dp = DeterministicProcess(index = train_x.index , constant = True , order = 1 , drop = True)\n",
    "X7 = dp.in_sample()\n",
    "lm7 = LinearRegression(fit_intercept=False)\n",
    "\n",
    "## STN 8\n",
    "train_hybrid_stn8 = train_hybrid[train_hybrid.stn == 8].reset_index(drop = True)\n",
    "train_hybrid_stn8 = pd.get_dummies(data = train_hybrid_stn8, drop_first = True)\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_hybrid_stn8[f_col] , train_hybrid_stn8[target] , test_size = 0.2 , shuffle = False)\n",
    "dp = DeterministicProcess(index = train_x.index , constant = True , order = 1 , drop = True)\n",
    "X8 = dp.in_sample()\n",
    "lm8 = LinearRegression(fit_intercept=False)\n",
    "\n",
    "## STN 9\n",
    "train_hybrid_stn9 = train_hybrid[train_hybrid.stn == 9].reset_index(drop = True)\n",
    "train_hybrid_stn9 = pd.get_dummies(data = train_hybrid_stn9, drop_first = True)\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_hybrid_stn9[f_col] , train_hybrid_stn9[target] , test_size = 0.2 , shuffle = False)\n",
    "dp = DeterministicProcess(index = train_x.index , constant = True , order = 1 , drop = True)\n",
    "X9 = dp.in_sample()\n",
    "lm9 = LinearRegression(fit_intercept=False)\n",
    "\n",
    "## STN 10\n",
    "train_hybrid_stn10 = train_hybrid[train_hybrid.stn == 10].reset_index(drop = True)\n",
    "train_hybrid_stn10 = pd.get_dummies(data = train_hybrid_stn2, drop_first = True)\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_hybrid_stn10[f_col] , train_hybrid_stn10[target] , test_size = 0.2 , shuffle = False)\n",
    "dp = DeterministicProcess(index = train_x.index , constant = True , order = 1 , drop = True)\n",
    "X10 = dp.in_sample()\n",
    "lm10 = LinearRegression(fit_intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28b7b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp\n",
    "dp1 = DeterministicProcess(index = train_hybrid_stn1.index, constant = True, order = 1,drop = True)\n",
    "dp2 = DeterministicProcess(index = train_hybrid_stn2.index, constant = True, order = 1,drop = True)\n",
    "dp3 = DeterministicProcess(index = train_hybrid_stn3.index, constant = True, order = 1,drop = True)\n",
    "dp4 = DeterministicProcess(index = train_hybrid_stn4.index, constant = True, order = 1,drop = True)\n",
    "dp5 = DeterministicProcess(index = train_hybrid_stn5.index, constant = True, order = 1,drop = True)\n",
    "dp6 = DeterministicProcess(index = train_hybrid_stn6.index, constant = True, order = 1,drop = True)\n",
    "dp7 = DeterministicProcess(index = train_hybrid_stn7.index, constant = True, order = 1,drop = True)\n",
    "dp8 = DeterministicProcess(index = train_hybrid_stn8.index, constant = True, order = 1,drop = True)\n",
    "dp9 = DeterministicProcess(index = train_hybrid_stn9.index, constant = True, order = 1,drop = True)\n",
    "dp10 = DeterministicProcess(index = train_hybrid_stn10.index, constant = True, order = 1,drop = True)\n",
    "\n",
    "# X\n",
    "X1 = dp1.in_sample() ; X2 = dp2.in_sample()\n",
    "X3 = dp3.in_sample() ; X4 = dp4.in_sample()\n",
    "X5 = dp5.in_sample() ; X6 = dp6.in_sample()\n",
    "X7 = dp7.in_sample() ; X8 = dp8.in_sample()\n",
    "X9 = dp9.in_sample() ; X10 = dp10.in_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65af2435",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm1.fit(X1, train_hybrid_stn1[target])\n",
    "lm2.fit(X2, train_hybrid_stn2[target])\n",
    "lm3.fit(X3, train_hybrid_stn3[target])\n",
    "lm4.fit(X4, train_hybrid_stn4[target])\n",
    "lm5.fit(X5, train_hybrid_stn5[target])\n",
    "lm6.fit(X6, train_hybrid_stn6[target])\n",
    "lm7.fit(X7, train_hybrid_stn7[target])\n",
    "lm8.fit(X8, train_hybrid_stn8[target])\n",
    "lm9.fit(X9, train_hybrid_stn9[target])\n",
    "lm10.fit(X10, train_hybrid_stn10[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e100615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지점별 train data 추세 추출\n",
    "trend_stn1 = lm1.predict(X1)\n",
    "trend_stn2 = lm2.predict(X2)\n",
    "trend_stn3 = lm3.predict(X3)\n",
    "trend_stn4 = lm4.predict(X4)\n",
    "trend_stn5 = lm5.predict(X5)\n",
    "trend_stn6 = lm6.predict(X6)\n",
    "trend_stn7 = lm7.predict(X7)\n",
    "trend_stn8 = lm8.predict(X8)\n",
    "trend_stn9 = lm9.predict(X9)\n",
    "trend_stn10 = lm10.predict(X10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420f1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Parameter\n",
    "lgb_param1 = {'learning_rate': 0.12829594410470951, 'n_estimators': 500, 'max_depth': 16, 'min_child_weight': 2, 'l2_regularization': 0.5336210596747876}\n",
    "lgb_param2 = {'learning_rate': 0.16422823124652633, 'n_estimators': 800, 'max_depth': 16, 'min_child_weight': 2, 'l2_regularization': 0.48628837398667374,\n",
    "              'colsample_bytree': 0.5877431614700145, 'num_leaves': 20, 'min_split_gain': 0.6322304426546818, 'bagging_freq': 10}\n",
    "lgb_param3 = {'learning_rate': 0.18889534485982132, 'n_estimators': 500, 'max_depth': 17, 'min_child_weight': 4, 'l2_regularization': 0.32482046710162793}\n",
    "lgb_param4 = {'learning_rate': 0.13046182446397234, 'n_estimators': 400, 'max_depth': 16, 'min_child_weight': 4, 'l2_regularization': 0.8668413429347246}\n",
    "lgb_param5  = {'learning_rate': 0.1444166093465713,'n_estimators': 500,'max_depth': 17,'min_child_weight': 4, 'l2_regularization': 0.14729681542445355}\n",
    "lgb_param6  = {'learning_rate': 0.16680093590253176 ,'n_estimators': 500, 'max_depth': 16, 'min_child_weight': 3,'l2_regularization': 0.5478551432621794}\n",
    "lgb_param7  = {'learning_rate': 0.11803343495390418, 'n_estimators': 500, 'max_depth': 15, 'min_child_weight': 4, 'l2_regularization': 0.19592898855184526}\n",
    "lgb_param8  ={'learning_rate': 0.12829594410470951, 'n_estimators': 500, 'max_depth': 16, 'min_child_weight': 2, 'l2_regularization': 0.5336210596747876}\n",
    "lgb_param9  = {'learning_rate': 0.12829594410470951, 'n_estimators': 500, 'max_depth': 16, 'min_child_weight': 2, 'l2_regularization': 0.5336210596747876}\n",
    "lgb_param10 = {'learning_rate': 0.11803433443831565,'n_estimators': 500,'max_depth': 17,'min_child_weight': 4, 'l2_regularization': 0.7771696259733507}\n",
    "\n",
    "# Fit the model using best Parameter\n",
    "lgb_best1 = lgb.LGBMRegressor(**lgb_param1)\n",
    "lgb_best2 = lgb.LGBMRegressor(**lgb_param2)\n",
    "lgb_best3 = lgb.LGBMRegressor(**lgb_param3)\n",
    "lgb_best4 = lgb.LGBMRegressor(**lgb_param4)\n",
    "lgb_best5 = lgb.LGBMRegressor(**lgb_param5)\n",
    "lgb_best6 = lgb.LGBMRegressor(**lgb_param6)\n",
    "lgb_best7 = lgb.LGBMRegressor(**lgb_param7)\n",
    "lgb_best8 = lgb.LGBMRegressor(**lgb_param8)\n",
    "lgb_best9 = lgb.LGBMRegressor(**lgb_param9)\n",
    "lgb_best10 = lgb.LGBMRegressor(**lgb_param10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622147e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_best1.fit(train_hybrid_stn1[f_col], [i-j for i,j in zip(train_hybrid_stn1[target].ts.tolist(),  sum(trend_stn1.tolist(), []))])\n",
    "lgb_best2.fit(train_hybrid_stn2[f_col], [i-j for i,j in zip(train_hybrid_stn2[target].ts.tolist(),  sum(trend_stn2.tolist(), []))])\n",
    "lgb_best3.fit(train_hybrid_stn3[f_col], [i-j for i,j in zip(train_hybrid_stn3[target].ts.tolist(),  sum(trend_stn3.tolist(), []))])\n",
    "lgb_best4.fit(train_hybrid_stn4[f_col], [i-j for i,j in zip(train_hybrid_stn4[target].ts.tolist(),  sum(trend_stn4.tolist(), []))])\n",
    "lgb_best5.fit(train_hybrid_stn5[f_col], [i-j for i,j in zip(train_hybrid_stn5[target].ts.tolist(),  sum(trend_stn5.tolist(), []))])\n",
    "lgb_best6.fit(train_hybrid_stn6[f_col], [i-j for i,j in zip(train_hybrid_stn6[target].ts.tolist(),  sum(trend_stn6.tolist(), []))])\n",
    "lgb_best7.fit(train_hybrid_stn7[f_col], [i-j for i,j in zip(train_hybrid_stn7[target].ts.tolist(),  sum(trend_stn7.tolist(), []))])\n",
    "lgb_best8.fit(train_hybrid_stn8[f_col], [i-j for i,j in zip(train_hybrid_stn8[target].ts.tolist(),  sum(trend_stn8.tolist(), []))])\n",
    "lgb_best9.fit(train_hybrid_stn9[f_col], [i-j for i,j in zip(train_hybrid_stn9[target].ts.tolist(),  sum(trend_stn9.tolist(), []))])\n",
    "lgb_best10.fit(train_hybrid_stn10[f_col], [i-j for i,j in zip(train_hybrid_stn10[target].ts.tolist(),  sum(trend_stn10.tolist(), []))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2516e806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a년도의 지면온도를 stn 별로 예측\n",
    "a_seasonality_lgb_best1 = lgb_best1.predict(test_hybrid_stn1[f_col])\n",
    "a_seasonality_lgb_best2 = lgb_best2.predict(test_hybrid_stn1[f_col])\n",
    "a_seasonality_lgb_best3 = lgb_best3.predict(test_hybrid_stn1[f_col])\n",
    "a_seasonality_lgb_best4 = lgb_best4.predict(test_hybrid_stn1[f_col])\n",
    "a_seasonality_lgb_best5 = lgb_best5.predict(test_hybrid_stn1[f_col])\n",
    "a_seasonality_lgb_best6 = lgb_best6.predict(test_hybrid_stn1[f_col])\n",
    "a_seasonality_lgb_best7 = lgb_best7.predict(test_hybrid_stn1[f_col])\n",
    "a_seasonality_lgb_best8 = lgb_best8.predict(test_hybrid_stn1[f_col])\n",
    "a_seasonality_lgb_best9 = lgb_best9.predict(test_hybrid_stn1[f_col])\n",
    "a_seasonality_lgb_best10 = lgb_best10.predict(test_hybrid_stn1[f_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e223dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b년도의 지면온도를 stn 별로 예측\n",
    "b_seasonality_lgb_best1 = lgb_best1.predict(test_hybrid_stn2[f_col])\n",
    "b_seasonality_lgb_best2 = lgb_best2.predict(test_hybrid_stn2[f_col])\n",
    "b_seasonality_lgb_best3 = lgb_best3.predict(test_hybrid_stn2[f_col])\n",
    "b_seasonality_lgb_best4 = lgb_best4.predict(test_hybrid_stn2[f_col])\n",
    "b_seasonality_lgb_best5 = lgb_best5.predict(test_hybrid_stn2[f_col])\n",
    "b_seasonality_lgb_best6 = lgb_best6.predict(test_hybrid_stn2[f_col])\n",
    "b_seasonality_lgb_best7 = lgb_best7.predict(test_hybrid_stn2[f_col])\n",
    "b_seasonality_lgb_best8 = lgb_best8.predict(test_hybrid_stn2[f_col])\n",
    "b_seasonality_lgb_best9 = lgb_best9.predict(test_hybrid_stn2[f_col])\n",
    "b_seasonality_lgb_best10 = lgb_best10.predict(test_hybrid_stn2[f_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14ab113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c년도의 지면온도를 stn 별로 예측\n",
    "c_seasonality_lgb_best1 = lgb_best1.predict(test_hybrid_stn3[f_col])\n",
    "c_seasonality_lgb_best2 = lgb_best2.predict(test_hybrid_stn3[f_col])\n",
    "c_seasonality_lgb_best3 = lgb_best3.predict(test_hybrid_stn3[f_col])\n",
    "c_seasonality_lgb_best4 = lgb_best4.predict(test_hybrid_stn3[f_col])\n",
    "c_seasonality_lgb_best5 = lgb_best5.predict(test_hybrid_stn3[f_col])\n",
    "c_seasonality_lgb_best6 = lgb_best6.predict(test_hybrid_stn3[f_col])\n",
    "c_seasonality_lgb_best7 = lgb_best7.predict(test_hybrid_stn3[f_col])\n",
    "c_seasonality_lgb_best8 = lgb_best8.predict(test_hybrid_stn3[f_col])\n",
    "c_seasonality_lgb_best9 = lgb_best9.predict(test_hybrid_stn3[f_col])\n",
    "c_seasonality_lgb_best10 = lgb_best10.predict(test_hybrid_stn3[f_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d012794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10개 지점 추세 평균값을 사용\n",
    "avg_trend = [(a+b+c+d+e+f+g+h+i+j)/10 for a,b,c,d,e,f,g,h,i,j in zip(sum(trend_stn1.tolist(), []), sum(trend_stn2.tolist(), []), sum(trend_stn3.tolist(), []),\n",
    "                                    sum(trend_stn4.tolist(), []), sum(trend_stn5.tolist(), []), sum(trend_stn6.tolist(), []),\n",
    "                                    sum(trend_stn7.tolist(), []), sum(trend_stn8.tolist(), []), sum(trend_stn9.tolist(), []),\n",
    "                                    sum(trend_stn10.tolist(), []))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d9a880",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_seasonality = [(a+b+c+d+e+f+g+h+i+j)/10 for a,b,c,d,e,f,g,h,i,j in zip(a_seasonality_lgb_best1.tolist(),a_seasonality_lgb_best2.tolist(), a_seasonality_lgb_best3.tolist(),\n",
    "                                                                         a_seasonality_lgb_best4.tolist(),a_seasonality_lgb_best5.tolist(),a_seasonality_lgb_best6.tolist(),\n",
    "                                                                         a_seasonality_lgb_best7.tolist(),a_seasonality_lgb_best8.tolist(),a_seasonality_lgb_best9.tolist(),\n",
    "                                                                         a_seasonality_lgb_best10.tolist())]\n",
    "\n",
    "a_pred = [i+j for i,j in zip(avg_trend, a_seasonality)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088afd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_seasonality = [(a+b+c+d+e+f+g+h+i+j)/10 for a,b,c,d,e,f,g,h,i,j in zip(b_seasonality_lgb_best1.tolist(),b_seasonality_lgb_best2.tolist(), b_seasonality_lgb_best3.tolist(),\n",
    "                                                                         b_seasonality_lgb_best4.tolist(),b_seasonality_lgb_best5.tolist(), b_seasonality_lgb_best6.tolist(),\n",
    "                                                                         b_seasonality_lgb_best7.tolist(),b_seasonality_lgb_best8.tolist(), b_seasonality_lgb_best9.tolist(),\n",
    "                                                                         b_seasonality_lgb_best10.tolist())]\n",
    "\n",
    "b_pred = [i+j for i,j in zip(avg_trend, b_seasonality)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba2d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_seasonality = [(a+b+c+d+e+f+g+h+i+j)/10 for a,b,c,d,e,f,g,h,i,j in zip(c_seasonality_lgb_best1.tolist(),c_seasonality_lgb_best2.tolist(), c_seasonality_lgb_best3.tolist(),\n",
    "                                                                         c_seasonality_lgb_best4.tolist(),c_seasonality_lgb_best5.tolist(), c_seasonality_lgb_best6.tolist(),\n",
    "                                                                         c_seasonality_lgb_best7.tolist(),c_seasonality_lgb_best8.tolist(), c_seasonality_lgb_best9.tolist(),\n",
    "                                                                         c_seasonality_lgb_best10.tolist())]\n",
    "\n",
    "c_pred = [i+j for i,j in zip(avg_trend, c_seasonality)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ab367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측한 ts 저장\n",
    "a_pred.extend(b_pred)\n",
    "a_pred.extend(c_pred)\n",
    "test_fin['ts'] = a_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb9497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime에서 month 분리하기\n",
    "test_fin.reset_index(inplace = True)\n",
    "test_fin['month'] = test_fin['datetime'].str.split(\"-\").str.get(1)\n",
    "test_fin['month'] = test_fin['month'].astype(int)\n",
    "test_fin.drop(['datetime'],axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9dd224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 월 별로 나누기\n",
    "winter = [11 , 12, 1] ; spring = [2, 3, 4] ; summer = [5, 6, 7]  ; autumn = [8 , 9, 10]\n",
    "\n",
    "# Test\n",
    "winter_test = test_fin[test_fin['month'].isin(winter)]\n",
    "spring_test = test_fin[test_fin['month'].isin(spring)]\n",
    "summer_test = test_fin[test_fin['month'].isin(summer)]\n",
    "autumn_test = test_fin[test_fin['month'].isin(autumn)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
